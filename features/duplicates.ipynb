{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "691226d2",
   "metadata": {},
   "source": [
    "## **Feature:** Duplicate Detection\n",
    "\n",
    "**Names:** Gia Bao Ngo\n",
    "\n",
    "### **What it does**\n",
    "Intelligently detects and handles duplicate records in datasets using exact matching and fuzzy string matching. Provides multiple strategies for duplicate removal and comprehensive analysis of duplicate patterns to help users understand data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff1f7555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dotenv\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Get API Key\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"OpenAI API Key not found\")\n",
    "\n",
    "# Import libraries\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Additional imports for duplicate detection\n",
    "import math\n",
    "import re\n",
    "import datetime\n",
    "from sklearn import preprocessing, impute\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# Langchain imports\n",
    "from langchain_openai import ChatOpenAI  \n",
    "from langchain.schema import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a88657",
   "metadata": {},
   "source": [
    "### **Helper Functions**\n",
    "- `find_exact_duplicates(df, subset=None)` - Find rows with identical values\n",
    "- `find_near_duplicates(df, threshold=0.95, subset=None)` - Find similar rows using fuzzy matching\n",
    "- `remove_duplicates(df, strategy='first', subset=None)` - Remove duplicates with different strategies\n",
    "- `flag_duplicates(df, subset=None)` - Add boolean column marking duplicates\n",
    "- `analyze_duplicate_patterns(df, subset=None)` - Show duplicate statistics and patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60424d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_exact_duplicates(df, subset=None):\n",
    "    \"\"\"\n",
    "    Find rows with identical values across specified columns.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - subset: list of column names to check for duplicates (None = all columns)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with duplicate rows\n",
    "    \"\"\"\n",
    "    if subset is None:\n",
    "        subset = df.columns.tolist()\n",
    "    \n",
    "    # Find duplicates\n",
    "    duplicated_mask = df.duplicated(subset=subset, keep=False)\n",
    "    duplicates = df[duplicated_mask]\n",
    "    \n",
    "    if len(duplicates) > 0:\n",
    "        print(f\"Found {len(duplicates)} duplicate rows across columns: {subset}\")\n",
    "        return duplicates.sort_values(by=subset)\n",
    "    else:\n",
    "        print(\"No exact duplicates found.\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46a090e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_near_duplicates(df, threshold=0.95, subset=None):\n",
    "    \"\"\"\n",
    "    Find similar rows using fuzzy string matching with difflib.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - threshold: similarity threshold (0-1, default 0.95)\n",
    "    - subset: list of column names to check (None = text columns only)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with pairs of similar rows\n",
    "    \"\"\"\n",
    "    if subset is None:\n",
    "        # Focus on text columns for fuzzy matching\n",
    "        subset = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    if not subset:\n",
    "        print(\"No text columns found for fuzzy matching.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    near_duplicates = []\n",
    "    \n",
    "    # Convert to string and combine text columns for comparison\n",
    "    df_text = df[subset].astype(str)\n",
    "    combined_text = df_text.apply(lambda x: ' '.join(x), axis=1)\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        for j in range(i+1, len(df)):\n",
    "            similarity = SequenceMatcher(None, combined_text.iloc[i], combined_text.iloc[j]).ratio()\n",
    "            if similarity >= threshold:\n",
    "                near_duplicates.append({\n",
    "                    'index_1': df.index[i],\n",
    "                    'index_2': df.index[j],\n",
    "                    'similarity': round(similarity, 3),\n",
    "                    'text_1': combined_text.iloc[i][:100] + '...' if len(combined_text.iloc[i]) > 100 else combined_text.iloc[i],\n",
    "                    'text_2': combined_text.iloc[j][:100] + '...' if len(combined_text.iloc[j]) > 100 else combined_text.iloc[j]\n",
    "                })\n",
    "    \n",
    "    if near_duplicates:\n",
    "        result_df = pd.DataFrame(near_duplicates)\n",
    "        print(f\"Found {len(near_duplicates)} pairs of near-duplicate rows (similarity >= {threshold})\")\n",
    "        return result_df\n",
    "    else:\n",
    "        print(f\"No near-duplicates found with similarity >= {threshold}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "akska8s2gkp",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df, strategy='first', subset=None):\n",
    "    \"\"\"\n",
    "    Remove duplicates with different strategies.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - strategy: 'first', 'last', or 'most_complete'\n",
    "    - subset: list of column names to check for duplicates (None = all columns)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with duplicates removed\n",
    "    \"\"\"\n",
    "    if subset is None:\n",
    "        subset = df.columns.tolist()\n",
    "    \n",
    "    original_count = len(df)\n",
    "    \n",
    "    if strategy == 'first':\n",
    "        result_df = df.drop_duplicates(subset=subset, keep='first')\n",
    "    elif strategy == 'last':\n",
    "        result_df = df.drop_duplicates(subset=subset, keep='last')\n",
    "    elif strategy == 'most_complete':\n",
    "        # Keep the row with the fewest missing values\n",
    "        duplicated_mask = df.duplicated(subset=subset, keep=False)\n",
    "        if duplicated_mask.any():\n",
    "            duplicated_groups = df[duplicated_mask].groupby(df[duplicated_mask][subset].apply(tuple, axis=1))\n",
    "            keep_indices = []\n",
    "            \n",
    "            for name, group in duplicated_groups:\n",
    "                # Find row with minimum missing values\n",
    "                missing_counts = group.isnull().sum(axis=1)\n",
    "                best_row_idx = missing_counts.idxmin()\n",
    "                keep_indices.append(best_row_idx)\n",
    "            \n",
    "            # Keep non-duplicated rows and best duplicated rows\n",
    "            non_duplicated = df[~duplicated_mask]\n",
    "            best_duplicated = df.loc[keep_indices]\n",
    "            result_df = pd.concat([non_duplicated, best_duplicated]).sort_index()\n",
    "        else:\n",
    "            result_df = df.copy()\n",
    "    else:\n",
    "        print(f\"Unknown strategy '{strategy}'. Using 'first' instead.\")\n",
    "        result_df = df.drop_duplicates(subset=subset, keep='first')\n",
    "    \n",
    "    removed_count = original_count - len(result_df)\n",
    "    print(f\"Removed {removed_count} duplicate rows using '{strategy}' strategy\")\n",
    "    print(f\"Dataset reduced from {original_count} to {len(result_df)} rows\")\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8ldoqmc69g",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_duplicates(df, subset=None):\n",
    "    \"\"\"\n",
    "    Add boolean column marking duplicates for inspection before removal.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - subset: list of column names to check for duplicates (None = all columns)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with added 'is_duplicate' column\n",
    "    \"\"\"\n",
    "    if subset is None:\n",
    "        subset = df.columns.tolist()\n",
    "    \n",
    "    result_df = df.copy()\n",
    "    result_df['is_duplicate'] = df.duplicated(subset=subset, keep=False)\n",
    "    \n",
    "    duplicate_count = result_df['is_duplicate'].sum()\n",
    "    total_count = len(result_df)\n",
    "    \n",
    "    print(f\"Flagged {duplicate_count} rows as duplicates out of {total_count} total rows\")\n",
    "    print(f\"Duplicate percentage: {(duplicate_count/total_count)*100:.2f}%\")\n",
    "    \n",
    "    if duplicate_count > 0:\n",
    "        print(\"\\nDuplicate rows marked with 'is_duplicate=True' column\")\n",
    "        print(\"Review flagged rows before deciding on removal strategy\")\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hb38iwgyw5q",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_duplicate_patterns(df, subset=None):\n",
    "    \"\"\"\n",
    "    Show duplicate statistics and patterns to understand which columns contribute most to duplicates.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - subset: list of column names to analyze (None = all columns)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame (unchanged, prints analysis)\n",
    "    \"\"\"\n",
    "    if subset is None:\n",
    "        subset = df.columns.tolist()\n",
    "    \n",
    "    print(\"=== DUPLICATE PATTERN ANALYSIS ===\\n\")\n",
    "    \n",
    "    # Overall duplicate statistics\n",
    "    total_duplicates = df.duplicated(subset=subset, keep=False).sum()\n",
    "    unique_duplicate_groups = df[df.duplicated(subset=subset, keep=False)].drop_duplicates(subset=subset).shape[0]\n",
    "    \n",
    "    print(f\"Total duplicate rows: {total_duplicates}\")\n",
    "    print(f\"Unique duplicate groups: {unique_duplicate_groups}\")\n",
    "    print(f\"Average duplicates per group: {total_duplicates/unique_duplicate_groups if unique_duplicate_groups > 0 else 0:.2f}\")\n",
    "    \n",
    "    # Column-wise duplicate analysis\n",
    "    print(\"\\n--- Column Contribution to Duplicates ---\")\n",
    "    duplicate_contributions = {}\n",
    "    \n",
    "    for col in subset:\n",
    "        col_duplicates = df.duplicated(subset=[col], keep=False).sum()\n",
    "        duplicate_contributions[col] = col_duplicates\n",
    "    \n",
    "    contribution_df = pd.DataFrame(list(duplicate_contributions.items()), \n",
    "                                  columns=['Column', 'Duplicate_Count'])\n",
    "    contribution_df['Duplicate_Percentage'] = (contribution_df['Duplicate_Count'] / len(df) * 100).round(2)\n",
    "    contribution_df = contribution_df.sort_values('Duplicate_Count', ascending=False)\n",
    "    \n",
    "    print(contribution_df.to_string(index=False))\n",
    "    \n",
    "    # Most duplicated values\n",
    "    print(\"\\n--- Most Frequent Duplicate Patterns ---\")\n",
    "    if total_duplicates > 0:\n",
    "        duplicate_rows = df[df.duplicated(subset=subset, keep=False)]\n",
    "        value_counts = duplicate_rows.groupby(subset).size().sort_values(ascending=False).head(5)\n",
    "        \n",
    "        for i, (values, count) in enumerate(value_counts.items(), 1):\n",
    "            if isinstance(values, tuple):\n",
    "                pattern = dict(zip(subset, values))\n",
    "            else:\n",
    "                pattern = {subset[0]: values}\n",
    "            print(f\"{i}. Count: {count}, Pattern: {pattern}\")\n",
    "    \n",
    "    print(f\"\\n=== END ANALYSIS ===\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c99ee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_docs = \"\"\" Helper functions available:\n",
    "- find_exact_duplicates(df, subset=None): Find rows with identical values across specified columns. Returns DataFrame with duplicate rows.\n",
    "- find_near_duplicates(df, threshold=0.95, subset=None): Find similar rows using fuzzy string matching with difflib. Returns DataFrame with pairs of similar rows and similarity scores.\n",
    "- remove_duplicates(df, strategy='first', subset=None): Remove duplicates with different strategies ('first', 'last', 'most_complete'). Returns cleaned DataFrame.\n",
    "- flag_duplicates(df, subset=None): Add boolean 'is_duplicate' column marking duplicates for inspection. Returns DataFrame with flag column.\n",
    "- analyze_duplicate_patterns(df, subset=None): Print comprehensive analysis of duplicate patterns and statistics. Returns unchanged DataFrame.\n",
    "\n",
    "Examples:\n",
    "- \"Find duplicate rows\" -> find_exact_duplicates(df)\n",
    "- \"Remove duplicates keeping first\" -> df = remove_duplicates(df, strategy='first')\n",
    "- \"Show duplicate patterns\" -> analyze_duplicate_patterns(df)\n",
    "- \"Flag duplicates for review\" -> df = flag_duplicates(df)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b02315b",
   "metadata": {},
   "source": [
    "# **MAIN FEATURE FUNCTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "881f15c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicates(df, user_query):\n",
    "    \"\"\"\n",
    "    Main function that gets called by the main router.\n",
    "    MUST take (df, user_query) and return df\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create message chain\n",
    "    messages = []\n",
    "    messages.append(SystemMessage(content=helper_docs))\n",
    "    messages.append(SystemMessage(content=f\"\"\"\n",
    "    You are a data cleaning agent focused on duplicate detection and handling.\n",
    "    \n",
    "    Dataset info: Shape: {df.shape}, Sample: {df.head(3).to_string()}\n",
    "\n",
    "    Libraries available:\n",
    "    - pd (pandas), np (numpy)\n",
    "    - math, re, datetime\n",
    "    - difflib (for fuzzy matching with SequenceMatcher)\n",
    "    - All helper functions listed above\n",
    "    \n",
    "    Rules:\n",
    "    - Return only executable Python code, no explanations, no markdown blocks\n",
    "    - Use helper functions when appropriate for duplicate detection tasks\n",
    "    - ASSUME \"df\" IS ALREADY DEFINED\n",
    "    - For analysis queries, use helper functions that print results (analyze_duplicate_patterns, find_exact_duplicates)\n",
    "    - For data cleaning, use helper functions that modify DataFrame (remove_duplicates, flag_duplicates)\n",
    "    - ALWAYS assign the result back to df when modifying: df = remove_duplicates(df, strategy='first')\n",
    "    - In order to generate a response/message to the user use print statements\n",
    "    print(\"message\")\n",
    "    - Write a detailed print message to summarise actions taken and reasons\n",
    "    \n",
    "    Common query patterns:\n",
    "    - \"Find duplicates\" or \"Show duplicate rows\" -> use find_exact_duplicates(df)\n",
    "    - \"Remove duplicates\" -> df = remove_duplicates(df, strategy='first')  \n",
    "    - \"Analyze duplicate patterns\" -> analyze_duplicate_patterns(df)\n",
    "    - \"Flag duplicates\" or \"Mark duplicates\" -> df = flag_duplicates(df)\n",
    "    - \"Find similar records\" -> find_near_duplicates(df, threshold=0.90)\n",
    "    \"\"\"))\n",
    "    messages.append(HumanMessage(content=f\"User request: {user_query}\"))\n",
    "    \n",
    "    # Call LLM with message chain\n",
    "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")\n",
    "    response = llm.invoke(messages)\n",
    "    generated_code = response.content.strip()\n",
    "    \n",
    "    # Execute code\n",
    "    try:\n",
    "        original_df = df.copy()\n",
    "        # Create local namespace with our variables\n",
    "        local_vars = {\n",
    "            'df': df.copy(),\n",
    "            'original_df': original_df,\n",
    "            'pd': pd,\n",
    "            'np': np,\n",
    "            'find_exact_duplicates': find_exact_duplicates,\n",
    "            'find_near_duplicates': find_near_duplicates,\n",
    "            'remove_duplicates': remove_duplicates,\n",
    "            'flag_duplicates': flag_duplicates,\n",
    "            'analyze_duplicate_patterns': analyze_duplicate_patterns,\n",
    "            'print': print\n",
    "        }\n",
    "        \n",
    "        exec(generated_code, globals(), local_vars)\n",
    "        return local_vars['df']\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"Generated Code:{generated_code}\")\n",
    "        return original_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30928b6",
   "metadata": {},
   "source": [
    "# **Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b618978f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test DataFrame:\n",
      "          Name  Age         City            Email\n",
      "0     John Doe   25     New York   john@email.com\n",
      "1   Jane Smith   30  Los Angeles   jane@email.com\n",
      "2     John Doe   25     New York   john@email.com\n",
      "3   Bob Wilson   35      Chicago    bob@email.com\n",
      "4   Jane Smith   30  Los Angeles   jane@email.com\n",
      "5  Alice Brown   28       Boston  alice@email.com\n"
     ]
    }
   ],
   "source": [
    "# # Create sample data with known duplicates\n",
    "# test_data = {\n",
    "#     'Name': ['John Doe', 'Jane Smith', 'John Doe', 'Bob Wilson', 'Jane Smith', 'Alice Brown'],\n",
    "#     'Age': [25, 30, 25, 35, 30, 28],\n",
    "#     'City': ['New York', 'Los Angeles', 'New York', 'Chicago', 'Los Angeles', 'Boston'],\n",
    "#     'Email': ['john@email.com', 'jane@email.com', 'john@email.com', 'bob@email.com', 'jane@email.com', 'alice@email.com']\n",
    "# }\n",
    "\n",
    "# test_df = pd.DataFrame(test_data)\n",
    "# print(\"Test DataFrame:\")\n",
    "# print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f98970af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 2 duplicate rows using 'first' strategy\n",
      "Dataset reduced from 6 to 4 rows\n",
      "Duplicated rows have been removed, keeping the first occurrence of each unique row.\n"
     ]
    }
   ],
   "source": [
    "# query = \"drop duplicated rows\"\n",
    "# result = duplicates(test_df, query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
