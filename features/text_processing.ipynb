{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "691226d2",
   "metadata": {},
   "source": [
    "## **Feature:** Text Processing\n",
    "\n",
    "**Names:** Gia Bao Ngo\n",
    "\n",
    "### **What it does**\n",
    "Comprehensive text cleaning and standardization functionality. Handles basic text cleaning (whitespace, encoding), case standardization, special character removal, categorical value standardization with fuzzy matching, numeric extraction from mixed text, and pattern validation using regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff1f7555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dotenv\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Get API Key\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"OpenAI API Key not found\")\n",
    "\n",
    "# Import libraries\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Additional imports for text processing\n",
    "import math\n",
    "import re\n",
    "import datetime\n",
    "import unicodedata\n",
    "from sklearn import preprocessing\n",
    "from difflib import get_close_matches\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Langchain imports\n",
    "from langchain_openai import ChatOpenAI  \n",
    "from langchain.schema import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a88657",
   "metadata": {},
   "source": [
    "### **Helper Functions**\n",
    "- `clean_text_basic(df, columns=None)` - Remove extra whitespace, standardize encoding\n",
    "- `standardize_case(df, columns=None, case='title')` - Consistent case formatting\n",
    "- `remove_special_chars(df, columns=None, keep_patterns=[])` - Clean special characters\n",
    "- `standardize_categorical_values(df, column, mapping_dict=None)` - Map variants to standard values\n",
    "- `extract_numeric_from_text(df, columns=None)` - Extract numbers from mixed text\n",
    "- `validate_text_patterns(df, column, pattern)` - Validate against regex patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60424d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_basic(df, columns=None):\n",
    "    \"\"\"\n",
    "    Remove extra whitespace and standardize encoding for text columns.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - columns: list of column names to clean (None = all object columns)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with cleaned text columns\n",
    "    \"\"\"\n",
    "    result_df = df.copy()\n",
    "    cleaned_cols = []\n",
    "    \n",
    "    if columns is None:\n",
    "        columns = result_df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    for col in columns:\n",
    "        if col not in result_df.columns:\n",
    "            continue\n",
    "            \n",
    "        # Skip if column is not text-like\n",
    "        if not pd.api.types.is_object_dtype(result_df[col]):\n",
    "            continue\n",
    "            \n",
    "        original_null_count = result_df[col].isnull().sum()\n",
    "        \n",
    "        try:\n",
    "            # Convert to string and handle NaN values\n",
    "            text_series = result_df[col].astype(str)\n",
    "            \n",
    "            # Basic cleaning operations\n",
    "            # 1. Normalize unicode characters\n",
    "            text_series = text_series.apply(lambda x: unicodedata.normalize('NFKD', x) if x != 'nan' else x)\n",
    "            \n",
    "            # 2. Strip leading/trailing whitespace\n",
    "            text_series = text_series.str.strip()\n",
    "            \n",
    "            # 3. Replace multiple whitespace with single space\n",
    "            text_series = text_series.str.replace(r'\\s+', ' ', regex=True)\n",
    "            \n",
    "            # 4. Remove zero-width characters\n",
    "            text_series = text_series.str.replace(r'[\\u200b-\\u200d\\ufeff]', '', regex=True)\n",
    "            \n",
    "            # 5. Convert back 'nan' strings to actual NaN\n",
    "            text_series = text_series.replace('nan', pd.NA)\n",
    "            \n",
    "            # Check if cleaning made meaningful changes\n",
    "            if not text_series.equals(result_df[col]):\n",
    "                result_df[col] = text_series\n",
    "                new_null_count = result_df[col].isnull().sum()\n",
    "                \n",
    "                cleaned_cols.append({\n",
    "                    'column': col,\n",
    "                    'null_count_before': original_null_count,\n",
    "                    'null_count_after': new_null_count,\n",
    "                    'total_rows': len(result_df)\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not clean column '{col}': {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"=== TEXT BASIC CLEANING RESULTS ===\")\n",
    "    print(f\"Columns processed: {len(columns)}\")\n",
    "    print(f\"Columns cleaned: {len(cleaned_cols)}\")\n",
    "    \n",
    "    if cleaned_cols:\n",
    "        print(\"\\\\nCleaning details:\")\n",
    "        for clean in cleaned_cols:\n",
    "            null_change = clean['null_count_after'] - clean['null_count_before']\n",
    "            null_change_str = f\"({null_change:+d} nulls)\" if null_change != 0 else \"\"\n",
    "            print(f\"  {clean['column']}: Whitespace and encoding normalized {null_change_str}\")\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46a090e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_case(df, columns=None, case='title'):\n",
    "    \"\"\"\n",
    "    Standardize case formatting for text columns.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - columns: list of column names to standardize (None = all object columns)\n",
    "    - case: case format ('upper', 'lower', 'title', 'sentence', 'proper')\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with standardized case\n",
    "    \"\"\"\n",
    "    result_df = df.copy()\n",
    "    standardized_cols = []\n",
    "    \n",
    "    if columns is None:\n",
    "        columns = result_df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    valid_cases = ['upper', 'lower', 'title', 'sentence', 'proper']\n",
    "    if case not in valid_cases:\n",
    "        print(f\"Warning: Invalid case '{case}'. Using 'title' instead.\")\n",
    "        case = 'title'\n",
    "    \n",
    "    for col in columns:\n",
    "        if col not in result_df.columns:\n",
    "            continue\n",
    "            \n",
    "        # Skip if column is not text-like\n",
    "        if not pd.api.types.is_object_dtype(result_df[col]):\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            original_values = result_df[col].copy()\n",
    "            \n",
    "            if case == 'upper':\n",
    "                result_df[col] = result_df[col].str.upper()\n",
    "            elif case == 'lower':\n",
    "                result_df[col] = result_df[col].str.lower()\n",
    "            elif case == 'title':\n",
    "                result_df[col] = result_df[col].str.title()\n",
    "            elif case == 'sentence':\n",
    "                # First letter capitalized, rest lowercase\n",
    "                result_df[col] = result_df[col].str.lower().str.capitalize()\n",
    "            elif case == 'proper':\n",
    "                # Smart title case (avoids capitalizing articles, prepositions)\n",
    "                def proper_case(text):\n",
    "                    if pd.isna(text):\n",
    "                        return text\n",
    "                    \n",
    "                    # Words that shouldn't be capitalized unless they're first/last\n",
    "                    minor_words = {'a', 'an', 'and', 'as', 'at', 'but', 'by', 'for', \n",
    "                                   'if', 'in', 'of', 'on', 'or', 'the', 'to', 'with'}\n",
    "                    \n",
    "                    words = str(text).lower().split()\n",
    "                    if not words:\n",
    "                        return text\n",
    "                    \n",
    "                    # Always capitalize first and last word\n",
    "                    words[0] = words[0].capitalize()\n",
    "                    if len(words) > 1:\n",
    "                        words[-1] = words[-1].capitalize()\n",
    "                    \n",
    "                    # Capitalize middle words unless they're minor words\n",
    "                    for i in range(1, len(words) - 1):\n",
    "                        if words[i] not in minor_words:\n",
    "                            words[i] = words[i].capitalize()\n",
    "                    \n",
    "                    return ' '.join(words)\n",
    "                \n",
    "                result_df[col] = result_df[col].apply(proper_case)\n",
    "            \n",
    "            # Check if changes were made\n",
    "            if not result_df[col].equals(original_values):\n",
    "                # Count changes\n",
    "                changes_made = (~result_df[col].equals(original_values)).sum()\n",
    "                standardized_cols.append({\n",
    "                    'column': col,\n",
    "                    'case_format': case,\n",
    "                    'changes_made': changes_made,\n",
    "                    'total_non_null': result_df[col].notna().sum()\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not standardize case for column '{col}': {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"=== CASE STANDARDIZATION RESULTS ===\")\n",
    "    print(f\"Columns processed: {len(columns)}\")\n",
    "    print(f\"Columns standardized: {len(standardized_cols)}\")\n",
    "    print(f\"Case format applied: {case}\")\n",
    "    \n",
    "    if standardized_cols:\n",
    "        print(\"\\\\nStandardization details:\")\n",
    "        for std in standardized_cols:\n",
    "            change_ratio = std['changes_made'] / std['total_non_null'] if std['total_non_null'] > 0 else 0\n",
    "            print(f\"  {std['column']}: {std['changes_made']} values changed ({change_ratio:.1%})\")\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "prlf6ft3xka",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_chars(df, columns=None, keep_patterns=None):\n",
    "    \"\"\"\n",
    "    Remove special characters from text columns with customizable keep-lists.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - columns: list of column names to clean (None = all object columns)\n",
    "    - keep_patterns: list of regex patterns to keep (e.g., ['[a-zA-Z0-9]', '\\\\s', '-'])\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with special characters removed\n",
    "    \"\"\"\n",
    "    result_df = df.copy()\n",
    "    cleaned_cols = []\n",
    "    \n",
    "    if columns is None:\n",
    "        columns = result_df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    # Default patterns to keep: alphanumeric, spaces, and basic punctuation\n",
    "    if keep_patterns is None:\n",
    "        keep_patterns = ['[a-zA-Z0-9]', '\\\\s', '[.,!?;:()\\\\-\\'\\\"\"]']\n",
    "    \n",
    "    # Build regex pattern for characters to keep\n",
    "    keep_pattern = '|'.join(keep_patterns)\n",
    "    remove_pattern = f'[^{keep_pattern}]'\n",
    "    \n",
    "    for col in columns:\n",
    "        if col not in result_df.columns:\n",
    "            continue\n",
    "            \n",
    "        # Skip if column is not text-like\n",
    "        if not pd.api.types.is_object_dtype(result_df[col]):\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            original_values = result_df[col].copy()\n",
    "            \n",
    "            # Remove special characters using regex\n",
    "            cleaned_series = result_df[col].str.replace(remove_pattern, '', regex=True)\n",
    "            \n",
    "            # Remove multiple spaces that might result from character removal\n",
    "            cleaned_series = cleaned_series.str.replace(r'\\\\s+', ' ', regex=True)\n",
    "            cleaned_series = cleaned_series.str.strip()\n",
    "            \n",
    "            # Check if changes were made\n",
    "            if not cleaned_series.equals(original_values):\n",
    "                # Count how many values were changed\n",
    "                changes_made = (~cleaned_series.equals(original_values)).sum()\n",
    "                \n",
    "                # Sample some changes for reporting\n",
    "                changed_indices = ~cleaned_series.equals(original_values)\n",
    "                if changed_indices.any():\n",
    "                    sample_changes = []\n",
    "                    changed_rows = result_df[changed_indices].head(3)\n",
    "                    for idx in changed_rows.index:\n",
    "                        if pd.notna(original_values.loc[idx]) and pd.notna(cleaned_series.loc[idx]):\n",
    "                            original = str(original_values.loc[idx])[:50]\n",
    "                            cleaned = str(cleaned_series.loc[idx])[:50]\n",
    "                            if original != cleaned:\n",
    "                                sample_changes.append((original, cleaned))\n",
    "                \n",
    "                result_df[col] = cleaned_series\n",
    "                \n",
    "                cleaned_cols.append({\n",
    "                    'column': col,\n",
    "                    'changes_made': changes_made,\n",
    "                    'total_non_null': result_df[col].notna().sum(),\n",
    "                    'keep_patterns': keep_patterns,\n",
    "                    'sample_changes': sample_changes[:2]  # Keep first 2 examples\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not remove special characters from column '{col}': {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"=== SPECIAL CHARACTER REMOVAL RESULTS ===\")\n",
    "    print(f\"Columns processed: {len(columns)}\")\n",
    "    print(f\"Columns cleaned: {len(cleaned_cols)}\")\n",
    "    print(f\"Patterns kept: {keep_patterns}\")\n",
    "    \n",
    "    if cleaned_cols:\n",
    "        print(\"\\\\nCleaning details:\")\n",
    "        for clean in cleaned_cols:\n",
    "            change_ratio = clean['changes_made'] / clean['total_non_null'] if clean['total_non_null'] > 0 else 0\n",
    "            print(f\"  {clean['column']}: {clean['changes_made']} values changed ({change_ratio:.1%})\")\n",
    "            \n",
    "            # Show sample changes\n",
    "            for orig, new in clean['sample_changes']:\n",
    "                if orig != new:\n",
    "                    print(f\"    Example: '{orig}' → '{new}'\")\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "klyswhxuon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_categorical_values(df, column, mapping_dict=None, similarity_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Map variants of categorical values to standard values using fuzzy matching.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - column: column name to standardize\n",
    "    - mapping_dict: optional dict with exact mappings {'variant': 'standard'}\n",
    "    - similarity_threshold: threshold for fuzzy matching (0.0 to 1.0)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with standardized categorical values\n",
    "    \"\"\"\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    if column not in result_df.columns:\n",
    "        print(f\"Warning: Column '{column}' not found in DataFrame\")\n",
    "        return result_df\n",
    "    \n",
    "    # Get unique non-null values\n",
    "    unique_values = result_df[column].dropna().unique()\n",
    "    if len(unique_values) == 0:\n",
    "        print(f\"Warning: No non-null values found in column '{column}'\")\n",
    "        return result_df\n",
    "    \n",
    "    # Convert to strings for processing\n",
    "    unique_str_values = [str(val).strip() for val in unique_values]\n",
    "    \n",
    "    # Initialize mappings\n",
    "    final_mapping = {}\n",
    "    \n",
    "    # Apply user-provided mappings first\n",
    "    if mapping_dict:\n",
    "        for variant, standard in mapping_dict.items():\n",
    "            if variant in unique_str_values:\n",
    "                final_mapping[variant] = standard\n",
    "    \n",
    "    # Find fuzzy matches for remaining values\n",
    "    unmapped_values = [val for val in unique_str_values if val not in final_mapping]\n",
    "    \n",
    "    if len(unmapped_values) > 1:\n",
    "        # Group similar values\n",
    "        groups = []\n",
    "        used_values = set()\n",
    "        \n",
    "        for value in unmapped_values:\n",
    "            if value in used_values:\n",
    "                continue\n",
    "                \n",
    "            # Find similar values\n",
    "            similar_values = get_close_matches(\n",
    "                value, \n",
    "                unmapped_values, \n",
    "                n=len(unmapped_values), \n",
    "                cutoff=similarity_threshold\n",
    "            )\n",
    "            \n",
    "            if len(similar_values) > 1:\n",
    "                # Create group with most common/shortest as standard\n",
    "                group_values = [val for val in similar_values if val not in used_values]\n",
    "                if group_values:\n",
    "                    # Choose standard value (prefer shorter, more common)\n",
    "                    value_counts = result_df[column].astype(str).value_counts()\n",
    "                    \n",
    "                    # Sort by frequency (desc) then by length (asc)\n",
    "                    standard_value = max(group_values, \n",
    "                                       key=lambda x: (value_counts.get(x, 0), -len(x)))\n",
    "                    \n",
    "                    # Map all variants to standard\n",
    "                    for variant in group_values:\n",
    "                        if variant != standard_value:\n",
    "                            final_mapping[variant] = standard_value\n",
    "                        used_values.add(variant)\n",
    "                    \n",
    "                    groups.append({\n",
    "                        'standard': standard_value,\n",
    "                        'variants': [v for v in group_values if v != standard_value],\n",
    "                        'count': len(group_values)\n",
    "                    })\n",
    "    \n",
    "    # Apply mappings\n",
    "    if final_mapping:\n",
    "        original_values = result_df[column].copy()\n",
    "        result_df[column] = result_df[column].astype(str).replace(final_mapping)\n",
    "        \n",
    "        # Convert back to original type if possible\n",
    "        try:\n",
    "            if original_values.dtype != 'object':\n",
    "                result_df[column] = result_df[column].astype(original_values.dtype)\n",
    "        except:\n",
    "            pass  # Keep as string if conversion fails\n",
    "        \n",
    "        # Report results\n",
    "        changes_made = (~result_df[column].equals(original_values)).sum()\n",
    "        \n",
    "        print(f\"=== CATEGORICAL STANDARDIZATION RESULTS ===\")\n",
    "        print(f\"Column: {column}\")\n",
    "        print(f\"Total mappings applied: {len(final_mapping)}\")\n",
    "        print(f\"Values changed: {changes_made}\")\n",
    "        print(f\"Unique values before: {len(unique_values)}\")\n",
    "        print(f\"Unique values after: {result_df[column].nunique()}\")\n",
    "        \n",
    "        if final_mapping:\n",
    "            print(\"\\\\nMappings applied:\")\n",
    "            for variant, standard in final_mapping.items():\n",
    "                count = (original_values.astype(str) == variant).sum()\n",
    "                print(f\"  '{variant}' → '{standard}' ({count} occurrences)\")\n",
    "        \n",
    "        # Show fuzzy groups if any\n",
    "        if 'groups' in locals() and groups:\n",
    "            print(\"\\\\nFuzzy matching groups:\")\n",
    "            for group in groups:\n",
    "                if group['variants']:\n",
    "                    print(f\"  Standard: '{group['standard']}'\")\n",
    "                    for variant in group['variants']:\n",
    "                        print(f\"    Variant: '{variant}'\")\n",
    "    else:\n",
    "        print(f\"=== CATEGORICAL STANDARDIZATION RESULTS ===\")\n",
    "        print(f\"Column: {column}\")\n",
    "        print(f\"No mappings needed - values already standardized\")\n",
    "        print(f\"Unique values: {len(unique_values)}\")\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "k3ci3vyxjz",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_numeric_from_text(df, columns=None, create_new_columns=True):\n",
    "    \"\"\"\n",
    "    Extract numeric values from mixed text columns.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - columns: list of column names to process (None = all object columns)\n",
    "    - create_new_columns: if True, create new columns with '_numeric' suffix\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with extracted numeric values\n",
    "    \"\"\"\n",
    "    result_df = df.copy()\n",
    "    extracted_cols = []\n",
    "    \n",
    "    if columns is None:\n",
    "        columns = result_df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    # Regex patterns for different numeric formats\n",
    "    patterns = {\n",
    "        'integer': r'-?\\\\b\\\\d+\\\\b',\n",
    "        'decimal': r'-?\\\\b\\\\d+\\\\.\\\\d+\\\\b',\n",
    "        'currency': r'\\\\$?\\\\s*-?\\\\d{1,3}(?:,\\\\d{3})*(?:\\\\.\\\\d{2})?',\n",
    "        'percentage': r'-?\\\\d+(?:\\\\.\\\\d+)?%',\n",
    "        'scientific': r'-?\\\\d+(?:\\\\.\\\\d+)?[eE][-+]?\\\\d+',\n",
    "        'general_number': r'-?\\\\d+(?:\\\\.\\\\d+)?'\n",
    "    }\n",
    "    \n",
    "    for col in columns:\n",
    "        if col not in result_df.columns:\n",
    "            continue\n",
    "            \n",
    "        # Skip if column is not text-like\n",
    "        if not pd.api.types.is_object_dtype(result_df[col]):\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            text_series = result_df[col].astype(str)\n",
    "            extracted_numbers = []\n",
    "            extraction_info = {\n",
    "                'pattern_matches': {},\n",
    "                'successful_extractions': 0,\n",
    "                'total_attempts': 0\n",
    "            }\n",
    "            \n",
    "            for value in text_series:\n",
    "                if pd.isna(value) or value == 'nan':\n",
    "                    extracted_numbers.append(np.nan)\n",
    "                    continue\n",
    "                    \n",
    "                extraction_info['total_attempts'] += 1\n",
    "                number_found = None\n",
    "                pattern_used = None\n",
    "                \n",
    "                # Try patterns in order of specificity\n",
    "                for pattern_name, pattern in patterns.items():\n",
    "                    matches = re.findall(pattern, str(value))\n",
    "                    if matches:\n",
    "                        # Take the first/largest match\n",
    "                        best_match = max(matches, key=len) if len(matches) > 1 else matches[0]\n",
    "                        \n",
    "                        try:\n",
    "                            # Clean and convert the match\n",
    "                            clean_number = best_match.replace(',', '').replace('$', '').replace('%', '')\n",
    "                            if pattern_name == 'percentage':\n",
    "                                number_found = float(clean_number) / 100\n",
    "                            else:\n",
    "                                number_found = float(clean_number)\n",
    "                            \n",
    "                            pattern_used = pattern_name\n",
    "                            extraction_info['pattern_matches'][pattern_name] = extraction_info['pattern_matches'].get(pattern_name, 0) + 1\n",
    "                            extraction_info['successful_extractions'] += 1\n",
    "                            break\n",
    "                            \n",
    "                        except ValueError:\n",
    "                            continue\n",
    "                \n",
    "                extracted_numbers.append(number_found)\n",
    "            \n",
    "            # Convert to pandas Series\n",
    "            numeric_series = pd.Series(extracted_numbers, index=result_df.index)\n",
    "            \n",
    "            # Check if extraction was successful\n",
    "            success_rate = extraction_info['successful_extractions'] / extraction_info['total_attempts'] if extraction_info['total_attempts'] > 0 else 0\n",
    "            \n",
    "            if success_rate > 0.1:  # At least 10% success rate\n",
    "                if create_new_columns:\n",
    "                    new_col_name = f\"{col}_numeric\"\n",
    "                    result_df[new_col_name] = numeric_series\n",
    "                else:\n",
    "                    result_df[col] = numeric_series\n",
    "                \n",
    "                extracted_cols.append({\n",
    "                    'original_column': col,\n",
    "                    'new_column': new_col_name if create_new_columns else col,\n",
    "                    'success_rate': success_rate,\n",
    "                    'successful_extractions': extraction_info['successful_extractions'],\n",
    "                    'total_attempts': extraction_info['total_attempts'],\n",
    "                    'pattern_matches': extraction_info['pattern_matches'],\n",
    "                    'created_new_column': create_new_columns\n",
    "                })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not extract numbers from column '{col}': {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"=== NUMERIC EXTRACTION RESULTS ===\")\n",
    "    print(f\"Columns processed: {len(columns)}\")\n",
    "    print(f\"Successful extractions: {len(extracted_cols)}\")\n",
    "    \n",
    "    if extracted_cols:\n",
    "        print(\"\\\\nExtraction details:\")\n",
    "        for extract in extracted_cols:\n",
    "            print(f\"  {extract['original_column']}:\")\n",
    "            print(f\"    → {extract['new_column']} (success rate: {extract['success_rate']:.1%})\")\n",
    "            print(f\"    → {extract['successful_extractions']}/{extract['total_attempts']} values extracted\")\n",
    "            \n",
    "            # Show pattern usage\n",
    "            if extract['pattern_matches']:\n",
    "                pattern_str = ', '.join([f\"{k}: {v}\" for k, v in extract['pattern_matches'].items()])\n",
    "                print(f\"    → Patterns used: {pattern_str}\")\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dwel2m5xk5r",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_text_patterns(df, column, pattern, pattern_name=None):\n",
    "    \"\"\"\n",
    "    Validate text values against regex patterns and report violations.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - column: column name to validate\n",
    "    - pattern: regex pattern to validate against\n",
    "    - pattern_name: descriptive name for the pattern (optional)\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with validation results and DataFrame with validation flags\n",
    "    \"\"\"\n",
    "    if column not in df.columns:\n",
    "        print(f\"Warning: Column '{column}' not found in DataFrame\")\n",
    "        return {'valid': False, 'error': 'Column not found'}\n",
    "    \n",
    "    try:\n",
    "        # Compile pattern for efficiency\n",
    "        compiled_pattern = re.compile(pattern)\n",
    "        pattern_display = pattern_name if pattern_name else pattern[:50]\n",
    "        \n",
    "        # Get non-null values as strings\n",
    "        text_series = df[column].dropna().astype(str)\n",
    "        total_values = len(text_series)\n",
    "        \n",
    "        if total_values == 0:\n",
    "            print(f\"Warning: No non-null values found in column '{column}'\")\n",
    "            return {'valid': True, 'matches': 0, 'total': 0, 'violations': []}\n",
    "        \n",
    "        # Test each value against pattern\n",
    "        matches = []\n",
    "        violations = []\n",
    "        \n",
    "        for idx, value in text_series.items():\n",
    "            is_match = bool(compiled_pattern.fullmatch(str(value)))\n",
    "            matches.append(is_match)\n",
    "            \n",
    "            if not is_match:\n",
    "                violations.append({\n",
    "                    'index': idx,\n",
    "                    'value': str(value)[:100],  # Truncate long values\n",
    "                    'issue': 'Pattern mismatch'\n",
    "                })\n",
    "        \n",
    "        # Calculate statistics\n",
    "        match_count = sum(matches)\n",
    "        violation_count = len(violations)\n",
    "        match_rate = match_count / total_values if total_values > 0 else 0\n",
    "        \n",
    "        # Create validation flag column\n",
    "        result_df = df.copy()\n",
    "        validation_col_name = f\"{column}_pattern_valid\"\n",
    "        \n",
    "        # Initialize with True (valid) for all rows\n",
    "        result_df[validation_col_name] = True\n",
    "        \n",
    "        # Set False for violations\n",
    "        if violations:\n",
    "            violation_indices = [v['index'] for v in violations]\n",
    "            result_df.loc[violation_indices, validation_col_name] = False\n",
    "        \n",
    "        # Report results\n",
    "        print(f\"=== PATTERN VALIDATION RESULTS ===\")\n",
    "        print(f\"Column: {column}\")\n",
    "        print(f\"Pattern: {pattern_display}\")\n",
    "        print(f\"Total values checked: {total_values}\")\n",
    "        print(f\"Valid matches: {match_count} ({match_rate:.1%})\")\n",
    "        print(f\"Violations found: {violation_count}\")\n",
    "        \n",
    "        if violations:\n",
    "            print(f\"\\\\nValidation flag column created: {validation_col_name}\")\n",
    "            print(\"Sample violations:\")\n",
    "            for violation in violations[:5]:  # Show first 5 violations\n",
    "                print(f\"  Row {violation['index']}: '{violation['value']}'\")\n",
    "            \n",
    "            if len(violations) > 5:\n",
    "                print(f\"  ... and {len(violations) - 5} more violations\")\n",
    "        \n",
    "        # Common pattern suggestions if many violations\n",
    "        if violation_count > total_values * 0.3:  # More than 30% violations\n",
    "            print(\"\\\\nSuggestion: High violation rate detected.\")\n",
    "            print(\"Consider reviewing the pattern or cleaning the data first.\")\n",
    "            \n",
    "            # Show some common characteristics of violations\n",
    "            violation_values = [v['value'] for v in violations[:10]]\n",
    "            print(\"Sample violation values for pattern analysis:\")\n",
    "            for val in violation_values:\n",
    "                print(f\"  '{val}'\")\n",
    "        \n",
    "        # Return comprehensive results\n",
    "        return {\n",
    "            'valid': True,\n",
    "            'column': column,\n",
    "            'pattern': pattern,\n",
    "            'pattern_name': pattern_name,\n",
    "            'total_values': total_values,\n",
    "            'matches': match_count,\n",
    "            'violations': violation_count,\n",
    "            'match_rate': match_rate,\n",
    "            'violation_details': violations[:100],  # Limit to first 100\n",
    "            'result_df': result_df,\n",
    "            'validation_column': validation_col_name\n",
    "        }\n",
    "        \n",
    "    except re.error as e:\n",
    "        print(f\"Error: Invalid regex pattern '{pattern}': {e}\")\n",
    "        return {'valid': False, 'error': f'Invalid regex: {e}'}\n",
    "    except Exception as e:\n",
    "        print(f\"Error during pattern validation: {e}\")\n",
    "        return {'valid': False, 'error': str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c99ee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_docs = \"\"\" Helper functions available:\n",
    "- clean_text_basic(df, columns=None): Remove extra whitespace and standardize encoding for text columns. Returns DataFrame with cleaned text.\n",
    "- standardize_case(df, columns=None, case='title'): Standardize case formatting ('upper', 'lower', 'title', 'sentence', 'proper'). Returns DataFrame with standardized case.\n",
    "- remove_special_chars(df, columns=None, keep_patterns=None): Remove special characters with customizable keep-lists. Returns DataFrame with cleaned text.\n",
    "- standardize_categorical_values(df, column, mapping_dict=None, similarity_threshold=0.8): Map variants to standard values using fuzzy matching. Returns DataFrame with standardized values.\n",
    "- extract_numeric_from_text(df, columns=None, create_new_columns=True): Extract numbers from mixed text columns. Returns DataFrame with numeric columns.\n",
    "- validate_text_patterns(df, column, pattern, pattern_name=None): Validate against regex patterns and create validation flags. Returns results dict and DataFrame.\n",
    "\n",
    "Examples:\n",
    "- \"Clean text columns\" -> df = clean_text_basic(df)\n",
    "- \"Standardize city names\" -> df = standardize_categorical_values(df, 'city')\n",
    "- \"Convert to title case\" -> df = standardize_case(df, case='title')\n",
    "- \"Remove special characters\" -> df = remove_special_chars(df)\n",
    "- \"Extract numbers from product codes\" -> df = extract_numeric_from_text(df, ['product_code'])\n",
    "- \"Validate email format\" -> result = validate_text_patterns(df, 'email', r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$', 'email')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b02315b",
   "metadata": {},
   "source": [
    "# **MAIN FEATURE FUNCTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "881f15c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(df, user_query):\n",
    "    \"\"\"\n",
    "    Main function that gets called by the main router.\n",
    "    MUST take (df, user_query) and return df\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create message chain\n",
    "    messages = []\n",
    "    messages.append(SystemMessage(content=helper_docs))\n",
    "    messages.append(SystemMessage(content=f\"\"\"\n",
    "    You are a data cleaning agent focused on text processing and standardization.\n",
    "    \n",
    "    Dataset info: Shape: {df.shape}, Sample: {df.head(3).to_string()}\n",
    "\n",
    "    Libraries available:\n",
    "    - pd (pandas), np (numpy)\n",
    "    - math, re, datetime, unicodedata\n",
    "    - difflib.get_close_matches\n",
    "    - All helper functions listed above\n",
    "    \n",
    "    Rules:\n",
    "    - Return only executable Python code, no explanations, no markdown blocks\n",
    "    - Use helper functions for text processing tasks when appropriate\n",
    "    - ASSUME \\\"df\\\" IS ALREADY DEFINED\n",
    "    - For cleaning operations, use helper functions that modify DataFrame (clean_text_basic, standardize_case, etc.)\n",
    "    - For validation, use validate_text_patterns which returns results dict\n",
    "    - ALWAYS assign the result back to df when modifying: df = clean_text_basic(df)\n",
    "    - For validation results, use: result = validate_text_patterns(df, 'column', 'pattern'); df = result['result_df'] if result['valid'] else df\n",
    "    - In order to generate a response/message to the user use print statements\n",
    "    print(\"message\")\n",
    "    - Write a detailed print message to summarise actions taken and reasons\n",
    "    \n",
    "    Common query patterns:\n",
    "    - \"Clean text\" or \"Basic cleaning\" -> df = clean_text_basic(df)\n",
    "    - \"Standardize case\" or \"Fix capitalization\" -> df = standardize_case(df, case='title')\n",
    "    - \"Remove special characters\" -> df = remove_special_chars(df)\n",
    "    - \"Standardize city names\" or \"Fix categorical values\" -> df = standardize_categorical_values(df, 'column_name')\n",
    "    - \"Extract numbers\" or \"Get numeric values\" -> df = extract_numeric_from_text(df)\n",
    "    - \"Validate email\" or \"Check pattern\" -> result = validate_text_patterns(df, 'email', r'pattern'); df = result['result_df'] if result['valid'] else df\n",
    "    - \"Clean all text\" -> df = clean_text_basic(df); df = standardize_case(df); df = remove_special_chars(df)\n",
    "    \"\"\"))\n",
    "    messages.append(HumanMessage(content=f\"User request: {user_query}\"))\n",
    "    \n",
    "    # Call LLM with message chain\n",
    "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")\n",
    "    response = llm.invoke(messages)\n",
    "    generated_code = response.content.strip()\n",
    "    \n",
    "    # Execute code\n",
    "    try:\n",
    "        original_df = df.copy()\n",
    "        # Create local namespace with our variables\n",
    "        local_vars = {\n",
    "            'df': df.copy(),\n",
    "            'original_df': original_df,\n",
    "            'pd': pd,\n",
    "            'np': np,\n",
    "            're': re,\n",
    "            'unicodedata': unicodedata,\n",
    "            'get_close_matches': get_close_matches,\n",
    "            'clean_text_basic': clean_text_basic,\n",
    "            'standardize_case': standardize_case,\n",
    "            'remove_special_chars': remove_special_chars,\n",
    "            'standardize_categorical_values': standardize_categorical_values,\n",
    "            'extract_numeric_from_text': extract_numeric_from_text,\n",
    "            'validate_text_patterns': validate_text_patterns,\n",
    "            'print': print\n",
    "        }\n",
    "        \n",
    "        exec(generated_code, globals(), local_vars)\n",
    "        return local_vars['df']\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"Generated Code:{generated_code}\")\n",
    "        return original_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30928b6",
   "metadata": {},
   "source": [
    "# **Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b618978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Enter CSV filename from \"datasets\" folder\n",
    "# dataset_name = \"Life Expectancy Data.csv\"\n",
    "\n",
    "# # Build CSV path (to avoid import errors)\n",
    "# load_dotenv()\n",
    "# PROJECT_ROOT = Path(os.environ[\"PROJECT_ROOT\"])\n",
    "# path = PROJECT_ROOT / \"datasets\" / dataset_name\n",
    "\n",
    "# df = pd.read_csv(path)\n",
    "# test_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9610f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"clean text\"\n",
    "# result = text_processing(test_df, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f9c065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df.info()\n",
    "# print(\"---------------------------------\")\n",
    "# result.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
