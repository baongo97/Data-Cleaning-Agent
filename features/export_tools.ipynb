{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "691226d2",
   "metadata": {},
   "source": [
    "## **Feature:** Export & Formatting Tools\n",
    "\n",
    "**Names:** Gia Bao Ngo\n",
    "\n",
    "### **What it does**\n",
    "Provides comprehensive export and formatting capabilities for data cleaning results. Includes enhanced export options with proper formatting, automatic data dictionary generation, comprehensive summary reports, Excel-optimized exports with multiple sheets, and variable codebook creation for documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff1f7555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dotenv\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Get API Key\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"OpenAI API Key not found\")\n",
    "\n",
    "# Import libraries\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Additional imports for export and formatting\n",
    "import math\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from typing import Dict, List, Optional, Union\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For Excel formatting\n",
    "try:\n",
    "    import openpyxl\n",
    "    from openpyxl.styles import Font, Alignment, PatternFill, Border, Side\n",
    "    from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "    EXCEL_AVAILABLE = True\n",
    "except ImportError:\n",
    "    EXCEL_AVAILABLE = False\n",
    "    print(\"Warning: openpyxl not available. Excel formatting will be limited.\")\n",
    "\n",
    "# Langchain imports\n",
    "from langchain_openai import ChatOpenAI  \n",
    "from langchain.schema import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a88657",
   "metadata": {},
   "source": [
    "### **Helper Functions**\n",
    "- `export_with_formatting(df, filename, format='csv', options=None)` - Enhanced export options with custom formatting\n",
    "- `create_data_dictionary(df, filename=None)` - Generate comprehensive data documentation  \n",
    "- `export_summary_report(df, filename=None)` - Create detailed data summary report\n",
    "- `format_for_excel(df, filename, sheets=None)` - Excel-optimized export with formatting\n",
    "- `create_codebook(df, filename=None)` - Generate variable codebook for documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60424d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_with_formatting(df, filename, format='csv', options=None):\n",
    "    \"\"\"\n",
    "    Enhanced export with custom formatting and encoding options.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame to export\n",
    "    - filename: output filename (extension will be added if not present)\n",
    "    - format: export format ('csv', 'json', 'parquet', 'pickle', 'html')\n",
    "    - options: dict of format-specific options\n",
    "    \n",
    "    Returns:\n",
    "    - Success status and file path\n",
    "    \"\"\"\n",
    "    if options is None:\n",
    "        options = {}\n",
    "    \n",
    "    # Ensure filename has correct extension\n",
    "    file_path = Path(filename)\n",
    "    if format == 'csv' and file_path.suffix.lower() != '.csv':\n",
    "        file_path = file_path.with_suffix('.csv')\n",
    "    elif format == 'json' and file_path.suffix.lower() != '.json':\n",
    "        file_path = file_path.with_suffix('.json')\n",
    "    elif format == 'parquet' and file_path.suffix.lower() != '.parquet':\n",
    "        file_path = file_path.with_suffix('.parquet')\n",
    "    elif format == 'pickle' and file_path.suffix.lower() not in ['.pkl', '.pickle']:\n",
    "        file_path = file_path.with_suffix('.pkl')\n",
    "    elif format == 'html' and file_path.suffix.lower() != '.html':\n",
    "        file_path = file_path.with_suffix('.html')\n",
    "    \n",
    "    try:\n",
    "        # Default options for each format\n",
    "        default_options = {\n",
    "            'csv': {'index': False, 'encoding': 'utf-8'},\n",
    "            'json': {'orient': 'records', 'indent': 2},\n",
    "            'parquet': {'index': False, 'compression': 'snappy'},\n",
    "            'pickle': {'protocol': 4},\n",
    "            'html': {'index': False, 'escape': False, 'table_id': 'data_table'}\n",
    "        }\n",
    "        \n",
    "        # Merge with user options\n",
    "        export_options = default_options.get(format, {})\n",
    "        export_options.update(options)\n",
    "        \n",
    "        # Export based on format\n",
    "        if format == 'csv':\n",
    "            df.to_csv(file_path, **export_options)\n",
    "            print(f\"CSV exported to {file_path}\")\n",
    "            print(f\"Encoding: {export_options.get('encoding', 'utf-8')}\")\n",
    "            \n",
    "        elif format == 'json':\n",
    "            df.to_json(file_path, **export_options)\n",
    "            print(f\"JSON exported to {file_path}\")\n",
    "            print(f\"Format: {export_options.get('orient', 'records')}\")\n",
    "            \n",
    "        elif format == 'parquet':\n",
    "            df.to_parquet(file_path, **export_options)\n",
    "            print(f\"Parquet exported to {file_path}\")\n",
    "            print(f\"Compression: {export_options.get('compression', 'snappy')}\")\n",
    "            \n",
    "        elif format == 'pickle':\n",
    "            df.to_pickle(file_path, **export_options)\n",
    "            print(f\"Pickle exported to {file_path}\")\n",
    "            \n",
    "        elif format == 'html':\n",
    "            # Enhanced HTML with basic styling\n",
    "            html_options = export_options.copy()\n",
    "            if 'classes' not in html_options:\n",
    "                html_options['classes'] = 'table table-striped table-bordered'\n",
    "            \n",
    "            df.to_html(file_path, **html_options)\n",
    "            print(f\"HTML exported to {file_path}\")\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported format: {format}\")\n",
    "        \n",
    "        # File size info\n",
    "        file_size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "        print(f\"File size: {file_size_mb:.2f} MB\")\n",
    "        print(f\"Data shape: {df.shape}\")\n",
    "        \n",
    "        return True, str(file_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Export failed: {e}\")\n",
    "        return False, str(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a090e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_dictionary(df, filename=None):\n",
    "    \"\"\"\n",
    "    Generate comprehensive data documentation including types, statistics, and quality metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame to document\n",
    "    - filename: output filename (optional, defaults to 'data_dictionary.csv')\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with data dictionary information\n",
    "    \"\"\"\n",
    "    if filename is None:\n",
    "        filename = 'data_dictionary.csv'\n",
    "    \n",
    "    dictionary_data = []\n",
    "    \n",
    "    print(\"=== GENERATING DATA DICTIONARY ===\")\n",
    "    print(f\"Analyzing {len(df.columns)} columns...\")\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_info = {\n",
    "            'Column_Name': col,\n",
    "            'Data_Type': str(df[col].dtype),\n",
    "            'Non_Null_Count': df[col].count(),\n",
    "            'Null_Count': df[col].isnull().sum(),\n",
    "            'Null_Percentage': (df[col].isnull().sum() / len(df)) * 100,\n",
    "            'Unique_Values': df[col].nunique(),\n",
    "            'Cardinality_Ratio': df[col].nunique() / len(df),\n",
    "            'Memory_Usage_MB': df[col].memory_usage(deep=True) / (1024 * 1024)\n",
    "        }\n",
    "        \n",
    "        # Type-specific statistics\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            # Numeric statistics\n",
    "            non_null_data = df[col].dropna()\n",
    "            if len(non_null_data) > 0:\n",
    "                col_info.update({\n",
    "                    'Min_Value': non_null_data.min(),\n",
    "                    'Max_Value': non_null_data.max(),\n",
    "                    'Mean': non_null_data.mean(),\n",
    "                    'Median': non_null_data.median(),\n",
    "                    'Std_Deviation': non_null_data.std(),\n",
    "                    'Sample_Values': str(non_null_data.head(3).tolist())\n",
    "                })\n",
    "            else:\n",
    "                col_info.update({\n",
    "                    'Min_Value': None, 'Max_Value': None, 'Mean': None,\n",
    "                    'Median': None, 'Std_Deviation': None, 'Sample_Values': '[]'\n",
    "                })\n",
    "        \n",
    "        elif pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            # Datetime statistics\n",
    "            non_null_data = df[col].dropna()\n",
    "            if len(non_null_data) > 0:\n",
    "                col_info.update({\n",
    "                    'Min_Value': non_null_data.min(),\n",
    "                    'Max_Value': non_null_data.max(),\n",
    "                    'Sample_Values': str(non_null_data.head(3).tolist())\n",
    "                })\n",
    "            else:\n",
    "                col_info.update({\n",
    "                    'Min_Value': None, 'Max_Value': None, 'Sample_Values': '[]'\n",
    "                })\n",
    "        \n",
    "        else:\n",
    "            # Categorical/Text statistics\n",
    "            non_null_data = df[col].dropna()\n",
    "            if len(non_null_data) > 0:\n",
    "                # Most common values\n",
    "                value_counts = non_null_data.value_counts()\n",
    "                most_common = value_counts.head(3).index.tolist()\n",
    "                \n",
    "                col_info.update({\n",
    "                    'Most_Common_Values': str(most_common),\n",
    "                    'Most_Common_Counts': str(value_counts.head(3).values.tolist()),\n",
    "                    'Sample_Values': str(non_null_data.head(3).tolist())\n",
    "                })\n",
    "                \n",
    "                # Text length statistics for string data\n",
    "                if df[col].dtype == 'object':\n",
    "                    try:\n",
    "                        text_lengths = non_null_data.astype(str).str.len()\n",
    "                        col_info.update({\n",
    "                            'Min_Text_Length': text_lengths.min(),\n",
    "                            'Max_Text_Length': text_lengths.max(),\n",
    "                            'Avg_Text_Length': text_lengths.mean()\n",
    "                        })\n",
    "                    except:\n",
    "                        pass\n",
    "            else:\n",
    "                col_info.update({\n",
    "                    'Most_Common_Values': '[]',\n",
    "                    'Most_Common_Counts': '[]',\n",
    "                    'Sample_Values': '[]'\n",
    "                })\n",
    "        \n",
    "        # Data quality assessment\n",
    "        quality_issues = []\n",
    "        if col_info['Null_Percentage'] > 50:\n",
    "            quality_issues.append('High_Null_Rate')\n",
    "        if col_info['Cardinality_Ratio'] == 1.0:\n",
    "            quality_issues.append('All_Unique')\n",
    "        if col_info['Cardinality_Ratio'] < 0.01 and col_info['Unique_Values'] > 1:\n",
    "            quality_issues.append('Low_Cardinality')\n",
    "        \n",
    "        col_info['Quality_Flags'] = ', '.join(quality_issues) if quality_issues else 'Good'\n",
    "        \n",
    "        # Suggestions for optimization/cleaning\n",
    "        suggestions = []\n",
    "        if pd.api.types.is_numeric_dtype(df[col]) and df[col].dtype in ['int64', 'float64']:\n",
    "            suggestions.append('Consider_Downcasting')\n",
    "        if df[col].dtype == 'object' and col_info['Cardinality_Ratio'] < 0.05:\n",
    "            suggestions.append('Convert_To_Category')\n",
    "        if col_info['Null_Percentage'] > 5:\n",
    "            suggestions.append('Handle_Missing_Values')\n",
    "        \n",
    "        col_info['Optimization_Suggestions'] = ', '.join(suggestions) if suggestions else 'None'\n",
    "        \n",
    "        dictionary_data.append(col_info)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    dict_df = pd.DataFrame(dictionary_data)\n",
    "    \n",
    "    # Export to file\n",
    "    try:\n",
    "        dict_df.to_csv(filename, index=False, encoding='utf-8')\n",
    "        print(f\"Data dictionary saved to: {filename}\")\n",
    "        print(f\"Dictionary contains {len(dict_df)} column descriptions\")\n",
    "        \n",
    "        # Summary statistics\n",
    "        total_memory = dict_df['Memory_Usage_MB'].sum()\n",
    "        avg_null_rate = dict_df['Null_Percentage'].mean()\n",
    "        high_null_cols = (dict_df['Null_Percentage'] > 20).sum()\n",
    "        \n",
    "        print(f\"Total dataset memory: {total_memory:.2f} MB\")\n",
    "        print(f\"Average null rate: {avg_null_rate:.1f}%\")\n",
    "        print(f\"Columns with >20% nulls: {high_null_cols}\")\n",
    "        \n",
    "        return dict_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save data dictionary: {e}\")\n",
    "        return dict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jbdhuq3mkph",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_summary_report(df, filename=None):\n",
    "    \"\"\"\n",
    "    Create comprehensive data summary report with statistics and visualizations.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame to analyze\n",
    "    - filename: output filename (optional, defaults to 'summary_report.html')\n",
    "    \n",
    "    Returns:\n",
    "    - HTML content string\n",
    "    \"\"\"\n",
    "    if filename is None:\n",
    "        filename = 'summary_report.html'\n",
    "    \n",
    "    print(\"=== GENERATING SUMMARY REPORT ===\")\n",
    "    print(f\"Analyzing dataset with shape: {df.shape}\")\n",
    "    \n",
    "    # Basic dataset information\n",
    "    total_cells = df.shape[0] * df.shape[1]\n",
    "    total_missing = df.isnull().sum().sum()\n",
    "    missing_percentage = (total_missing / total_cells) * 100\n",
    "    memory_usage = df.memory_usage(deep=True).sum() / (1024 * 1024)\n",
    "    \n",
    "    # Data type summary\n",
    "    dtype_counts = df.dtypes.value_counts()\n",
    "    \n",
    "    # Missing data by column\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_data = missing_data[missing_data > 0].sort_values(ascending=False)\n",
    "    \n",
    "    # Numeric columns summary\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    numeric_summary = df[numeric_cols].describe() if len(numeric_cols) > 0 else pd.DataFrame()\n",
    "    \n",
    "    # Categorical columns summary\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    categorical_summary = []\n",
    "    \n",
    "    for col in categorical_cols[:10]:  # Limit to first 10 for performance\n",
    "        col_summary = {\n",
    "            'Column': col,\n",
    "            'Unique_Values': df[col].nunique(),\n",
    "            'Most_Common': df[col].mode().iloc[0] if not df[col].mode().empty else 'N/A',\n",
    "            'Most_Common_Count': df[col].value_counts().iloc[0] if len(df[col].value_counts()) > 0 else 0\n",
    "        }\n",
    "        categorical_summary.append(col_summary)\n",
    "    \n",
    "    categorical_df = pd.DataFrame(categorical_summary)\n",
    "    \n",
    "    # Generate HTML content\n",
    "    html_content = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>Data Summary Report</title>\n",
    "        <style>\n",
    "            body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
    "            table {{ border-collapse: collapse; width: 100%; margin: 10px 0; }}\n",
    "            th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
    "            th {{ background-color: #f2f2f2; }}\n",
    "            .section {{ margin: 30px 0; }}\n",
    "            .metric {{ background-color: #f8f9fa; padding: 15px; margin: 10px; border-radius: 5px; }}\n",
    "            .metric-value {{ font-size: 24px; font-weight: bold; color: #007bff; }}\n",
    "            .metric-label {{ font-size: 14px; color: #6c757d; }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Data Summary Report</h1>\n",
    "        <p>Generated on: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\n",
    "        \n",
    "        <div class=\"section\">\n",
    "            <h2>Dataset Overview</h2>\n",
    "            <div class=\"metric\">\n",
    "                <div class=\"metric-value\">{df.shape[0]:,}</div>\n",
    "                <div class=\"metric-label\">Rows</div>\n",
    "            </div>\n",
    "            <div class=\"metric\">\n",
    "                <div class=\"metric-value\">{df.shape[1]:,}</div>\n",
    "                <div class=\"metric-label\">Columns</div>\n",
    "            </div>\n",
    "            <div class=\"metric\">\n",
    "                <div class=\"metric-value\">{memory_usage:.2f} MB</div>\n",
    "                <div class=\"metric-label\">Memory Usage</div>\n",
    "            </div>\n",
    "            <div class=\"metric\">\n",
    "                <div class=\"metric-value\">{missing_percentage:.1f}%</div>\n",
    "                <div class=\"metric-label\">Missing Data</div>\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"section\">\n",
    "            <h2>Data Types Distribution</h2>\n",
    "            {dtype_counts.to_frame('Count').to_html(classes='table')}\n",
    "        </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add missing data section if there are missing values\n",
    "    if len(missing_data) > 0:\n",
    "        html_content += f\"\"\"\n",
    "        <div class=\"section\">\n",
    "            <h2>Missing Data by Column</h2>\n",
    "            {missing_data.head(20).to_frame('Missing Count').to_html(classes='table')}\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    \n",
    "    # Add numeric summary if numeric columns exist\n",
    "    if not numeric_summary.empty:\n",
    "        html_content += f\"\"\"\n",
    "        <div class=\"section\">\n",
    "            <h2>Numeric Columns Summary</h2>\n",
    "            {numeric_summary.round(2).to_html(classes='table')}\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    \n",
    "    # Add categorical summary if categorical columns exist\n",
    "    if not categorical_df.empty:\n",
    "        html_content += f\"\"\"\n",
    "        <div class=\"section\">\n",
    "            <h2>Categorical Columns Summary (Top 10)</h2>\n",
    "            {categorical_df.to_html(classes='table', index=False)}\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    \n",
    "    # Column details table\n",
    "    column_details = []\n",
    "    for col in df.columns:\n",
    "        details = {\n",
    "            'Column': col,\n",
    "            'Type': str(df[col].dtype),\n",
    "            'Non-Null Count': df[col].count(),\n",
    "            'Unique Values': df[col].nunique(),\n",
    "            'Memory (MB)': df[col].memory_usage(deep=True) / (1024 * 1024)\n",
    "        }\n",
    "        column_details.append(details)\n",
    "    \n",
    "    column_details_df = pd.DataFrame(column_details)\n",
    "    \n",
    "    html_content += f\"\"\"\n",
    "        <div class=\"section\">\n",
    "            <h2>Column Details</h2>\n",
    "            {column_details_df.round(3).to_html(classes='table', index=False)}\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"section\">\n",
    "            <h2>Data Quality Insights</h2>\n",
    "            <ul>\n",
    "                <li>Columns with high missing data: {len(missing_data[missing_data > len(df) * 0.2])}</li>\n",
    "                <li>Potential categorical columns (low cardinality): {len([c for c in df.columns if df[c].nunique() / len(df) < 0.05 and df[c].dtype == 'object'])}</li>\n",
    "                <li>High cardinality columns: {len([c for c in df.columns if df[c].nunique() > len(df) * 0.5])}</li>\n",
    "                <li>Memory optimization opportunities: {len([c for c in df.columns if df[c].dtype in ['int64', 'float64']])}</li>\n",
    "            </ul>\n",
    "        </div>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Save to file\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(html_content)\n",
    "        \n",
    "        print(f\"Summary report saved to: {filename}\")\n",
    "        print(f\"Report includes {len(df.columns)} column analyses\")\n",
    "        print(f\"File size: {Path(filename).stat().st_size / 1024:.1f} KB\")\n",
    "        \n",
    "        return html_content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save summary report: {e}\")\n",
    "        return html_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "glxwd08fi77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_excel(df, filename, sheets=None):\n",
    "    \"\"\"\n",
    "    Excel-optimized export with formatting and multiple sheets.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame to export\n",
    "    - filename: output Excel filename (.xlsx will be added if not present)\n",
    "    - sheets: dict of {sheet_name: dataframe} for multiple sheets, or None for single sheet\n",
    "    \n",
    "    Returns:\n",
    "    - Success status and file path\n",
    "    \"\"\"\n",
    "    # Ensure .xlsx extension\n",
    "    file_path = Path(filename)\n",
    "    if file_path.suffix.lower() != '.xlsx':\n",
    "        file_path = file_path.with_suffix('.xlsx')\n",
    "    \n",
    "    print(\"=== EXPORTING TO EXCEL ===\")\n",
    "    print(f\"Target file: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        if sheets is None:\n",
    "            # Single sheet export\n",
    "            sheets = {'Data': df}\n",
    "        \n",
    "        with pd.ExcelWriter(file_path, engine='openpyxl') as writer:\n",
    "            for sheet_name, sheet_df in sheets.items():\n",
    "                print(f\"Writing sheet: {sheet_name} (shape: {sheet_df.shape})\")\n",
    "                \n",
    "                # Write DataFrame to Excel\n",
    "                sheet_df.to_excel(writer, sheet_name=sheet_name, index=False, startrow=1)\n",
    "                \n",
    "                # Get the workbook and worksheet for formatting\n",
    "                if EXCEL_AVAILABLE:\n",
    "                    workbook = writer.book\n",
    "                    worksheet = writer.sheets[sheet_name]\n",
    "                    \n",
    "                    # Add title row\n",
    "                    worksheet['A1'] = f\"Data Export - {sheet_name}\"\n",
    "                    title_cell = worksheet['A1']\n",
    "                    title_cell.font = Font(bold=True, size=14)\n",
    "                    title_cell.fill = PatternFill(start_color=\"366092\", end_color=\"366092\", fill_type=\"solid\")\n",
    "                    title_cell.font = Font(bold=True, size=14, color=\"FFFFFF\")\n",
    "                    \n",
    "                    # Format header row\n",
    "                    header_row = 2\n",
    "                    for col_num, column_title in enumerate(sheet_df.columns, 1):\n",
    "                        cell = worksheet.cell(row=header_row, column=col_num)\n",
    "                        cell.font = Font(bold=True)\n",
    "                        cell.fill = PatternFill(start_color=\"D9E1F2\", end_color=\"D9E1F2\", fill_type=\"solid\")\n",
    "                        cell.alignment = Alignment(horizontal=\"center\")\n",
    "                        \n",
    "                        # Add borders\n",
    "                        thin_border = Border(\n",
    "                            left=Side(style='thin'),\n",
    "                            right=Side(style='thin'),\n",
    "                            top=Side(style='thin'),\n",
    "                            bottom=Side(style='thin')\n",
    "                        )\n",
    "                        cell.border = thin_border\n",
    "                    \n",
    "                    # Auto-adjust column widths\n",
    "                    for column in worksheet.columns:\n",
    "                        max_length = 0\n",
    "                        column_letter = column[0].column_letter\n",
    "                        \n",
    "                        for cell in column:\n",
    "                            try:\n",
    "                                if len(str(cell.value)) > max_length:\n",
    "                                    max_length = len(str(cell.value))\n",
    "                            except:\n",
    "                                pass\n",
    "                        \n",
    "                        adjusted_width = min(max_length + 2, 50)  # Cap at 50 characters\n",
    "                        worksheet.column_dimensions[column_letter].width = adjusted_width\n",
    "                    \n",
    "                    # Format data rows\n",
    "                    for row_num in range(3, len(sheet_df) + 3):  # Starting from row 3 (after title and header)\n",
    "                        for col_num in range(1, len(sheet_df.columns) + 1):\n",
    "                            cell = worksheet.cell(row=row_num, column=col_num)\n",
    "                            \n",
    "                            # Add borders\n",
    "                            cell.border = thin_border\n",
    "                            \n",
    "                            # Format specific data types\n",
    "                            if col_num <= len(sheet_df.columns):\n",
    "                                col_name = sheet_df.columns[col_num - 1]\n",
    "                                if pd.api.types.is_numeric_dtype(sheet_df[col_name]):\n",
    "                                    cell.alignment = Alignment(horizontal=\"right\")\n",
    "                                    if sheet_df[col_name].dtype in ['float32', 'float64']:\n",
    "                                        cell.number_format = '0.00'\n",
    "                                elif pd.api.types.is_datetime64_any_dtype(sheet_df[col_name]):\n",
    "                                    cell.number_format = 'yyyy-mm-dd'\n",
    "                                    cell.alignment = Alignment(horizontal=\"center\")\n",
    "                    \n",
    "                    # Add data validation and filtering if not too many rows\n",
    "                    if len(sheet_df) <= 1000:\n",
    "                        # Add autofilter\n",
    "                        worksheet.auto_filter.ref = f\"A{header_row}:{worksheet.max_column}{worksheet.max_row}\"\n",
    "                    \n",
    "                    # Freeze header row\n",
    "                    worksheet.freeze_panes = f\"A{header_row + 1}\"\n",
    "                    \n",
    "                else:\n",
    "                    print(\"Advanced Excel formatting not available (openpyxl not installed)\")\n",
    "        \n",
    "        # File statistics\n",
    "        file_size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "        total_rows = sum(len(sheet_df) for sheet_df in sheets.values())\n",
    "        \n",
    "        print(f\"Excel export successful!\")\n",
    "        print(f\"File size: {file_size_mb:.2f} MB\")\n",
    "        print(f\"Sheets: {len(sheets)}\")\n",
    "        print(f\"Total rows: {total_rows:,}\")\n",
    "        print(f\"Features added: Headers, formatting, auto-width, borders\")\n",
    "        \n",
    "        return True, str(file_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Excel export failed: {e}\")\n",
    "        print(\"Tip: Ensure openpyxl is installed for advanced Excel features\")\n",
    "        \n",
    "        # Fallback to basic export\n",
    "        try:\n",
    "            if len(sheets) == 1:\n",
    "                list(sheets.values())[0].to_excel(file_path, index=False)\n",
    "                print(f\"Basic Excel export successful to {file_path}\")\n",
    "                return True, str(file_path)\n",
    "            else:\n",
    "                print(\"Multiple sheets require openpyxl. Export failed.\")\n",
    "                return False, str(file_path)\n",
    "        except Exception as fallback_error:\n",
    "            print(f\"Fallback export also failed: {fallback_error}\")\n",
    "            return False, str(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ko2wt16q8i",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_codebook(df, filename=None):\n",
    "    \"\"\"\n",
    "    Generate variable codebook for documentation with detailed metadata.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame to document\n",
    "    - filename: output filename (optional, defaults to 'codebook.json')\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with codebook information\n",
    "    \"\"\"\n",
    "    if filename is None:\n",
    "        filename = 'codebook.json'\n",
    "    \n",
    "    print(\"=== GENERATING CODEBOOK ===\")\n",
    "    print(f\"Creating codebook for {len(df.columns)} variables...\")\n",
    "    \n",
    "    codebook = {\n",
    "        'dataset_info': {\n",
    "            'name': 'Dataset',\n",
    "            'description': 'Automatically generated codebook',\n",
    "            'shape': list(df.shape),\n",
    "            'memory_usage_mb': df.memory_usage(deep=True).sum() / (1024 * 1024),\n",
    "            'creation_date': datetime.datetime.now().isoformat(),\n",
    "            'total_missing_values': int(df.isnull().sum().sum()),\n",
    "            'missing_percentage': float((df.isnull().sum().sum() / (df.shape[0] * df.shape[1])) * 100)\n",
    "        },\n",
    "        'variables': {}\n",
    "    }\n",
    "    \n",
    "    for col in df.columns:\n",
    "        print(f\"Processing variable: {col}\")\n",
    "        \n",
    "        variable_info = {\n",
    "            'name': col,\n",
    "            'position': int(df.columns.get_loc(col)),\n",
    "            'data_type': str(df[col].dtype),\n",
    "            'python_type': str(type(df[col].iloc[0]).__name__) if len(df[col].dropna()) > 0 else 'unknown',\n",
    "            'non_null_count': int(df[col].count()),\n",
    "            'null_count': int(df[col].isnull().sum()),\n",
    "            'null_percentage': float((df[col].isnull().sum() / len(df)) * 100),\n",
    "            'unique_values': int(df[col].nunique()),\n",
    "            'cardinality_ratio': float(df[col].nunique() / len(df)),\n",
    "            'memory_usage_bytes': int(df[col].memory_usage(deep=True))\n",
    "        }\n",
    "        \n",
    "        # Type-specific information\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            non_null_data = df[col].dropna()\n",
    "            if len(non_null_data) > 0:\n",
    "                variable_info.update({\n",
    "                    'variable_type': 'numeric',\n",
    "                    'min_value': float(non_null_data.min()) if pd.api.types.is_float_dtype(non_null_data) else int(non_null_data.min()),\n",
    "                    'max_value': float(non_null_data.max()) if pd.api.types.is_float_dtype(non_null_data) else int(non_null_data.max()),\n",
    "                    'mean': float(non_null_data.mean()),\n",
    "                    'median': float(non_null_data.median()),\n",
    "                    'std_deviation': float(non_null_data.std()),\n",
    "                    'quartiles': {\n",
    "                        'q25': float(non_null_data.quantile(0.25)),\n",
    "                        'q50': float(non_null_data.quantile(0.50)),\n",
    "                        'q75': float(non_null_data.quantile(0.75))\n",
    "                    },\n",
    "                    'skewness': float(non_null_data.skew()),\n",
    "                    'kurtosis': float(non_null_data.kurtosis())\n",
    "                })\n",
    "                \n",
    "                # Detect potential issues\n",
    "                issues = []\n",
    "                if variable_info['skewness'] > 2 or variable_info['skewness'] < -2:\n",
    "                    issues.append('highly_skewed')\n",
    "                if variable_info['std_deviation'] == 0:\n",
    "                    issues.append('no_variation')\n",
    "                if len(non_null_data) != len(df[col]):\n",
    "                    issues.append('missing_values')\n",
    "                \n",
    "                variable_info['data_quality_issues'] = issues\n",
    "            else:\n",
    "                variable_info.update({\n",
    "                    'variable_type': 'numeric',\n",
    "                    'min_value': None,\n",
    "                    'max_value': None,\n",
    "                    'data_quality_issues': ['all_missing']\n",
    "                })\n",
    "        \n",
    "        elif pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            non_null_data = df[col].dropna()\n",
    "            if len(non_null_data) > 0:\n",
    "                variable_info.update({\n",
    "                    'variable_type': 'datetime',\n",
    "                    'min_date': non_null_data.min().isoformat(),\n",
    "                    'max_date': non_null_data.max().isoformat(),\n",
    "                    'date_range_days': int((non_null_data.max() - non_null_data.min()).days)\n",
    "                })\n",
    "            else:\n",
    "                variable_info.update({\n",
    "                    'variable_type': 'datetime',\n",
    "                    'data_quality_issues': ['all_missing']\n",
    "                })\n",
    "        \n",
    "        elif df[col].dtype == 'bool':\n",
    "            value_counts = df[col].value_counts()\n",
    "            variable_info.update({\n",
    "                'variable_type': 'boolean',\n",
    "                'true_count': int(value_counts.get(True, 0)),\n",
    "                'false_count': int(value_counts.get(False, 0)),\n",
    "                'true_percentage': float((value_counts.get(True, 0) / len(df[col].dropna())) * 100) if len(df[col].dropna()) > 0 else 0\n",
    "            })\n",
    "        \n",
    "        else:\n",
    "            # Categorical/Text variable\n",
    "            non_null_data = df[col].dropna()\n",
    "            if len(non_null_data) > 0:\n",
    "                value_counts = non_null_data.value_counts()\n",
    "                \n",
    "                variable_info.update({\n",
    "                    'variable_type': 'categorical' if variable_info['cardinality_ratio'] < 0.5 else 'text',\n",
    "                    'most_frequent_values': value_counts.head(10).index.tolist(),\n",
    "                    'most_frequent_counts': value_counts.head(10).values.tolist(),\n",
    "                    'least_frequent_values': value_counts.tail(5).index.tolist() if len(value_counts) > 5 else [],\n",
    "                    'least_frequent_counts': value_counts.tail(5).values.tolist() if len(value_counts) > 5 else []\n",
    "                })\n",
    "                \n",
    "                # Text length analysis for string data\n",
    "                if df[col].dtype == 'object':\n",
    "                    try:\n",
    "                        text_lengths = non_null_data.astype(str).str.len()\n",
    "                        variable_info.update({\n",
    "                            'text_length_min': int(text_lengths.min()),\n",
    "                            'text_length_max': int(text_lengths.max()),\n",
    "                            'text_length_mean': float(text_lengths.mean()),\n",
    "                            'text_length_std': float(text_lengths.std())\n",
    "                        })\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                # Detect patterns for categorical variables\n",
    "                if variable_info['cardinality_ratio'] < 0.1:\n",
    "                    # Check for potential boolean patterns\n",
    "                    unique_lower = set(non_null_data.astype(str).str.lower().str.strip())\n",
    "                    boolean_patterns = [\n",
    "                        {'yes', 'no'}, {'true', 'false'}, {'y', 'n'}, \n",
    "                        {'1', '0'}, {'on', 'off'}, {'active', 'inactive'}\n",
    "                    ]\n",
    "                    \n",
    "                    for pattern in boolean_patterns:\n",
    "                        if unique_lower == pattern or (len(unique_lower) <= 2 and unique_lower.issubset(pattern)):\n",
    "                            variable_info['suggested_conversion'] = 'boolean'\n",
    "                            break\n",
    "                    \n",
    "                    # Check for percentage patterns\n",
    "                    if any(str(val).endswith('%') for val in non_null_data.head(10)):\n",
    "                        variable_info['suggested_conversion'] = 'percentage_to_float'\n",
    "                    \n",
    "                    # Low cardinality suggestion\n",
    "                    if variable_info['cardinality_ratio'] < 0.05:\n",
    "                        variable_info['suggested_conversion'] = 'category_type'\n",
    "            else:\n",
    "                variable_info.update({\n",
    "                    'variable_type': 'unknown',\n",
    "                    'data_quality_issues': ['all_missing']\n",
    "                })\n",
    "        \n",
    "        # Add recommendations\n",
    "        recommendations = []\n",
    "        if variable_info.get('null_percentage', 0) > 20:\n",
    "            recommendations.append('investigate_missing_pattern')\n",
    "        if variable_info.get('cardinality_ratio', 0) == 1.0:\n",
    "            recommendations.append('consider_removing_unique_identifier')\n",
    "        if variable_info.get('cardinality_ratio', 0) < 0.05 and variable_info.get('variable_type') == 'categorical':\n",
    "            recommendations.append('convert_to_category_dtype')\n",
    "        if variable_info.get('data_type') in ['int64', 'float64']:\n",
    "            recommendations.append('consider_numeric_downcasting')\n",
    "        \n",
    "        variable_info['recommendations'] = recommendations\n",
    "        \n",
    "        codebook['variables'][col] = variable_info\n",
    "    \n",
    "    # Add summary statistics\n",
    "    codebook['summary'] = {\n",
    "        'numeric_variables': len([v for v in codebook['variables'].values() if v.get('variable_type') == 'numeric']),\n",
    "        'categorical_variables': len([v for v in codebook['variables'].values() if v.get('variable_type') == 'categorical']),\n",
    "        'datetime_variables': len([v for v in codebook['variables'].values() if v.get('variable_type') == 'datetime']),\n",
    "        'boolean_variables': len([v for v in codebook['variables'].values() if v.get('variable_type') == 'boolean']),\n",
    "        'text_variables': len([v for v in codebook['variables'].values() if v.get('variable_type') == 'text']),\n",
    "        'variables_with_missing': len([v for v in codebook['variables'].values() if v.get('null_count', 0) > 0]),\n",
    "        'high_cardinality_variables': len([v for v in codebook['variables'].values() if v.get('cardinality_ratio', 0) > 0.8]),\n",
    "        'potential_optimizations': len([v for v in codebook['variables'].values() if len(v.get('recommendations', [])) > 0])\n",
    "    }\n",
    "    \n",
    "    # Save to file\n",
    "    try:\n",
    "        # Save JSON codebook\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(codebook, f, indent=2, ensure_ascii=False, default=str)\n",
    "        \n",
    "        print(f\"Codebook saved to: {filename}\")\n",
    "        \n",
    "        # Also save a human-readable version\n",
    "        readable_filename = Path(filename).with_suffix('.txt')\n",
    "        with open(readable_filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"DATA CODEBOOK\\\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\\\n\\\\n\")\n",
    "            f.write(f\"Dataset Shape: {codebook['dataset_info']['shape']}\\\\n\")\n",
    "            f.write(f\"Memory Usage: {codebook['dataset_info']['memory_usage_mb']:.2f} MB\\\\n\")\n",
    "            f.write(f\"Missing Data: {codebook['dataset_info']['missing_percentage']:.1f}%\\\\n\\\\n\")\n",
    "            \n",
    "            f.write(\"VARIABLES:\\\\n\")\n",
    "            f.write(\"-\" * 30 + \"\\\\n\")\n",
    "            \n",
    "            for var_name, var_info in codebook['variables'].items():\n",
    "                f.write(f\"\\\\n{var_name}\\\\n\")\n",
    "                f.write(f\"  Type: {var_info['variable_type']} ({var_info['data_type']})\\\\n\")\n",
    "                f.write(f\"  Non-null: {var_info['non_null_count']:,} ({100 - var_info['null_percentage']:.1f}%)\\\\n\")\n",
    "                f.write(f\"  Unique: {var_info['unique_values']:,} ({var_info['cardinality_ratio']:.1%} cardinality)\\\\n\")\n",
    "                \n",
    "                if var_info.get('recommendations'):\n",
    "                    f.write(f\"  Recommendations: {', '.join(var_info['recommendations'])}\\\\n\")\n",
    "        \n",
    "        print(f\"Human-readable codebook saved to: {readable_filename}\")\n",
    "        print(f\"Summary: {codebook['summary']['numeric_variables']} numeric, {codebook['summary']['categorical_variables']} categorical, {codebook['summary']['datetime_variables']} datetime variables\")\n",
    "        \n",
    "        return codebook\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save codebook: {e}\")\n",
    "        return codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c99ee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_docs = \"\"\" Helper functions available:\n",
    "- export_with_formatting(df, filename, format='csv', options=None): Enhanced export with custom formatting for CSV, JSON, Parquet, Pickle, HTML formats. Returns success status and file path.\n",
    "- create_data_dictionary(df, filename=None): Generate comprehensive data documentation including types, statistics, quality metrics. Default filename: 'data_dictionary.csv'. Returns DataFrame with analysis.\n",
    "- export_summary_report(df, filename=None): Create detailed HTML summary report with statistics and insights. Default filename: 'summary_report.html'. Returns HTML content.\n",
    "- format_for_excel(df, filename, sheets=None): Excel-optimized export with professional formatting, borders, auto-width, freeze panes. Supports multiple sheets. Returns success status and file path.\n",
    "- create_codebook(df, filename=None): Generate variable codebook with detailed metadata in JSON and TXT formats. Default filename: 'codebook.json'. Returns codebook dictionary.\n",
    "\n",
    "Examples:\n",
    "- \"Export to CSV\" -> success, path = export_with_formatting(df, 'data.csv', format='csv')\n",
    "- \"Export to Excel with formatting\" -> success, path = format_for_excel(df, 'data.xlsx')\n",
    "- \"Create data dictionary\" -> dict_df = create_data_dictionary(df, 'dictionary.csv')\n",
    "- \"Generate summary report\" -> html_content = export_summary_report(df, 'report.html')\n",
    "- \"Create codebook\" -> codebook = create_codebook(df, 'codebook.json')\n",
    "- \"Export to JSON\" -> success, path = export_with_formatting(df, 'data.json', format='json')\n",
    "- \"Export to multiple Excel sheets\" -> format_for_excel(df, 'file.xlsx', {'Sheet1': df, 'Sheet2': df.head(100)})\n",
    "\n",
    "Format options for export_with_formatting:\n",
    "- CSV: encoding, index, separator\n",
    "- JSON: orient, indent\n",
    "- Parquet: compression, index\n",
    "- HTML: classes, table_id, escape\n",
    "- Pickle: protocol\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b02315b",
   "metadata": {},
   "source": [
    "# **MAIN FEATURE FUNCTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881f15c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_tools(df, user_query):\n",
    "    \"\"\"\n",
    "    Main function that gets called by the main router.\n",
    "    MUST take (df, user_query) and return df\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create message chain\n",
    "    messages = []\n",
    "    messages.append(SystemMessage(content=helper_docs))\n",
    "    messages.append(SystemMessage(content=f\"\"\"\n",
    "    You are a data cleaning agent focused on export and formatting operations.\n",
    "    \n",
    "    Dataset info: Shape: {df.shape}, Sample: {df.head(3).to_string()}\n",
    "\n",
    "    Libraries available:\n",
    "    - pd (pandas), np (numpy)\n",
    "    - Path (from pathlib), json, datetime\n",
    "    - All helper functions listed above\n",
    "    - openpyxl for Excel formatting (if available)\n",
    "    \n",
    "    Rules:\n",
    "    - Return only executable Python code, no explanations, no markdown blocks\n",
    "    - Use helper functions for all export and formatting operations\n",
    "    - ASSUME \"df\" IS ALREADY DEFINED\n",
    "    - For export operations, capture return values: success, path = export_with_formatting(df, filename, format)\n",
    "    - For documentation operations, capture return values: result = create_data_dictionary(df)\n",
    "    - In order to generate a response/message to the user use print statements\n",
    "    print(\"message\")\n",
    "    - Write a detailed print message to summarise actions taken and reasons\n",
    "    - DEFAULT to current directory for file outputs unless user specifies path\n",
    "    - Always return the original df (export functions don't modify the DataFrame)\n",
    "    \n",
    "    Common query patterns:\n",
    "    - \"Export to [format]\" -> export_with_formatting(df, filename, format=format)\n",
    "    - \"Export to Excel\" or \"Excel export\" -> format_for_excel(df, filename)\n",
    "    - \"Create data dictionary\" -> create_data_dictionary(df)\n",
    "    - \"Generate summary report\" -> export_summary_report(df)\n",
    "    - \"Create codebook\" -> create_codebook(df)\n",
    "    - \"Export with custom options\" -> export_with_formatting(df, filename, format, options=dict)\n",
    "    - \"Multiple sheets\" -> format_for_excel(df, filename, sheets=dict)\n",
    "    \n",
    "    File naming:\n",
    "    - If user doesn't specify filename, use descriptive defaults like 'cleaned_data.csv', 'data_summary.html'\n",
    "    - Add appropriate extensions based on format\n",
    "    - Use current timestamp in filename if multiple exports might conflict\n",
    "    \"\"\"))\n",
    "    messages.append(HumanMessage(content=f\"User request: {user_query}\"))\n",
    "    \n",
    "    # Call LLM with message chain\n",
    "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")\n",
    "    response = llm.invoke(messages)\n",
    "    generated_code = response.content.strip()\n",
    "    \n",
    "    # Execute code\n",
    "    try:\n",
    "        original_df = df.copy()\n",
    "        # Create local namespace with our variables\n",
    "        local_vars = {\n",
    "            'df': df.copy(),\n",
    "            'original_df': original_df,\n",
    "            'pd': pd,\n",
    "            'np': np,\n",
    "            'Path': Path,\n",
    "            'json': json,\n",
    "            'datetime': datetime,\n",
    "            'export_with_formatting': export_with_formatting,\n",
    "            'create_data_dictionary': create_data_dictionary,\n",
    "            'export_summary_report': export_summary_report,\n",
    "            'format_for_excel': format_for_excel,\n",
    "            'create_codebook': create_codebook,\n",
    "            'print': print\n",
    "        }\n",
    "        \n",
    "        exec(generated_code, globals(), local_vars)\n",
    "        # Return original df since export operations don't modify the data\n",
    "        return original_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"Generated Code:{generated_code}\")\n",
    "        return original_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30928b6",
   "metadata": {},
   "source": [
    "# **Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b618978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create test data with various export opportunities\n",
    "# test_data = {\n",
    "#     'id': range(1, 11),  \n",
    "#     'name': ['John Doe', 'Jane Smith', 'Bob Johnson', 'Mary Brown', 'Alice White',\n",
    "#              'Tom Wilson', 'Lisa Davis', 'Mike Chen', 'Sarah Lee', 'David Kim'],\n",
    "#     'age': [25, 35, 45, 30, 28, 32, 41, 29, 38, 26],\n",
    "#     'salary': [50000.0, 75000.0, 90000.0, 60000.0, 55000.0,\n",
    "#                68000.0, 82000.0, 51000.0, 77000.0, 58000.0],\n",
    "#     'department': ['IT', 'HR', 'Finance', 'IT', 'Marketing',\n",
    "#                    'HR', 'Finance', 'IT', 'Marketing', 'Finance'],\n",
    "#     'join_date': ['2023-01-15', '2022-03-20', '2021-06-10', '2023-02-28', '2022-11-05',\n",
    "#                   '2023-04-12', '2022-08-18', '2023-01-30', '2022-12-14', '2023-03-08'],\n",
    "#     'active': [True, True, False, True, True, False, True, True, True, False],\n",
    "#     'performance_score': [85.5, 92.3, 78.1, 88.9, 91.2, 86.7, 89.4, 83.2, 90.8, 87.6]\n",
    "# }\n",
    "\n",
    "# test_df = pd.DataFrame(test_data)\n",
    "# test_df['join_date'] = pd.to_datetime(test_df['join_date'])\n",
    "\n",
    "# print(\"Test DataFrame created for export tools:\")\n",
    "# print(f\"Shape: {test_df.shape}\")\n",
    "# print(\"\\\\nSample data:\")\n",
    "# print(test_df.head())\n",
    "# print(\"\\\\nData types:\")\n",
    "# print(test_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9610f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the export_tools feature with various queries\n",
    "# print(\"=== TESTING EXPORT TOOLS FEATURE ===\\\\n\")\n",
    "\n",
    "# # Test 1: Basic CSV export\n",
    "# print(\"1. Testing CSV export:\")\n",
    "# query1 = \"Export data to CSV\"\n",
    "# result1 = export_tools(test_df, query1)\n",
    "# print(\" CSV export test completed\\\\n\")\n",
    "\n",
    "# # Test 2: Data dictionary creation\n",
    "# print(\"2. Testing data dictionary creation:\")\n",
    "# query2 = \"Create a comprehensive data dictionary\"\n",
    "# result2 = export_tools(test_df, query2)\n",
    "# print(\" Data dictionary test completed\\\\n\")\n",
    "\n",
    "# # Test 3: Summary report generation\n",
    "# print(\"3. Testing summary report generation:\")\n",
    "# query3 = \"Generate detailed summary report\"\n",
    "# result3 = export_tools(test_df, query3)\n",
    "# print(\" Summary report test completed\\\\n\")\n",
    "\n",
    "# print(\"=== ALL EXPORT TOOLS TESTS COMPLETED ===\")\n",
    "# print(\"If no errors appeared, the export_tools feature is working correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f9c065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df.info()\n",
    "# result.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
