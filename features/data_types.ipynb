{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "691226d2",
   "metadata": {},
   "source": [
    "## **Feature:** Data Type Optimization\n",
    "\n",
    "**Names:** Gia Bao Ngo\n",
    "\n",
    "### **What it does**\n",
    "Analyzes and optimizes data types to reduce memory usage and improve performance. Automatically detects and converts appropriate data types including downcasting numeric types, parsing dates, converting percentages, categorizing low-cardinality columns, and detecting boolean patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff1f7555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dotenv\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Get API Key\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"OpenAI API Key not found\")\n",
    "\n",
    "# Import libraries\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Additional imports for data type optimization\n",
    "import math\n",
    "import re\n",
    "import datetime\n",
    "from sklearn import preprocessing\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Langchain imports\n",
    "from langchain_openai import ChatOpenAI  \n",
    "from langchain.schema import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a88657",
   "metadata": {},
   "source": [
    "### **Helper Functions**\n",
    "- `analyze_data_types(df)` - Analyze current types and suggest optimizations\n",
    "- `optimize_numeric_types(df, downcast='infer')` - Downcast int64→int32, float64→float32\n",
    "- `parse_dates_auto(df, columns=None)` - Auto-detect and parse date columns\n",
    "- `convert_percentages(df, columns=None)` - Convert \"50%\" strings to 0.5 floats\n",
    "- `categorize_low_cardinality(df, threshold=0.05)` - Convert low-cardinality objects to category\n",
    "- `detect_boolean_columns(df)` - Find columns that should be boolean\n",
    "- `memory_usage_comparison(df_before, df_after)` - Show memory savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60424d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_data_types(df):\n",
    "    \"\"\"\n",
    "    Analyze current data types and suggest optimizations.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with current usage and suggested optimizations\n",
    "    \"\"\"\n",
    "    analysis = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_info = {\n",
    "            'Column': col,\n",
    "            'Current_Type': str(df[col].dtype),\n",
    "            'Memory_Usage_MB': df[col].memory_usage(deep=True) / 1024**2,\n",
    "            'Null_Count': df[col].isnull().sum(),\n",
    "            'Unique_Values': df[col].nunique(),\n",
    "            'Cardinality_Ratio': df[col].nunique() / len(df),\n",
    "        }\n",
    "        \n",
    "        # Suggest optimizations\n",
    "        suggestions = []\n",
    "        \n",
    "        # Numeric optimization suggestions\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            if df[col].dtype == 'int64':\n",
    "                min_val, max_val = df[col].min(), df[col].max()\n",
    "                if min_val >= -128 and max_val <= 127:\n",
    "                    suggestions.append('int8')\n",
    "                elif min_val >= -32768 and max_val <= 32767:\n",
    "                    suggestions.append('int16')\n",
    "                elif min_val >= -2147483648 and max_val <= 2147483647:\n",
    "                    suggestions.append('int32')\n",
    "            elif df[col].dtype == 'float64':\n",
    "                suggestions.append('float32')\n",
    "        \n",
    "        # Category optimization\n",
    "        elif df[col].dtype == 'object':\n",
    "            if col_info['Cardinality_Ratio'] < 0.05:  # Low cardinality\n",
    "                suggestions.append('category')\n",
    "            \n",
    "            # Check for percentage strings\n",
    "            if df[col].dropna().astype(str).str.contains(r'%$').any():\n",
    "                suggestions.append('convert_percentage')\n",
    "            \n",
    "            # Check for boolean patterns\n",
    "            unique_vals = set(df[col].dropna().astype(str).str.lower())\n",
    "            bool_patterns = {'yes', 'no', 'true', 'false', '1', '0', 'y', 'n'}\n",
    "            if unique_vals.issubset(bool_patterns) and len(unique_vals) == 2:\n",
    "                suggestions.append('boolean')\n",
    "            \n",
    "            # Check for date patterns\n",
    "            sample_vals = df[col].dropna().head(10).astype(str)\n",
    "            date_patterns = [\n",
    "                r'\\d{4}-\\d{2}-\\d{2}',  # YYYY-MM-DD\n",
    "                r'\\d{2}/\\d{2}/\\d{4}',  # MM/DD/YYYY\n",
    "                r'\\d{2}-\\d{2}-\\d{4}',  # MM-DD-YYYY\n",
    "            ]\n",
    "            if any(sample_vals.str.contains(pattern).any() for pattern in date_patterns):\n",
    "                suggestions.append('datetime')\n",
    "        \n",
    "        col_info['Suggested_Optimizations'] = ', '.join(suggestions) if suggestions else 'None'\n",
    "        col_info['Potential_Memory_Savings'] = 'High' if suggestions else 'Low'\n",
    "        \n",
    "        analysis.append(col_info)\n",
    "    \n",
    "    analysis_df = pd.DataFrame(analysis)\n",
    "    \n",
    "    print(\"=== DATA TYPE ANALYSIS ===\")\n",
    "    print(f\"Total memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    print(f\"Columns analyzed: {len(df.columns)}\")\n",
    "    print(f\"Optimization opportunities: {len([x for x in analysis if x['Suggested_Optimizations'] != 'None'])}\")\n",
    "    \n",
    "    return analysis_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46a090e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_numeric_types(df, downcast='infer'):\n",
    "    \"\"\"\n",
    "    Downcast numeric types to save memory (int64→int32, float64→float32).\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - downcast: 'infer' for automatic, 'integer', 'signed', 'unsigned', 'float'\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with optimized numeric types\n",
    "    \"\"\"\n",
    "    result_df = df.copy()\n",
    "    optimized_cols = []\n",
    "    memory_savings = 0\n",
    "    \n",
    "    for col in result_df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(result_df[col]):\n",
    "            original_memory = result_df[col].memory_usage(deep=True)\n",
    "            original_type = result_df[col].dtype\n",
    "            \n",
    "            try:\n",
    "                # Try to downcast the column\n",
    "                if downcast == 'infer':\n",
    "                    if pd.api.types.is_integer_dtype(result_df[col]):\n",
    "                        result_df[col] = pd.to_numeric(result_df[col], downcast='integer')\n",
    "                    elif pd.api.types.is_float_dtype(result_df[col]):\n",
    "                        result_df[col] = pd.to_numeric(result_df[col], downcast='float')\n",
    "                else:\n",
    "                    result_df[col] = pd.to_numeric(result_df[col], downcast=downcast)\n",
    "                \n",
    "                new_memory = result_df[col].memory_usage(deep=True)\n",
    "                new_type = result_df[col].dtype\n",
    "                \n",
    "                if original_memory > new_memory:\n",
    "                    memory_savings += (original_memory - new_memory)\n",
    "                    optimized_cols.append({\n",
    "                        'column': col,\n",
    "                        'original_type': original_type,\n",
    "                        'new_type': new_type,\n",
    "                        'memory_saved_mb': (original_memory - new_memory) / 1024**2\n",
    "                    })\n",
    "                else:\n",
    "                    # Revert if no savings\n",
    "                    result_df[col] = df[col]\n",
    "                    \n",
    "            except (ValueError, OverflowError):\n",
    "                # Revert if conversion fails\n",
    "                result_df[col] = df[col]\n",
    "                continue\n",
    "    \n",
    "    print(f\"=== NUMERIC TYPE OPTIMIZATION ===\")\n",
    "    print(f\"Columns optimized: {len(optimized_cols)}\")\n",
    "    print(f\"Total memory saved: {memory_savings / 1024**2:.2f} MB\")\n",
    "    \n",
    "    if optimized_cols:\n",
    "        print(\"\\\\nOptimization details:\")\n",
    "        for opt in optimized_cols:\n",
    "            print(f\"  {opt['column']}: {opt['original_type']} → {opt['new_type']} (saved {opt['memory_saved_mb']:.2f} MB)\")\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "p0rj5xaccds",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dates_auto(df, columns=None):\n",
    "    \"\"\"\n",
    "    Auto-detect and parse date columns with multiple format detection.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - columns: list of column names to check (None = all object columns)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with converted datetime columns\n",
    "    \"\"\"\n",
    "    result_df = df.copy()\n",
    "    converted_cols = []\n",
    "    \n",
    "    if columns is None:\n",
    "        columns = result_df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    # Common date formats to try\n",
    "    date_formats = [\n",
    "        '%Y-%m-%d',           # 2023-12-31\n",
    "        '%m/%d/%Y',           # 12/31/2023\n",
    "        '%m-%d-%Y',           # 12-31-2023\n",
    "        '%d/%m/%Y',           # 31/12/2023\n",
    "        '%d-%m-%Y',           # 31-12-2023\n",
    "        '%Y/%m/%d',           # 2023/12/31\n",
    "        '%Y-%m-%d %H:%M:%S',  # 2023-12-31 23:59:59\n",
    "        '%m/%d/%Y %H:%M:%S',  # 12/31/2023 23:59:59\n",
    "        '%Y-%m-%d %H:%M',     # 2023-12-31 23:59\n",
    "        '%m/%d/%Y %H:%M',     # 12/31/2023 23:59\n",
    "    ]\n",
    "    \n",
    "    for col in columns:\n",
    "        if col not in result_df.columns:\n",
    "            continue\n",
    "            \n",
    "        # Skip if already datetime\n",
    "        if pd.api.types.is_datetime64_any_dtype(result_df[col]):\n",
    "            continue\n",
    "            \n",
    "        # Get non-null sample\n",
    "        sample_data = result_df[col].dropna()\n",
    "        if len(sample_data) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Try pandas automatic parsing first\n",
    "        try:\n",
    "            converted = pd.to_datetime(sample_data, infer_datetime_format=True, errors='coerce')\n",
    "            if converted.notna().sum() > len(sample_data) * 0.8:  # 80% success rate\n",
    "                result_df[col] = pd.to_datetime(result_df[col], infer_datetime_format=True, errors='coerce')\n",
    "                converted_cols.append({\n",
    "                    'column': col,\n",
    "                    'method': 'auto_infer',\n",
    "                    'success_rate': converted.notna().sum() / len(sample_data)\n",
    "                })\n",
    "                continue\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Try specific formats\n",
    "        best_format = None\n",
    "        best_success_rate = 0\n",
    "        \n",
    "        for fmt in date_formats:\n",
    "            try:\n",
    "                converted = pd.to_datetime(sample_data, format=fmt, errors='coerce')\n",
    "                success_rate = converted.notna().sum() / len(sample_data)\n",
    "                \n",
    "                if success_rate > best_success_rate and success_rate > 0.8:\n",
    "                    best_format = fmt\n",
    "                    best_success_rate = success_rate\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # Apply best format if found\n",
    "        if best_format:\n",
    "            try:\n",
    "                result_df[col] = pd.to_datetime(result_df[col], format=best_format, errors='coerce')\n",
    "                converted_cols.append({\n",
    "                    'column': col,\n",
    "                    'method': f'format: {best_format}',\n",
    "                    'success_rate': best_success_rate\n",
    "                })\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    print(f\"=== DATE PARSING RESULTS ===\")\n",
    "    print(f\"Columns processed: {len(columns)}\")\n",
    "    print(f\"Columns converted: {len(converted_cols)}\")\n",
    "    \n",
    "    if converted_cols:\n",
    "        print(\"\\\\nConversion details:\")\n",
    "        for conv in converted_cols:\n",
    "            print(f\"  {conv['column']}: {conv['method']} (success: {conv['success_rate']:.1%})\")\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "yfbo29wf9h",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_percentages(df, columns=None):\n",
    "    \"\"\"\n",
    "    Convert percentage strings (\"50%\") to float values (0.5).\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - columns: list of column names to check (None = all object columns)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with converted percentage columns\n",
    "    \"\"\"\n",
    "    result_df = df.copy()\n",
    "    converted_cols = []\n",
    "    \n",
    "    if columns is None:\n",
    "        columns = result_df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    for col in columns:\n",
    "        if col not in result_df.columns:\n",
    "            continue\n",
    "            \n",
    "        # Check if column contains percentage strings\n",
    "        sample_data = result_df[col].dropna().astype(str)\n",
    "        if len(sample_data) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Count how many values end with %\n",
    "        pct_count = sample_data.str.contains(r'%$').sum()\n",
    "        pct_ratio = pct_count / len(sample_data)\n",
    "        \n",
    "        if pct_ratio > 0.5:  # More than 50% are percentages\n",
    "            try:\n",
    "                # Remove % and convert to float\n",
    "                converted_values = result_df[col].astype(str).str.replace('%', '', regex=False)\n",
    "                converted_values = pd.to_numeric(converted_values, errors='coerce') / 100\n",
    "                \n",
    "                # Check conversion success rate\n",
    "                success_rate = converted_values.notna().sum() / len(result_df[col].dropna())\n",
    "                \n",
    "                if success_rate > 0.8:  # 80% success rate\n",
    "                    result_df[col] = converted_values\n",
    "                    converted_cols.append({\n",
    "                        'column': col,\n",
    "                        'percentage_ratio': pct_ratio,\n",
    "                        'success_rate': success_rate\n",
    "                    })\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    print(f\"=== PERCENTAGE CONVERSION RESULTS ===\")\n",
    "    print(f\"Columns processed: {len(columns)}\")\n",
    "    print(f\"Columns converted: {len(converted_cols)}\")\n",
    "    \n",
    "    if converted_cols:\n",
    "        print(\"\\\\nConversion details:\")\n",
    "        for conv in converted_cols:\n",
    "            print(f\"  {conv['column']}: {conv['percentage_ratio']:.1%} had % symbols (success: {conv['success_rate']:.1%})\")\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fknv52abfwa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_low_cardinality(df, threshold=0.05):\n",
    "    \"\"\"\n",
    "    Convert low-cardinality object columns to category type for memory savings.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - threshold: cardinality ratio threshold (unique values / total values)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with categorical columns converted\n",
    "    \"\"\"\n",
    "    result_df = df.copy()\n",
    "    converted_cols = []\n",
    "    \n",
    "    object_cols = result_df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    for col in object_cols:\n",
    "        # Calculate cardinality ratio\n",
    "        unique_vals = result_df[col].nunique()\n",
    "        total_vals = len(result_df[col])\n",
    "        cardinality_ratio = unique_vals / total_vals\n",
    "        \n",
    "        if cardinality_ratio <= threshold:\n",
    "            original_memory = result_df[col].memory_usage(deep=True)\n",
    "            \n",
    "            # Convert to category\n",
    "            result_df[col] = result_df[col].astype('category')\n",
    "            new_memory = result_df[col].memory_usage(deep=True)\n",
    "            \n",
    "            memory_saved = original_memory - new_memory\n",
    "            if memory_saved > 0:\n",
    "                converted_cols.append({\n",
    "                    'column': col,\n",
    "                    'unique_values': unique_vals,\n",
    "                    'cardinality_ratio': cardinality_ratio,\n",
    "                    'memory_saved_mb': memory_saved / 1024**2\n",
    "                })\n",
    "            else:\n",
    "                # Revert if no memory savings\n",
    "                result_df[col] = df[col]\n",
    "    \n",
    "    print(f\"=== CATEGORICAL CONVERSION RESULTS ===\")\n",
    "    print(f\"Object columns processed: {len(object_cols)}\")\n",
    "    print(f\"Columns converted to category: {len(converted_cols)}\")\n",
    "    \n",
    "    total_memory_saved = sum([conv['memory_saved_mb'] for conv in converted_cols])\n",
    "    print(f\"Total memory saved: {total_memory_saved:.2f} MB\")\n",
    "    \n",
    "    if converted_cols:\n",
    "        print(\"\\\\nConversion details:\")\n",
    "        for conv in converted_cols:\n",
    "            print(f\"  {conv['column']}: {conv['unique_values']} unique values \"\n",
    "                  f\"({conv['cardinality_ratio']:.1%} cardinality, saved {conv['memory_saved_mb']:.2f} MB)\")\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8zn451um8q",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_boolean_columns(df):\n",
    "    \"\"\"\n",
    "    Find columns that should be boolean and convert them.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with boolean columns converted\n",
    "    \"\"\"\n",
    "    result_df = df.copy()\n",
    "    converted_cols = []\n",
    "    \n",
    "    # Boolean patterns to detect\n",
    "    boolean_patterns = {\n",
    "        'yes_no': {'yes', 'no'},\n",
    "        'true_false': {'true', 'false'},\n",
    "        'y_n': {'y', 'n'},\n",
    "        'one_zero_str': {'1', '0'},\n",
    "        'on_off': {'on', 'off'},\n",
    "        'active_inactive': {'active', 'inactive'},\n",
    "        'enabled_disabled': {'enabled', 'disabled'}\n",
    "    }\n",
    "    \n",
    "    for col in result_df.columns:\n",
    "        # Skip if already boolean\n",
    "        if result_df[col].dtype == 'bool':\n",
    "            continue\n",
    "            \n",
    "        # Get unique non-null values as lowercase strings\n",
    "        unique_vals = set(result_df[col].dropna().astype(str).str.lower().str.strip())\n",
    "        \n",
    "        if len(unique_vals) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Check if values match any boolean pattern\n",
    "        pattern_matched = None\n",
    "        for pattern_name, pattern_vals in boolean_patterns.items():\n",
    "            if unique_vals == pattern_vals or (len(unique_vals) <= 2 and unique_vals.issubset(pattern_vals)):\n",
    "                pattern_matched = pattern_name\n",
    "                break\n",
    "        \n",
    "        if pattern_matched:\n",
    "            try:\n",
    "                # Create mapping based on pattern\n",
    "                if pattern_matched in ['yes_no', 'y_n']:\n",
    "                    mapping = {'yes': True, 'y': True, 'no': False, 'n': False}\n",
    "                elif pattern_matched == 'true_false':\n",
    "                    mapping = {'true': True, 'false': False}\n",
    "                elif pattern_matched == 'one_zero_str':\n",
    "                    mapping = {'1': True, '0': False}\n",
    "                elif pattern_matched == 'on_off':\n",
    "                    mapping = {'on': True, 'off': False}\n",
    "                elif pattern_matched == 'active_inactive':\n",
    "                    mapping = {'active': True, 'inactive': False}\n",
    "                elif pattern_matched == 'enabled_disabled':\n",
    "                    mapping = {'enabled': True, 'disabled': False}\n",
    "                \n",
    "                # Apply mapping\n",
    "                original_memory = result_df[col].memory_usage(deep=True)\n",
    "                result_df[col] = result_df[col].astype(str).str.lower().str.strip().map(mapping)\n",
    "                new_memory = result_df[col].memory_usage(deep=True)\n",
    "                \n",
    "                converted_cols.append({\n",
    "                    'column': col,\n",
    "                    'pattern': pattern_matched,\n",
    "                    'unique_values': list(unique_vals),\n",
    "                    'memory_saved_mb': (original_memory - new_memory) / 1024**2\n",
    "                })\n",
    "                \n",
    "            except Exception:\n",
    "                # Revert if conversion fails\n",
    "                result_df[col] = df[col]\n",
    "                continue\n",
    "    \n",
    "    print(f\"=== BOOLEAN DETECTION RESULTS ===\")\n",
    "    print(f\"Columns processed: {len(result_df.columns)}\")\n",
    "    print(f\"Boolean columns detected and converted: {len(converted_cols)}\")\n",
    "    \n",
    "    total_memory_saved = sum([conv['memory_saved_mb'] for conv in converted_cols])\n",
    "    print(f\"Total memory saved: {total_memory_saved:.2f} MB\")\n",
    "    \n",
    "    if converted_cols:\n",
    "        print(\"\\\\nConversion details:\")\n",
    "        for conv in converted_cols:\n",
    "            print(f\"  {conv['column']}: {conv['pattern']} pattern ({conv['unique_values']}) \"\n",
    "                  f\"(saved {conv['memory_saved_mb']:.2f} MB)\")\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "sllxh3t2sa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_usage_comparison(df_before, df_after):\n",
    "    \"\"\"\n",
    "    Show memory savings comparison between two DataFrames.\n",
    "    \n",
    "    Parameters:\n",
    "    - df_before: original DataFrame\n",
    "    - df_after: optimized DataFrame\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with memory comparison details\n",
    "    \"\"\"\n",
    "    comparison = []\n",
    "    \n",
    "    # Overall memory comparison\n",
    "    memory_before = df_before.memory_usage(deep=True).sum() / 1024**2\n",
    "    memory_after = df_after.memory_usage(deep=True).sum() / 1024**2\n",
    "    memory_saved = memory_before - memory_after\n",
    "    savings_pct = (memory_saved / memory_before) * 100 if memory_before > 0 else 0\n",
    "    \n",
    "    print(f\"=== MEMORY USAGE COMPARISON ===\")\n",
    "    print(f\"Memory before optimization: {memory_before:.2f} MB\")\n",
    "    print(f\"Memory after optimization: {memory_after:.2f} MB\")\n",
    "    print(f\"Total memory saved: {memory_saved:.2f} MB ({savings_pct:.1f}%)\")\n",
    "    print(f\"Data shape: {df_after.shape}\")\n",
    "    \n",
    "    # Column-by-column comparison\n",
    "    for col in df_before.columns:\n",
    "        if col in df_after.columns:\n",
    "            mem_before = df_before[col].memory_usage(deep=True) / 1024**2\n",
    "            mem_after = df_after[col].memory_usage(deep=True) / 1024**2\n",
    "            mem_saved = mem_before - mem_after\n",
    "            \n",
    "            comparison.append({\n",
    "                'Column': col,\n",
    "                'Type_Before': str(df_before[col].dtype),\n",
    "                'Type_After': str(df_after[col].dtype),\n",
    "                'Memory_Before_MB': mem_before,\n",
    "                'Memory_After_MB': mem_after,\n",
    "                'Memory_Saved_MB': mem_saved,\n",
    "                'Savings_Percent': (mem_saved / mem_before) * 100 if mem_before > 0 else 0\n",
    "            })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison)\n",
    "    \n",
    "    # Show top memory savers\n",
    "    if len(comparison_df) > 0:\n",
    "        top_savers = comparison_df[comparison_df['Memory_Saved_MB'] > 0].nlargest(5, 'Memory_Saved_MB')\n",
    "        \n",
    "        if len(top_savers) > 0:\n",
    "            print(f\"\\\\nTop memory saving columns:\")\n",
    "            for _, row in top_savers.iterrows():\n",
    "                print(f\"  {row['Column']}: {row['Type_Before']} → {row['Type_After']} \"\n",
    "                      f\"(saved {row['Memory_Saved_MB']:.2f} MB, {row['Savings_Percent']:.1f}%)\")\n",
    "        \n",
    "        # Summary by data type changes\n",
    "        type_changes = comparison_df[comparison_df['Type_Before'] != comparison_df['Type_After']]\n",
    "        if len(type_changes) > 0:\n",
    "            print(f\"\\\\nType conversions: {len(type_changes)} columns\")\n",
    "            change_summary = type_changes.groupby(['Type_Before', 'Type_After']).size().reset_index(name='Count')\n",
    "            for _, row in change_summary.iterrows():\n",
    "                print(f\"  {row['Type_Before']} → {row['Type_After']}: {row['Count']} columns\")\n",
    "    \n",
    "    return comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c99ee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_docs = \"\"\" Helper functions available:\n",
    "- analyze_data_types(df): Analyze current types and suggest optimizations. Returns DataFrame with analysis.\n",
    "- optimize_numeric_types(df, downcast='infer'): Downcast int64→int32, float64→float32 to save memory. Returns optimized DataFrame.\n",
    "- parse_dates_auto(df, columns=None): Auto-detect and parse date columns with multiple format detection. Returns DataFrame with datetime columns.\n",
    "- convert_percentages(df, columns=None): Convert \"50%\" strings to 0.5 floats. Returns DataFrame with converted columns.\n",
    "- categorize_low_cardinality(df, threshold=0.05): Convert low-cardinality objects to category type. Returns DataFrame with categorical columns.\n",
    "- detect_boolean_columns(df): Find and convert columns that should be boolean (Yes/No, True/False, 1/0 patterns). Returns DataFrame with boolean columns.\n",
    "- memory_usage_comparison(df_before, df_after): Show memory savings between DataFrames. Returns comparison DataFrame.\n",
    "\n",
    "Examples:\n",
    "- \"Analyze data types\" -> analysis_df = analyze_data_types(df)\n",
    "- \"Optimize numeric types\" -> df = optimize_numeric_types(df)\n",
    "- \"Convert dates automatically\" -> df = parse_dates_auto(df)\n",
    "- \"Fix percentage columns\" -> df = convert_percentages(df)\n",
    "- \"Convert to categories\" -> df = categorize_low_cardinality(df)\n",
    "- \"Detect boolean columns\" -> df = detect_boolean_columns(df)\n",
    "- \"Compare memory usage\" -> comparison = memory_usage_comparison(original_df, df)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b02315b",
   "metadata": {},
   "source": [
    "# **MAIN FEATURE FUNCTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "881f15c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_types(df, user_query):\n",
    "    \"\"\"\n",
    "    Main function that gets called by the main router.\n",
    "    MUST take (df, user_query) and return df\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create message chain\n",
    "    messages = []\n",
    "    messages.append(SystemMessage(content=helper_docs))\n",
    "    messages.append(SystemMessage(content=f\"\"\"\n",
    "    You are a data cleaning agent focused on data type optimization and memory management.\n",
    "    \n",
    "    Dataset info: Shape: {df.shape}, Sample: {df.head(3).to_string()}\n",
    "\n",
    "    Libraries available:\n",
    "    - pd (pandas), np (numpy)\n",
    "    - math, re, datetime\n",
    "    - sklearn.preprocessing\n",
    "    - All helper functions listed above\n",
    "    \n",
    "    Rules:\n",
    "    - Return only executable Python code, no explanations, no markdown blocks\n",
    "    - Use helper functions when appropriate for data type optimization tasks\n",
    "    - ASSUME \"df\" IS ALREADY DEFINED\n",
    "    - For analysis queries, use helper functions that print results (analyze_data_types, memory_usage_comparison)\n",
    "    - For optimization, use helper functions that modify DataFrame (optimize_numeric_types, parse_dates_auto, etc.)\n",
    "    - ALWAYS assign the result back to df when modifying: df = optimize_numeric_types(df)\n",
    "    - In order to generate a response/message to the user use print statements\n",
    "    print(\"message\")\n",
    "    - Write a detailed print message to summarise actions taken and reasons\n",
    "    \n",
    "    Common query patterns:\n",
    "    - \"Analyze data types\" or \"Check data types\" -> analysis_df = analyze_data_types(df)\n",
    "    - \"Optimize data types\" or \"Save memory\" -> df = optimize_numeric_types(df); df = categorize_low_cardinality(df)\n",
    "    - \"Convert dates\" or \"Parse dates\" -> df = parse_dates_auto(df)\n",
    "    - \"Fix percentage columns\" -> df = convert_percentages(df)\n",
    "    - \"Convert to categories\" -> df = categorize_low_cardinality(df)\n",
    "    - \"Detect boolean columns\" -> df = detect_boolean_columns(df)\n",
    "    - \"Compare memory usage\" -> original_df = df.copy(); [optimizations]; comparison = memory_usage_comparison(original_df, df)\n",
    "    \"\"\"))\n",
    "    messages.append(HumanMessage(content=f\"User request: {user_query}\"))\n",
    "    \n",
    "    # Call LLM with message chain\n",
    "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")\n",
    "    response = llm.invoke(messages)\n",
    "    generated_code = response.content.strip()\n",
    "    \n",
    "    # Execute code\n",
    "    try:\n",
    "        original_df = df.copy()\n",
    "        # Create local namespace with our variables\n",
    "        local_vars = {\n",
    "            'df': df.copy(),\n",
    "            'original_df': original_df,\n",
    "            'pd': pd,\n",
    "            'np': np,\n",
    "            'optimize_numeric_types': optimize_numeric_types,\n",
    "            'categorize_low_cardinality': categorize_low_cardinality,\n",
    "            'parse_dates_auto': parse_dates_auto,\n",
    "            'convert_percentages': convert_percentages,\n",
    "            'detect_boolean_columns': detect_boolean_columns,\n",
    "            'analyze_data_types': analyze_data_types,\n",
    "            'memory_usage_comparison': memory_usage_comparison,\n",
    "            'print': print\n",
    "        }\n",
    "        \n",
    "        exec(generated_code, globals(), local_vars)\n",
    "        return local_vars['df']\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"Generated Code:{generated_code}\")\n",
    "        return original_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30928b6",
   "metadata": {},
   "source": [
    "# **Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b618978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create sample data with various data type optimization opportunities\n",
    "# test_data = {\n",
    "#     'id': range(1, 101),  # Can be downcasted from int64 to int8\n",
    "#     'score': [float(x) for x in range(1, 101)],  # Can be downcasted from float64 to float32\n",
    "#     'category': ['A', 'B', 'C', 'A', 'B'] * 20,  # Low cardinality - can be categorical\n",
    "#     'active': ['Yes', 'No', 'Yes', 'No', 'Yes'] * 20,  # Boolean pattern\n",
    "#     'percentage': ['85%', '92%', '78%', '95%', '88%'] * 20,  # Percentage strings\n",
    "#     'date_string': ['2023-01-15', '2023-02-20', '2023-03-10'] * 33 + ['2023-04-05'],  # Date strings\n",
    "#     'large_text': ['This is some text content'] * 100,  # High cardinality text\n",
    "# }\n",
    "\n",
    "# test_df = pd.DataFrame(test_data)\n",
    "# print(\"Test DataFrame created:\")\n",
    "# print(f\"Shape: {test_df.shape}\")\n",
    "# print(\"\\\\nData types:\")\n",
    "# print(test_df.dtypes)\n",
    "# print(\"\\\\nMemory usage:\")\n",
    "# print(f\"Total: {test_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9610f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"Optimize data types to save memory\"\n",
    "# result = data_types(test_df, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff1a45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403afe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
