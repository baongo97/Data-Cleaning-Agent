{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "691226d2",
   "metadata": {},
   "source": [
    "## **Feature:** Feature Engineering\n",
    "\n",
    "**Names:** Gia Bao Ngo\n",
    "\n",
    "### **What it does**\n",
    "Creates new features from existing data through various transformations including categorical binning, ratio calculations, feature interactions, time series lag features, rolling statistics, and polynomial transformations to enhance machine learning model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1f7555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dotenv\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Get API Key\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"OpenAI API Key not found\")\n",
    "\n",
    "# Import libraries\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Additional imports for feature engineering\n",
    "import math\n",
    "import re\n",
    "import datetime\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Langchain imports\n",
    "from langchain_openai import ChatOpenAI  \n",
    "from langchain.schema import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a88657",
   "metadata": {},
   "source": [
    "### **Helper Functions**\n",
    "- `create_bins(df, column, bins=None, labels=None, method='equal_width')` - Create categorical bins from numeric columns\n",
    "- `create_ratios(df, numerator_col, denominator_col, new_name=None)` - Calculate ratios between two numeric columns\n",
    "- `create_interaction_features(df, col1, col2, operation='multiply')` - Create interaction features between columns\n",
    "- `create_lag_features(df, column, lags=[1], group_by=None)` - Create time series lag features\n",
    "- `create_rolling_features(df, column, window=3, operations=['mean'])` - Create rolling window statistics\n",
    "- `create_polynomial_features(df, columns, degree=2)` - Generate polynomial transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60424d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bins(df, column, bins=None, labels=None, method='equal_width'):\n",
    "    \"\"\"\n",
    "    Create categorical bins from numeric columns using various binning methods.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - column: column name to bin\n",
    "    - bins: number of bins (int) or bin edges (array-like)\n",
    "    - labels: labels for bins (default: auto-generated)\n",
    "    - method: 'equal_width', 'equal_frequency', or 'custom'\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with new binned column\n",
    "    \"\"\"\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"Column '{column}' not found in DataFrame\")\n",
    "    \n",
    "    if not pd.api.types.is_numeric_dtype(df[column]):\n",
    "        raise ValueError(f\"Column '{column}' must be numeric for binning\")\n",
    "    \n",
    "    result_df = df.copy()\n",
    "    new_col_name = f\"{column}_binned\"\n",
    "    \n",
    "    # Handle missing values\n",
    "    non_null_data = df[column].dropna()\n",
    "    if len(non_null_data) == 0:\n",
    "        print(f\"Warning: Column '{column}' has no valid data for binning\")\n",
    "        result_df[new_col_name] = np.nan\n",
    "        return result_df\n",
    "    \n",
    "    try:\n",
    "        if method == 'equal_width':\n",
    "            # Equal-width binning (default pandas behavior)\n",
    "            if bins is None:\n",
    "                bins = 5  # Default number of bins\n",
    "            \n",
    "            result_df[new_col_name] = pd.cut(\n",
    "                df[column], \n",
    "                bins=bins, \n",
    "                labels=labels, \n",
    "                include_lowest=True,\n",
    "                duplicates='drop'\n",
    "            )\n",
    "            \n",
    "        elif method == 'equal_frequency':\n",
    "            # Equal-frequency binning (quantile-based)\n",
    "            if bins is None:\n",
    "                bins = 5\n",
    "            \n",
    "            result_df[new_col_name] = pd.qcut(\n",
    "                df[column], \n",
    "                q=bins, \n",
    "                labels=labels, \n",
    "                duplicates='drop'\n",
    "            )\n",
    "            \n",
    "        elif method == 'custom':\n",
    "            # Custom bin edges\n",
    "            if bins is None:\n",
    "                raise ValueError(\"Custom method requires bin edges to be specified\")\n",
    "            \n",
    "            result_df[new_col_name] = pd.cut(\n",
    "                df[column], \n",
    "                bins=bins, \n",
    "                labels=labels, \n",
    "                include_lowest=True,\n",
    "                duplicates='drop'\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Method must be 'equal_width', 'equal_frequency', or 'custom'\")\n",
    "        \n",
    "        # Report binning results\n",
    "        bin_counts = result_df[new_col_name].value_counts().sort_index()\n",
    "        total_binned = bin_counts.sum()\n",
    "        null_count = result_df[new_col_name].isnull().sum()\n",
    "        \n",
    "        print(f\"=== BINNING RESULTS FOR '{column}' ===\")\n",
    "        print(f\"Method: {method}\")\n",
    "        print(f\"Number of bins created: {len(bin_counts)}\")\n",
    "        print(f\"Values binned: {total_binned}\")\n",
    "        print(f\"Null values: {null_count}\")\n",
    "        \n",
    "        print(f\"\\nBin distribution:\")\n",
    "        for bin_label, count in bin_counts.items():\n",
    "            percentage = (count / total_binned) * 100 if total_binned > 0 else 0\n",
    "            print(f\"  {bin_label}: {count} values ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Show range information\n",
    "        min_val, max_val = non_null_data.min(), non_null_data.max()\n",
    "        print(f\"\\nOriginal range: [{min_val:.2f}, {max_val:.2f}]\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating bins for column '{column}': {str(e)}\")\n",
    "        return df\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a090e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ratios(df, numerator_col, denominator_col, new_name=None):\n",
    "    \"\"\"\n",
    "    Calculate ratios between two numeric columns.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - numerator_col: column name for numerator\n",
    "    - denominator_col: column name for denominator\n",
    "    - new_name: name for new ratio column (default: auto-generated)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with new ratio column\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    if numerator_col not in df.columns:\n",
    "        raise ValueError(f\"Numerator column '{numerator_col}' not found in DataFrame\")\n",
    "    if denominator_col not in df.columns:\n",
    "        raise ValueError(f\"Denominator column '{denominator_col}' not found in DataFrame\")\n",
    "    \n",
    "    if not pd.api.types.is_numeric_dtype(df[numerator_col]):\n",
    "        raise ValueError(f\"Numerator column '{numerator_col}' must be numeric\")\n",
    "    if not pd.api.types.is_numeric_dtype(df[denominator_col]):\n",
    "        raise ValueError(f\"Denominator column '{denominator_col}' must be numeric\")\n",
    "    \n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Generate column name if not provided\n",
    "    if new_name is None:\n",
    "        new_name = f\"{numerator_col}_per_{denominator_col}\"\n",
    "    \n",
    "    try:\n",
    "        # Calculate ratio with division by zero handling\n",
    "        denominator_values = result_df[denominator_col].replace(0, np.nan)  # Replace 0 with NaN to avoid division by zero\n",
    "        result_df[new_name] = result_df[numerator_col] / denominator_values\n",
    "        \n",
    "        # Calculate statistics for reporting\n",
    "        ratio_stats = result_df[new_name].describe()\n",
    "        zero_denominators = (df[denominator_col] == 0).sum()\n",
    "        null_ratios = result_df[new_name].isnull().sum()\n",
    "        infinite_ratios = np.isinf(result_df[new_name]).sum()\n",
    "        \n",
    "        # Handle infinite values (in case any slipped through)\n",
    "        if infinite_ratios > 0:\n",
    "            result_df[new_name] = result_df[new_name].replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        print(f\"=== RATIO CREATION RESULTS ===\")\n",
    "        print(f\"New column: '{new_name}'\")\n",
    "        print(f\"Formula: {numerator_col} / {denominator_col}\")\n",
    "        print(f\"Values calculated: {len(df) - null_ratios}\")\n",
    "        print(f\"Null values (including zero denominators): {null_ratios}\")\n",
    "        print(f\"Zero denominators handled: {zero_denominators}\")\n",
    "        \n",
    "        print(f\"\\nRatio statistics:\")\n",
    "        print(f\"  Mean: {ratio_stats['mean']:.4f}\")\n",
    "        print(f\"  Median (50%): {ratio_stats['50%']:.4f}\")\n",
    "        print(f\"  Min: {ratio_stats['min']:.4f}\")\n",
    "        print(f\"  Max: {ratio_stats['max']:.4f}\")\n",
    "        print(f\"  Std: {ratio_stats['std']:.4f}\")\n",
    "        \n",
    "        # Check for potential issues\n",
    "        warnings = []\n",
    "        if zero_denominators > 0:\n",
    "            warnings.append(f\"{zero_denominators} zero denominators converted to NaN\")\n",
    "        if ratio_stats['max'] > 1000 or ratio_stats['min'] < -1000:\n",
    "            warnings.append(\"Extreme ratio values detected - consider data validation\")\n",
    "        if ratio_stats['std'] > ratio_stats['mean'] * 10:\n",
    "            warnings.append(\"High variance in ratios - consider outlier analysis\")\n",
    "        \n",
    "        if warnings:\n",
    "            print(\"\\nWarnings:\")\n",
    "            for warning in warnings:\n",
    "                print(f\"  - {warning}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating ratio column: {str(e)}\")\n",
    "        return df\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "itvw2vkcgst",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interaction_features(df, col1, col2, operation='multiply'):\n",
    "    \"\"\"\n",
    "    Create interaction features between two columns using various operations.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - col1: first column name\n",
    "    - col2: second column name\n",
    "    - operation: 'multiply', 'add', 'subtract', 'divide', 'mean', 'max', 'min'\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with new interaction feature\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    if col1 not in df.columns:\n",
    "        raise ValueError(f\"Column '{col1}' not found in DataFrame\")\n",
    "    if col2 not in df.columns:\n",
    "        raise ValueError(f\"Column '{col2}' not found in DataFrame\")\n",
    "    \n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Generate new column name based on operation\n",
    "    operation_symbols = {\n",
    "        'multiply': 'x',\n",
    "        'add': 'plus',\n",
    "        'subtract': 'minus',\n",
    "        'divide': 'div',\n",
    "        'mean': 'mean',\n",
    "        'max': 'max',\n",
    "        'min': 'min'\n",
    "    }\n",
    "    \n",
    "    symbol = operation_symbols.get(operation, operation)\n",
    "    new_col_name = f\"{col1}_{symbol}_{col2}\"\n",
    "    \n",
    "    try:\n",
    "        # Handle different data types and operations\n",
    "        if operation == 'multiply':\n",
    "            if pd.api.types.is_numeric_dtype(df[col1]) and pd.api.types.is_numeric_dtype(df[col2]):\n",
    "                result_df[new_col_name] = df[col1] * df[col2]\n",
    "            else:\n",
    "                # For non-numeric, might want string concatenation or other logic\n",
    "                print(f\"Warning: Multiply operation on non-numeric columns may not be meaningful\")\n",
    "                result_df[new_col_name] = pd.to_numeric(df[col1], errors='coerce') * pd.to_numeric(df[col2], errors='coerce')\n",
    "                \n",
    "        elif operation == 'add':\n",
    "            if pd.api.types.is_numeric_dtype(df[col1]) and pd.api.types.is_numeric_dtype(df[col2]):\n",
    "                result_df[new_col_name] = df[col1] + df[col2]\n",
    "            else:\n",
    "                # String concatenation for non-numeric\n",
    "                result_df[new_col_name] = df[col1].astype(str) + \"_\" + df[col2].astype(str)\n",
    "                \n",
    "        elif operation == 'subtract':\n",
    "            if pd.api.types.is_numeric_dtype(df[col1]) and pd.api.types.is_numeric_dtype(df[col2]):\n",
    "                result_df[new_col_name] = df[col1] - df[col2]\n",
    "            else:\n",
    "                print(f\"Warning: Subtract operation requires numeric columns\")\n",
    "                result_df[new_col_name] = pd.to_numeric(df[col1], errors='coerce') - pd.to_numeric(df[col2], errors='coerce')\n",
    "                \n",
    "        elif operation == 'divide':\n",
    "            if pd.api.types.is_numeric_dtype(df[col1]) and pd.api.types.is_numeric_dtype(df[col2]):\n",
    "                # Handle division by zero\n",
    "                denominator = df[col2].replace(0, np.nan)\n",
    "                result_df[new_col_name] = df[col1] / denominator\n",
    "            else:\n",
    "                print(f\"Warning: Divide operation requires numeric columns\")\n",
    "                num_col1 = pd.to_numeric(df[col1], errors='coerce')\n",
    "                num_col2 = pd.to_numeric(df[col2], errors='coerce').replace(0, np.nan)\n",
    "                result_df[new_col_name] = num_col1 / num_col2\n",
    "                \n",
    "        elif operation == 'mean':\n",
    "            if pd.api.types.is_numeric_dtype(df[col1]) and pd.api.types.is_numeric_dtype(df[col2]):\n",
    "                result_df[new_col_name] = (df[col1] + df[col2]) / 2\n",
    "            else:\n",
    "                print(f\"Warning: Mean operation requires numeric columns\")\n",
    "                num_col1 = pd.to_numeric(df[col1], errors='coerce')\n",
    "                num_col2 = pd.to_numeric(df[col2], errors='coerce')\n",
    "                result_df[new_col_name] = (num_col1 + num_col2) / 2\n",
    "                \n",
    "        elif operation == 'max':\n",
    "            if pd.api.types.is_numeric_dtype(df[col1]) and pd.api.types.is_numeric_dtype(df[col2]):\n",
    "                result_df[new_col_name] = np.maximum(df[col1], df[col2])\n",
    "            else:\n",
    "                print(f\"Warning: Max operation on non-numeric columns may not be meaningful\")\n",
    "                num_col1 = pd.to_numeric(df[col1], errors='coerce')\n",
    "                num_col2 = pd.to_numeric(df[col2], errors='coerce')\n",
    "                result_df[new_col_name] = np.maximum(num_col1, num_col2)\n",
    "                \n",
    "        elif operation == 'min':\n",
    "            if pd.api.types.is_numeric_dtype(df[col1]) and pd.api.types.is_numeric_dtype(df[col2]):\n",
    "                result_df[new_col_name] = np.minimum(df[col1], df[col2])\n",
    "            else:\n",
    "                print(f\"Warning: Min operation on non-numeric columns may not be meaningful\")\n",
    "                num_col1 = pd.to_numeric(df[col1], errors='coerce')\n",
    "                num_col2 = pd.to_numeric(df[col2], errors='coerce')\n",
    "                result_df[new_col_name] = np.minimum(num_col1, num_col2)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported operation: {operation}\")\n",
    "        \n",
    "        # Calculate and report statistics\n",
    "        if pd.api.types.is_numeric_dtype(result_df[new_col_name]):\n",
    "            stats = result_df[new_col_name].describe()\n",
    "            null_count = result_df[new_col_name].isnull().sum()\n",
    "            \n",
    "            print(f\"=== INTERACTION FEATURE RESULTS ===\")\n",
    "            print(f\"New feature: '{new_col_name}'\")\n",
    "            print(f\"Operation: {col1} {operation} {col2}\")\n",
    "            print(f\"Data type: {result_df[new_col_name].dtype}\")\n",
    "            print(f\"Valid values: {len(df) - null_count}\")\n",
    "            print(f\"Null values: {null_count}\")\n",
    "            \n",
    "            if not stats.empty:\n",
    "                print(f\"\\nFeature statistics:\")\n",
    "                print(f\"  Mean: {stats['mean']:.4f}\")\n",
    "                print(f\"  Median (50%): {stats['50%']:.4f}\")\n",
    "                print(f\"  Min: {stats['min']:.4f}\")\n",
    "                print(f\"  Max: {stats['max']:.4f}\")\n",
    "                print(f\"  Std: {stats['std']:.4f}\")\n",
    "        else:\n",
    "            # For non-numeric features\n",
    "            unique_count = result_df[new_col_name].nunique()\n",
    "            null_count = result_df[new_col_name].isnull().sum()\n",
    "            \n",
    "            print(f\"=== INTERACTION FEATURE RESULTS ===\")\n",
    "            print(f\"New feature: '{new_col_name}'\")\n",
    "            print(f\"Operation: {col1} {operation} {col2}\")\n",
    "            print(f\"Data type: {result_df[new_col_name].dtype}\")\n",
    "            print(f\"Unique values: {unique_count}\")\n",
    "            print(f\"Null values: {null_count}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating interaction feature: {str(e)}\")\n",
    "        return df\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6t98cb7qkpo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_features(df, column, lags=[1], group_by=None):\n",
    "    \"\"\"\n",
    "    Create time series lag features (previous values) for a column.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - column: column name to create lags for\n",
    "    - lags: list of lag periods (e.g., [1, 2, 3] for 1, 2, 3 periods back)\n",
    "    - group_by: column name to group by (for panel data)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with new lag columns\n",
    "    \"\"\"\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"Column '{column}' not found in DataFrame\")\n",
    "    \n",
    "    if group_by and group_by not in df.columns:\n",
    "        raise ValueError(f\"Group by column '{group_by}' not found in DataFrame\")\n",
    "    \n",
    "    result_df = df.copy()\n",
    "    created_cols = []\n",
    "    \n",
    "    try:\n",
    "        if group_by:\n",
    "            # Create lags within each group\n",
    "            for lag in lags:\n",
    "                lag_col_name = f\"{column}_lag_{lag}\"\n",
    "                result_df[lag_col_name] = result_df.groupby(group_by)[column].shift(lag)\n",
    "                created_cols.append(lag_col_name)\n",
    "        else:\n",
    "            # Create lags for entire dataset\n",
    "            for lag in lags:\n",
    "                lag_col_name = f\"{column}_lag_{lag}\"\n",
    "                result_df[lag_col_name] = result_df[column].shift(lag)\n",
    "                created_cols.append(lag_col_name)\n",
    "        \n",
    "        # Calculate statistics for reporting\n",
    "        total_rows = len(result_df)\n",
    "        print(f\"=== LAG FEATURES CREATION RESULTS ===\")\n",
    "        print(f\"Source column: '{column}'\")\n",
    "        if group_by:\n",
    "            print(f\"Grouped by: '{group_by}' ({result_df[group_by].nunique()} unique groups)\")\n",
    "        print(f\"Lag periods created: {lags}\")\n",
    "        print(f\"New columns: {created_cols}\")\n",
    "        \n",
    "        print(f\"\\nLag features summary:\")\n",
    "        for i, lag_col in enumerate(created_cols):\n",
    "            valid_values = result_df[lag_col].notna().sum()\n",
    "            null_values = result_df[lag_col].isna().sum()\n",
    "            lag_period = lags[i]\n",
    "            \n",
    "            print(f\"  {lag_col}:\")\n",
    "            print(f\"    Valid values: {valid_values} ({valid_values/total_rows:.1%})\")\n",
    "            print(f\"    Null values: {null_values} (expected {lag_period} at start of each group)\")\n",
    "            \n",
    "            if pd.api.types.is_numeric_dtype(result_df[lag_col]):\n",
    "                stats = result_df[lag_col].describe()\n",
    "                print(f\"    Mean: {stats['mean']:.4f}, Std: {stats['std']:.4f}\")\n",
    "        \n",
    "        # Check for potential issues\n",
    "        warnings = []\n",
    "        if group_by:\n",
    "            min_group_size = result_df.groupby(group_by).size().min()\n",
    "            max_lag = max(lags)\n",
    "            if min_group_size <= max_lag:\n",
    "                warnings.append(f\"Some groups have {min_group_size} rows but max lag is {max_lag}\")\n",
    "        \n",
    "        total_null_ratio = sum([result_df[col].isna().sum() for col in created_cols]) / (total_rows * len(created_cols))\n",
    "        if total_null_ratio > 0.3:\n",
    "            warnings.append(f\"High null ratio ({total_null_ratio:.1%}) in lag features\")\n",
    "        \n",
    "        if warnings:\n",
    "            print(\"\\nWarnings:\")\n",
    "            for warning in warnings:\n",
    "                print(f\"  - {warning}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating lag features: {str(e)}\")\n",
    "        return df\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b97doxeulf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rolling_features(df, column, window=3, operations=['mean'], group_by=None):\n",
    "    \"\"\"\n",
    "    Create rolling window statistics for a column.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - column: column name to create rolling features for\n",
    "    - window: size of rolling window\n",
    "    - operations: list of operations ['mean', 'sum', 'std', 'min', 'max', 'median']\n",
    "    - group_by: column name to group by (for panel data)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with new rolling feature columns\n",
    "    \"\"\"\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"Column '{column}' not found in DataFrame\")\n",
    "    \n",
    "    if group_by and group_by not in df.columns:\n",
    "        raise ValueError(f\"Group by column '{group_by}' not found in DataFrame\")\n",
    "    \n",
    "    if not pd.api.types.is_numeric_dtype(df[column]):\n",
    "        raise ValueError(f\"Column '{column}' must be numeric for rolling calculations\")\n",
    "    \n",
    "    result_df = df.copy()\n",
    "    created_cols = []\n",
    "    valid_operations = ['mean', 'sum', 'std', 'min', 'max', 'median', 'var', 'count']\n",
    "    \n",
    "    # Validate operations\n",
    "    invalid_ops = [op for op in operations if op not in valid_operations]\n",
    "    if invalid_ops:\n",
    "        raise ValueError(f\"Invalid operations: {invalid_ops}. Valid operations: {valid_operations}\")\n",
    "    \n",
    "    try:\n",
    "        if group_by:\n",
    "            # Create rolling features within each group\n",
    "            grouped = result_df.groupby(group_by)[column]\n",
    "            \n",
    "            for operation in operations:\n",
    "                col_name = f\"{column}_rolling_{window}_{operation}\"\n",
    "                \n",
    "                if operation == 'mean':\n",
    "                    result_df[col_name] = grouped.transform(lambda x: x.rolling(window, min_periods=1).mean())\n",
    "                elif operation == 'sum':\n",
    "                    result_df[col_name] = grouped.transform(lambda x: x.rolling(window, min_periods=1).sum())\n",
    "                elif operation == 'std':\n",
    "                    result_df[col_name] = grouped.transform(lambda x: x.rolling(window, min_periods=2).std())\n",
    "                elif operation == 'min':\n",
    "                    result_df[col_name] = grouped.transform(lambda x: x.rolling(window, min_periods=1).min())\n",
    "                elif operation == 'max':\n",
    "                    result_df[col_name] = grouped.transform(lambda x: x.rolling(window, min_periods=1).max())\n",
    "                elif operation == 'median':\n",
    "                    result_df[col_name] = grouped.transform(lambda x: x.rolling(window, min_periods=1).median())\n",
    "                elif operation == 'var':\n",
    "                    result_df[col_name] = grouped.transform(lambda x: x.rolling(window, min_periods=2).var())\n",
    "                elif operation == 'count':\n",
    "                    result_df[col_name] = grouped.transform(lambda x: x.rolling(window, min_periods=1).count())\n",
    "                \n",
    "                created_cols.append(col_name)\n",
    "        else:\n",
    "            # Create rolling features for entire dataset\n",
    "            for operation in operations:\n",
    "                col_name = f\"{column}_rolling_{window}_{operation}\"\n",
    "                rolling_obj = result_df[column].rolling(window, min_periods=1)\n",
    "                \n",
    "                if operation == 'mean':\n",
    "                    result_df[col_name] = rolling_obj.mean()\n",
    "                elif operation == 'sum':\n",
    "                    result_df[col_name] = rolling_obj.sum()\n",
    "                elif operation == 'std':\n",
    "                    result_df[col_name] = result_df[column].rolling(window, min_periods=2).std()\n",
    "                elif operation == 'min':\n",
    "                    result_df[col_name] = rolling_obj.min()\n",
    "                elif operation == 'max':\n",
    "                    result_df[col_name] = rolling_obj.max()\n",
    "                elif operation == 'median':\n",
    "                    result_df[col_name] = rolling_obj.median()\n",
    "                elif operation == 'var':\n",
    "                    result_df[col_name] = result_df[column].rolling(window, min_periods=2).var()\n",
    "                elif operation == 'count':\n",
    "                    result_df[col_name] = rolling_obj.count()\n",
    "                \n",
    "                created_cols.append(col_name)\n",
    "        \n",
    "        # Calculate and report statistics\n",
    "        total_rows = len(result_df)\n",
    "        print(f\"=== ROLLING FEATURES CREATION RESULTS ===\")\n",
    "        print(f\"Source column: '{column}'\")\n",
    "        print(f\"Window size: {window}\")\n",
    "        if group_by:\n",
    "            print(f\"Grouped by: '{group_by}' ({result_df[group_by].nunique()} unique groups)\")\n",
    "        print(f\"Operations: {operations}\")\n",
    "        print(f\"New columns: {created_cols}\")\n",
    "        \n",
    "        print(f\"\\nRolling features summary:\")\n",
    "        for col_name in created_cols:\n",
    "            valid_values = result_df[col_name].notna().sum()\n",
    "            null_values = result_df[col_name].isna().sum()\n",
    "            \n",
    "            print(f\"  {col_name}:\")\n",
    "            print(f\"    Valid values: {valid_values} ({valid_values/total_rows:.1%})\")\n",
    "            print(f\"    Null values: {null_values}\")\n",
    "            \n",
    "            if valid_values > 0:\n",
    "                stats = result_df[col_name].describe()\n",
    "                print(f\"    Mean: {stats['mean']:.4f}, Std: {stats['std']:.4f}\")\n",
    "                print(f\"    Range: [{stats['min']:.4f}, {stats['max']:.4f}]\")\n",
    "        \n",
    "        # Check for potential issues\n",
    "        warnings = []\n",
    "        \n",
    "        if group_by:\n",
    "            min_group_size = result_df.groupby(group_by).size().min()\n",
    "            if min_group_size < window:\n",
    "                warnings.append(f\"Some groups have {min_group_size} rows but window size is {window}\")\n",
    "        \n",
    "        # Check for high correlation between rolling features\n",
    "        if len(created_cols) > 1:\n",
    "            rolling_corr = result_df[created_cols].corr()\n",
    "            high_corr_pairs = []\n",
    "            for i in range(len(rolling_corr.columns)):\n",
    "                for j in range(i+1, len(rolling_corr.columns)):\n",
    "                    if abs(rolling_corr.iloc[i, j]) > 0.95:\n",
    "                        high_corr_pairs.append((rolling_corr.columns[i], rolling_corr.columns[j], rolling_corr.iloc[i, j]))\n",
    "            \n",
    "            if high_corr_pairs:\n",
    "                warnings.append(f\"High correlation detected between rolling features\")\n",
    "        \n",
    "        if warnings:\n",
    "            print(\"\\nWarnings:\")\n",
    "            for warning in warnings:\n",
    "                print(f\"  - {warning}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating rolling features: {str(e)}\")\n",
    "        return df\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bg0zkc78ctk",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_polynomial_features(df, columns, degree=2):\n",
    "    \"\"\"\n",
    "    Generate polynomial transformations of numeric columns.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - columns: list of column names to create polynomial features for\n",
    "    - degree: polynomial degree (2 = quadratic, 3 = cubic, etc.)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with new polynomial feature columns\n",
    "    \"\"\"\n",
    "    if isinstance(columns, str):\n",
    "        columns = [columns]\n",
    "    \n",
    "    # Validate inputs\n",
    "    for col in columns:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Column '{col}' not found in DataFrame\")\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            raise ValueError(f\"Column '{col}' must be numeric for polynomial features\")\n",
    "    \n",
    "    if degree < 1:\n",
    "        raise ValueError(\"Degree must be at least 1\")\n",
    "    \n",
    "    result_df = df.copy()\n",
    "    created_cols = []\n",
    "    \n",
    "    try:\n",
    "        # Select only the specified numeric columns\n",
    "        feature_data = result_df[columns].copy()\n",
    "        \n",
    "        # Handle missing values by filling with median (PolynomialFeatures doesn't handle NaN)\n",
    "        feature_data_filled = feature_data.fillna(feature_data.median())\n",
    "        \n",
    "        # Create polynomial features using sklearn\n",
    "        poly = PolynomialFeatures(degree=degree, include_bias=False, interaction_only=False)\n",
    "        poly_features = poly.fit_transform(feature_data_filled)\n",
    "        \n",
    "        # Get feature names\n",
    "        feature_names = poly.get_feature_names_out(columns)\n",
    "        \n",
    "        # Add new polynomial features to DataFrame\n",
    "        for i, feature_name in enumerate(feature_names):\n",
    "            # Skip original features (degree 1)\n",
    "            if feature_name not in columns:\n",
    "                # Clean up feature names for readability\n",
    "                clean_name = feature_name.replace(' ', '_').replace('^', '_pow_')\n",
    "                result_df[f\"poly_{clean_name}\"] = poly_features[:, i]\n",
    "                created_cols.append(f\"poly_{clean_name}\")\n",
    "        \n",
    "        # Calculate and report statistics\n",
    "        total_rows = len(result_df)\n",
    "        original_null_count = feature_data.isnull().sum().sum()\n",
    "        \n",
    "        print(f\"=== POLYNOMIAL FEATURES CREATION RESULTS ===\")\n",
    "        print(f\"Source columns: {columns}\")\n",
    "        print(f\"Polynomial degree: {degree}\")\n",
    "        print(f\"Features created: {len(created_cols)}\")\n",
    "        print(f\"New columns: {created_cols[:5]}{'...' if len(created_cols) > 5 else ''}\")\n",
    "        \n",
    "        if original_null_count > 0:\n",
    "            print(f\"Note: {original_null_count} missing values were filled with median before polynomial transformation\")\n",
    "        \n",
    "        print(f\"\\nPolynomial features summary:\")\n",
    "        for col_name in created_cols[:5]:  # Show first 5 features\n",
    "            if col_name in result_df.columns:\n",
    "                valid_values = result_df[col_name].notna().sum()\n",
    "                \n",
    "                if valid_values > 0:\n",
    "                    stats = result_df[col_name].describe()\n",
    "                    print(f\"  {col_name}:\")\n",
    "                    print(f\"    Mean: {stats['mean']:.4f}, Std: {stats['std']:.4f}\")\n",
    "                    print(f\"    Range: [{stats['min']:.4f}, {stats['max']:.4f}]\")\n",
    "        \n",
    "        if len(created_cols) > 5:\n",
    "            print(f\"  ... and {len(created_cols) - 5} more polynomial features\")\n",
    "        \n",
    "        # Check for potential issues\n",
    "        warnings = []\n",
    "        \n",
    "        # Check for extreme values that might cause numerical issues\n",
    "        for col_name in created_cols:\n",
    "            if col_name in result_df.columns:\n",
    "                col_data = result_df[col_name]\n",
    "                if col_data.max() > 1e10 or col_data.min() < -1e10:\n",
    "                    warnings.append(f\"Extreme values in {col_name} - consider scaling original features\")\n",
    "                    break\n",
    "        \n",
    "        # Check for high number of features created\n",
    "        if len(created_cols) > 50:\n",
    "            warnings.append(f\"Large number of features created ({len(created_cols)}) - consider feature selection\")\n",
    "        \n",
    "        # Check for potential multicollinearity\n",
    "        if len(columns) > 1 and degree > 2:\n",
    "            warnings.append(\"High-degree polynomials with multiple variables may cause multicollinearity\")\n",
    "        \n",
    "        if warnings:\n",
    "            print(\"\\nWarnings:\")\n",
    "            for warning in warnings:\n",
    "                print(f\"  - {warning}\")\n",
    "        \n",
    "        # Memory usage information\n",
    "        original_memory = feature_data.memory_usage(deep=True).sum() / 1024**2\n",
    "        new_features_memory = sum([result_df[col].memory_usage(deep=True) for col in created_cols]) / 1024**2\n",
    "        print(f\"\\nMemory impact:\")\n",
    "        print(f\"  Original features: {original_memory:.2f} MB\")\n",
    "        print(f\"  New polynomial features: {new_features_memory:.2f} MB\")\n",
    "        print(f\"  Total increase: {new_features_memory:.2f} MB\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating polynomial features: {str(e)}\")\n",
    "        return df\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c99ee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_docs = \"\"\" Helper functions available:\n",
    "- create_bins(df, column, bins=None, labels=None, method='equal_width'): Create categorical bins from numeric columns. Methods: 'equal_width', 'equal_frequency', 'custom'. Returns DataFrame with new binned column.\n",
    "- create_ratios(df, numerator_col, denominator_col, new_name=None): Calculate ratios between two numeric columns with zero-division handling. Returns DataFrame with new ratio column.\n",
    "- create_interaction_features(df, col1, col2, operation='multiply'): Create interaction features between columns. Operations: 'multiply', 'add', 'subtract', 'divide', 'mean', 'max', 'min'. Returns DataFrame with new interaction feature.\n",
    "- create_lag_features(df, column, lags=[1], group_by=None): Create time series lag features (previous values). Supports grouping for panel data. Returns DataFrame with lag columns.\n",
    "- create_rolling_features(df, column, window=3, operations=['mean'], group_by=None): Create rolling window statistics. Operations: 'mean', 'sum', 'std', 'min', 'max', 'median', 'var', 'count'. Returns DataFrame with rolling feature columns.\n",
    "- create_polynomial_features(df, columns, degree=2): Generate polynomial transformations using sklearn. Creates interaction terms and powers up to specified degree. Returns DataFrame with polynomial features.\n",
    "\n",
    "Examples:\n",
    "- \"Create age groups in 5 bins\" -> df = create_bins(df, 'age', bins=5)\n",
    "- \"Calculate price per square foot\" -> df = create_ratios(df, 'price', 'square_feet')\n",
    "- \"Create interaction between income and education\" -> df = create_interaction_features(df, 'income', 'education', 'multiply')\n",
    "- \"Add 1 and 2 period lags for sales\" -> df = create_lag_features(df, 'sales', lags=[1, 2])\n",
    "- \"Create 7-day rolling average\" -> df = create_rolling_features(df, 'value', window=7, operations=['mean'])\n",
    "- \"Generate quadratic features\" -> df = create_polynomial_features(df, ['feature1', 'feature2'], degree=2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b02315b",
   "metadata": {},
   "source": [
    "# **MAIN FEATURE FUNCTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881f15c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df, user_query):\n",
    "    \"\"\"\n",
    "    Main function that gets called by the main router.\n",
    "    MUST take (df, user_query) and return df\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create message chain\n",
    "    messages = []\n",
    "    messages.append(SystemMessage(content=helper_docs))\n",
    "    messages.append(SystemMessage(content=f\"\"\"\n",
    "    You are a data cleaning agent focused on feature engineering and transformation.\n",
    "    \n",
    "    Dataset info: Shape: {df.shape}, Sample: {df.head(3).to_string()}\n",
    "\n",
    "    Libraries available:\n",
    "    - pd (pandas), np (numpy)\n",
    "    - math, re, datetime\n",
    "    - sklearn.preprocessing, PolynomialFeatures\n",
    "    - All helper functions listed above\n",
    "    \n",
    "    Rules:\n",
    "    - Return only executable Python code, no explanations, no markdown blocks\n",
    "    - Use helper functions for feature engineering tasks\n",
    "    - ASSUME \"df\" IS ALREADY DEFINED\n",
    "    - ALWAYS assign the result back to df when modifying: df = create_bins(df, 'column_name')\n",
    "    - For multiple operations, chain them: df = create_bins(df, 'age'); df = create_ratios(df, 'price', 'sqft')\n",
    "    - In order to generate a response/message to the user use print statements\n",
    "    print(\"message\")\n",
    "    - Write a detailed print message to summarise actions taken and reasons\n",
    "    \n",
    "    Common query patterns and approaches:\n",
    "    - \"Create age groups\" or \"bin ages\" -> df = create_bins(df, 'age', bins=5, method='equal_width')\n",
    "    - \"Calculate ratio\" or \"price per unit\" -> df = create_ratios(df, 'price', 'square_feet')\n",
    "    - \"Create interaction\" or \"multiply features\" -> df = create_interaction_features(df, 'col1', 'col2', 'multiply')\n",
    "    - \"Add lag features\" or \"previous values\" -> df = create_lag_features(df, 'sales', lags=[1, 2, 3])\n",
    "    - \"Rolling average\" or \"moving average\" -> df = create_rolling_features(df, 'price', window=7, operations=['mean'])\n",
    "    - \"Polynomial features\" or \"quadratic terms\" -> df = create_polynomial_features(df, ['feature1', 'feature2'], degree=2)\n",
    "    - \"Engineering features\" -> Use multiple functions as appropriate\n",
    "    \n",
    "    Feature engineering best practices:\n",
    "    - For categorical binning: Use equal_width for uniform ranges, equal_frequency for balanced distributions\n",
    "    - For ratios: Common patterns are price/unit, value/time, rate calculations\n",
    "    - For interactions: Multiply for amplification effects, add for combined effects\n",
    "    - For time series: Use lags for autoregressive patterns, rolling for smoothing trends\n",
    "    - For polynomials: Degree 2-3 usually sufficient, higher degrees risk overfitting\n",
    "    \"\"\"))\n",
    "    messages.append(HumanMessage(content=f\"User request: {user_query}\"))\n",
    "    \n",
    "    # Call LLM with message chain\n",
    "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")\n",
    "    response = llm.invoke(messages)\n",
    "    generated_code = response.content.strip()\n",
    "    \n",
    "    # Execute code\n",
    "    try:\n",
    "        original_df = df.copy()\n",
    "        # Create local namespace with our variables\n",
    "        local_vars = {\n",
    "            'df': df.copy(),\n",
    "            'original_df': original_df,\n",
    "            'pd': pd,\n",
    "            'np': np,\n",
    "            'create_bins': create_bins,\n",
    "            'create_ratios': create_ratios,\n",
    "            'create_interaction_features': create_interaction_features,\n",
    "            'create_lag_features': create_lag_features,\n",
    "            'create_rolling_features': create_rolling_features,\n",
    "            'create_polynomial_features': create_polynomial_features,\n",
    "            'PolynomialFeatures': PolynomialFeatures,\n",
    "            'print': print\n",
    "        }\n",
    "        \n",
    "        exec(generated_code, globals(), local_vars)\n",
    "        return local_vars['df']\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"Generated Code:{generated_code}\")\n",
    "        return original_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30928b6",
   "metadata": {},
   "source": [
    "# **Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b618978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create sample data for feature engineering testing\n",
    "# np.random.seed(42)\n",
    "# n_samples = 100\n",
    "\n",
    "# test_data = {\n",
    "#     'age': np.random.randint(18, 80, n_samples),  # For binning\n",
    "#     'price': np.random.uniform(100, 1000, n_samples),  # For ratios\n",
    "#     'square_feet': np.random.uniform(500, 3000, n_samples),  # For ratios\n",
    "#     'income': np.random.uniform(30000, 150000, n_samples),  # For interactions\n",
    "#     'education_years': np.random.randint(8, 20, n_samples),  # For interactions\n",
    "#     'sales': np.cumsum(np.random.normal(100, 20, n_samples)),  # For time series features\n",
    "#     'customer_id': np.repeat(range(1, 21), 5),  # For grouped operations\n",
    "#     'time_period': np.tile(range(1, 6), 20),  # Time dimension\n",
    "#     'feature1': np.random.uniform(-2, 2, n_samples),  # For polynomial features\n",
    "#     'feature2': np.random.uniform(-1, 3, n_samples),  # For polynomial features\n",
    "# }\n",
    "\n",
    "# test_df = pd.DataFrame(test_data)\n",
    "# print(\"Test DataFrame created for feature engineering:\")\n",
    "# print(f\"Shape: {test_df.shape}\")\n",
    "# print(\"\\\\nColumns and data types:\")\n",
    "# print(test_df.dtypes)\n",
    "# print(\"\\\\nSample data:\")\n",
    "# print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9610f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test various feature engineering operations\n",
    "# print(\"=== TESTING FEATURE ENGINEERING FUNCTIONS ===\\\\n\")\n",
    "\n",
    "# # Test 1: Binning\n",
    "# print(\"1. Testing binning:\")\n",
    "# query1 = \"Create age groups in 5 equal-width bins\"\n",
    "# result1 = feature_engineering(test_df.copy(), query1)\n",
    "\n",
    "# # Test 2: Ratios\n",
    "# print(\"\\\\n2. Testing ratios:\")\n",
    "# query2 = \"Calculate price per square foot ratio\"\n",
    "# result2 = feature_engineering(test_df.copy(), query2)\n",
    "\n",
    "# # Test 3: Interaction features\n",
    "# print(\"\\\\n3. Testing interaction features:\")\n",
    "# query3 = \"Create interaction between income and education by multiplying them\"\n",
    "# result3 = feature_engineering(test_df.copy(), query3)\n",
    "\n",
    "# # Test 4: Lag features\n",
    "# print(\"\\\\n4. Testing lag features:\")\n",
    "# query4 = \"Add 1 and 2 period lag features for sales grouped by customer_id\"\n",
    "# result4 = feature_engineering(test_df.copy(), query4)\n",
    "\n",
    "# # Test 5: Rolling features\n",
    "# print(\"\\\\n5. Testing rolling features:\")\n",
    "# query5 = \"Create 3-period rolling mean and standard deviation for sales by customer\"\n",
    "# result5 = feature_engineering(test_df.copy(), query5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f9c065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test 6: Polynomial features\n",
    "# print(\"\\\\n6. Testing polynomial features:\")\n",
    "# query6 = \"Generate quadratic polynomial features for feature1 and feature2\"\n",
    "# result6 = feature_engineering(test_df.copy(), query6)\n",
    "\n",
    "# print(\"\\\\n=== FEATURE ENGINEERING TESTING COMPLETE ===\")\n",
    "# print(f\"Original columns: {len(test_df.columns)}\")\n",
    "# print(f\"Final columns after all tests: {len(result6.columns)}\")\n",
    "# print(f\"New columns added: {len(result6.columns) - len(test_df.columns)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-cleaning-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
