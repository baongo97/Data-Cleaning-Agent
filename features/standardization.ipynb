{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "691226d2",
   "metadata": {},
   "source": [
    "## **Feature:** Data Standardization\n",
    "\n",
    "**Names:** Gia Bao Ngo\n",
    "\n",
    "### **What it does**\n",
    "Provides comprehensive data standardization and preprocessing capabilities for machine learning preparation. Includes normalization, scaling, categorical encoding, rare category handling, and dummy variable creation with multicollinearity handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff1f7555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dotenv\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Get API Key\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"OpenAI API Key not found\")\n",
    "\n",
    "# Import libraries\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Additional imports for standardization\n",
    "import math\n",
    "import re\n",
    "import datetime\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Langchain imports\n",
    "from langchain_openai import ChatOpenAI  \n",
    "from langchain.schema import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a88657",
   "metadata": {},
   "source": [
    "### **Helper Functions**\n",
    "- `normalize_columns(df, columns=None, method='minmax')` - Min-max normalization (0-1 scale)\n",
    "- `scale_columns(df, columns=None, method='standard')` - Standard/robust scaling (z-score/robust)\n",
    "- `encode_categorical(df, columns=None, method='onehot')` - Categorical encoding (onehot, label, target)\n",
    "- `handle_rare_categories(df, column, threshold=0.01, replace_with='Other')` - Rare category handling\n",
    "- `create_dummy_variables(df, columns=None, drop_first=True)` - Dummy variable creation with multicollinearity handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60424d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_columns(df, columns=None, method='minmax'):\n",
    "    \"\"\"\n",
    "    Normalize numeric columns to 0-1 scale using different methods.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - columns: list of column names to normalize (None = all numeric columns)\n",
    "    - method: 'minmax' (0-1), 'maxabs' (-1 to 1), or 'unit' (unit vector scaling)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with normalized columns\n",
    "    \"\"\"\n",
    "    result_df = df.copy()\n",
    "    normalized_cols = []\n",
    "    \n",
    "    if columns is None:\n",
    "        columns = result_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Validate columns exist and are numeric\n",
    "    valid_columns = []\n",
    "    for col in columns:\n",
    "        if col not in result_df.columns:\n",
    "            print(f\"Warning: Column '{col}' not found in DataFrame\")\n",
    "            continue\n",
    "        if not pd.api.types.is_numeric_dtype(result_df[col]):\n",
    "            print(f\"Warning: Column '{col}' is not numeric, skipping\")\n",
    "            continue\n",
    "        valid_columns.append(col)\n",
    "    \n",
    "    if not valid_columns:\n",
    "        print(\"No valid numeric columns found for normalization\")\n",
    "        return result_df\n",
    "    \n",
    "    try:\n",
    "        if method == 'minmax':\n",
    "            scaler = MinMaxScaler()\n",
    "        elif method == 'maxabs':\n",
    "            from sklearn.preprocessing import MaxAbsScaler\n",
    "            scaler = MaxAbsScaler()\n",
    "        elif method == 'unit':\n",
    "            from sklearn.preprocessing import Normalizer\n",
    "            # Unit scaling works on samples (rows), not features\n",
    "            for col in valid_columns:\n",
    "                # Apply unit scaling to each column individually\n",
    "                values = result_df[col].values.reshape(-1, 1)\n",
    "                normalizer = Normalizer()\n",
    "                normalized_values = normalizer.fit_transform(values).flatten()\n",
    "                result_df[col] = normalized_values\n",
    "                normalized_cols.append({\n",
    "                    'column': col,\n",
    "                    'method': 'unit',\n",
    "                    'min_val': result_df[col].min(),\n",
    "                    'max_val': result_df[col].max()\n",
    "                })\n",
    "            \n",
    "            print(f\"=== NORMALIZATION RESULTS ({method.upper()}) ===\")\n",
    "            print(f\"Columns normalized: {len(normalized_cols)}\")\n",
    "            for norm in normalized_cols:\n",
    "                print(f\"  {norm['column']}: range [{norm['min_val']:.4f}, {norm['max_val']:.4f}]\")\n",
    "            return result_df\n",
    "        else:\n",
    "            print(f\"Error: Unknown normalization method '{method}'. Use 'minmax', 'maxabs', or 'unit'\")\n",
    "            return result_df\n",
    "        \n",
    "        # Apply scaling for minmax and maxabs methods\n",
    "        for col in valid_columns:\n",
    "            original_values = result_df[col].copy()\n",
    "            # Handle missing values\n",
    "            non_null_mask = result_df[col].notna()\n",
    "            \n",
    "            if non_null_mask.sum() == 0:\n",
    "                print(f\"Warning: Column '{col}' has no non-null values, skipping\")\n",
    "                continue\n",
    "                \n",
    "            # Fit and transform only non-null values\n",
    "            values_to_scale = result_df.loc[non_null_mask, col].values.reshape(-1, 1)\n",
    "            scaled_values = scaler.fit_transform(values_to_scale).flatten()\n",
    "            \n",
    "            # Update the column\n",
    "            result_df.loc[non_null_mask, col] = scaled_values\n",
    "            \n",
    "            normalized_cols.append({\n",
    "                'column': col,\n",
    "                'method': method,\n",
    "                'original_min': original_values.min(),\n",
    "                'original_max': original_values.max(),\n",
    "                'new_min': result_df[col].min(),\n",
    "                'new_max': result_df[col].max()\n",
    "            })\n",
    "        \n",
    "        print(f\"=== NORMALIZATION RESULTS ({method.upper()}) ===\")\n",
    "        print(f\"Columns normalized: {len(normalized_cols)}\")\n",
    "        \n",
    "        if normalized_cols:\n",
    "            print(\"\\nNormalization details:\")\n",
    "            for norm in normalized_cols:\n",
    "                print(f\"  {norm['column']}: [{norm['original_min']:.2f}, {norm['original_max']:.2f}] → [{norm['new_min']:.4f}, {norm['new_max']:.4f}]\")\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during normalization: {e}\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46a090e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_columns(df, columns=None, method='standard'):\n",
    "    \"\"\"\n",
    "    Scale numeric columns using z-score (standard) or robust scaling.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - columns: list of column names to scale (None = all numeric columns)\n",
    "    - method: 'standard' (z-score), 'robust' (median/IQR), or 'quantile' (uniform distribution)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with scaled columns\n",
    "    \"\"\"\n",
    "    result_df = df.copy()\n",
    "    scaled_cols = []\n",
    "    \n",
    "    if columns is None:\n",
    "        columns = result_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Validate columns exist and are numeric\n",
    "    valid_columns = []\n",
    "    for col in columns:\n",
    "        if col not in result_df.columns:\n",
    "            print(f\"Warning: Column '{col}' not found in DataFrame\")\n",
    "            continue\n",
    "        if not pd.api.types.is_numeric_dtype(result_df[col]):\n",
    "            print(f\"Warning: Column '{col}' is not numeric, skipping\")\n",
    "            continue\n",
    "        valid_columns.append(col)\n",
    "    \n",
    "    if not valid_columns:\n",
    "        print(\"No valid numeric columns found for scaling\")\n",
    "        return result_df\n",
    "    \n",
    "    try:\n",
    "        # Choose scaler based on method\n",
    "        if method == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "        elif method == 'robust':\n",
    "            scaler = RobustScaler()\n",
    "        elif method == 'quantile':\n",
    "            from sklearn.preprocessing import QuantileTransformer\n",
    "            scaler = QuantileTransformer(output_distribution='normal')\n",
    "        else:\n",
    "            print(f\"Error: Unknown scaling method '{method}'. Use 'standard', 'robust', or 'quantile'\")\n",
    "            return result_df\n",
    "        \n",
    "        # Apply scaling to each column\n",
    "        for col in valid_columns:\n",
    "            original_values = result_df[col].copy()\n",
    "            # Handle missing values\n",
    "            non_null_mask = result_df[col].notna()\n",
    "            \n",
    "            if non_null_mask.sum() == 0:\n",
    "                print(f\"Warning: Column '{col}' has no non-null values, skipping\")\n",
    "                continue\n",
    "            \n",
    "            # Get original statistics\n",
    "            original_mean = original_values.mean()\n",
    "            original_std = original_values.std()\n",
    "            original_median = original_values.median()\n",
    "            \n",
    "            # Fit and transform only non-null values\n",
    "            values_to_scale = result_df.loc[non_null_mask, col].values.reshape(-1, 1)\n",
    "            scaled_values = scaler.fit_transform(values_to_scale).flatten()\n",
    "            \n",
    "            # Update the column\n",
    "            result_df.loc[non_null_mask, col] = scaled_values\n",
    "            \n",
    "            # Calculate new statistics\n",
    "            new_mean = result_df[col].mean()\n",
    "            new_std = result_df[col].std()\n",
    "            new_median = result_df[col].median()\n",
    "            \n",
    "            scaled_cols.append({\n",
    "                'column': col,\n",
    "                'method': method,\n",
    "                'original_mean': original_mean,\n",
    "                'original_std': original_std,\n",
    "                'original_median': original_median,\n",
    "                'new_mean': new_mean,\n",
    "                'new_std': new_std,\n",
    "                'new_median': new_median\n",
    "            })\n",
    "        \n",
    "        print(f\"=== SCALING RESULTS ({method.upper()}) ===\")\n",
    "        print(f\"Columns scaled: {len(scaled_cols)}\")\n",
    "        \n",
    "        if scaled_cols:\n",
    "            print(\"\\nScaling details:\")\n",
    "            for scale in scaled_cols:\n",
    "                if method == 'standard':\n",
    "                    print(f\"  {scale['column']}: mean {scale['original_mean']:.2f}→{scale['new_mean']:.4f}, \"\n",
    "                          f\"std {scale['original_std']:.2f}→{scale['new_std']:.4f}\")\n",
    "                elif method == 'robust':\n",
    "                    print(f\"  {scale['column']}: median {scale['original_median']:.2f}→{scale['new_median']:.4f}, \"\n",
    "                          f\"std {scale['original_std']:.2f}→{scale['new_std']:.4f}\")\n",
    "                else:  # quantile\n",
    "                    print(f\"  {scale['column']}: transformed to normal distribution, \"\n",
    "                          f\"std {scale['original_std']:.2f}→{scale['new_std']:.4f}\")\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during scaling: {e}\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "zx0v2ey6ufe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical(df, columns=None, method='onehot'):\n",
    "    \"\"\"\n",
    "    Encode categorical columns using different methods.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - columns: list of column names to encode (None = all object/category columns)\n",
    "    - method: 'onehot', 'label', 'target' (requires target column), or 'binary'\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with encoded columns\n",
    "    \"\"\"\n",
    "    result_df = df.copy()\n",
    "    encoded_info = []\n",
    "    \n",
    "    if columns is None:\n",
    "        columns = result_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    # Validate columns exist and are categorical\n",
    "    valid_columns = []\n",
    "    for col in columns:\n",
    "        if col not in result_df.columns:\n",
    "            print(f\"Warning: Column '{col}' not found in DataFrame\")\n",
    "            continue\n",
    "        if not (result_df[col].dtype == 'object' or result_df[col].dtype.name == 'category'):\n",
    "            print(f\"Warning: Column '{col}' is not categorical, skipping\")\n",
    "            continue\n",
    "        valid_columns.append(col)\n",
    "    \n",
    "    if not valid_columns:\n",
    "        print(\"No valid categorical columns found for encoding\")\n",
    "        return result_df\n",
    "    \n",
    "    try:\n",
    "        if method == 'onehot':\n",
    "            # One-hot encoding\n",
    "            for col in valid_columns:\n",
    "                # Get unique values (excluding NaN)\n",
    "                unique_vals = result_df[col].dropna().unique()\n",
    "                n_categories = len(unique_vals)\n",
    "                \n",
    "                if n_categories > 50:\n",
    "                    print(f\"Warning: Column '{col}' has {n_categories} categories (>50). Consider using label encoding or handling rare categories first.\")\n",
    "                    continue\n",
    "                \n",
    "                # Create one-hot encoded columns\n",
    "                dummies = pd.get_dummies(result_df[col], prefix=col, dummy_na=False)\n",
    "                \n",
    "                # Drop original column and add dummy columns\n",
    "                result_df = result_df.drop(columns=[col])\n",
    "                result_df = pd.concat([result_df, dummies], axis=1)\n",
    "                \n",
    "                encoded_info.append({\n",
    "                    'original_column': col,\n",
    "                    'method': 'onehot',\n",
    "                    'n_categories': n_categories,\n",
    "                    'new_columns': list(dummies.columns),\n",
    "                    'n_new_columns': len(dummies.columns)\n",
    "                })\n",
    "        \n",
    "        elif method == 'label':\n",
    "            # Label encoding\n",
    "            for col in valid_columns:\n",
    "                encoder = LabelEncoder()\n",
    "                non_null_mask = result_df[col].notna()\n",
    "                \n",
    "                if non_null_mask.sum() == 0:\n",
    "                    print(f\"Warning: Column '{col}' has no non-null values, skipping\")\n",
    "                    continue\n",
    "                \n",
    "                # Fit and transform non-null values\n",
    "                encoded_values = encoder.fit_transform(result_df.loc[non_null_mask, col])\n",
    "                \n",
    "                # Create new column and preserve nulls\n",
    "                result_df[f\"{col}_encoded\"] = np.nan\n",
    "                result_df.loc[non_null_mask, f\"{col}_encoded\"] = encoded_values\n",
    "                \n",
    "                # Convert to appropriate integer type\n",
    "                max_label = encoded_values.max()\n",
    "                if max_label <= 127:\n",
    "                    result_df[f\"{col}_encoded\"] = result_df[f\"{col}_encoded\"].astype('Int8')\n",
    "                elif max_label <= 32767:\n",
    "                    result_df[f\"{col}_encoded\"] = result_df[f\"{col}_encoded\"].astype('Int16')\n",
    "                else:\n",
    "                    result_df[f\"{col}_encoded\"] = result_df[f\"{col}_encoded\"].astype('Int32')\n",
    "                \n",
    "                # Drop original column\n",
    "                result_df = result_df.drop(columns=[col])\n",
    "                \n",
    "                encoded_info.append({\n",
    "                    'original_column': col,\n",
    "                    'method': 'label',\n",
    "                    'n_categories': len(encoder.classes_),\n",
    "                    'new_columns': [f\"{col}_encoded\"],\n",
    "                    'label_mapping': dict(zip(encoder.classes_, range(len(encoder.classes_))))\n",
    "                })\n",
    "        \n",
    "        elif method == 'binary':\n",
    "            # Binary encoding (for high-cardinality categorical variables)\n",
    "            for col in valid_columns:\n",
    "                # Get unique values\n",
    "                unique_vals = result_df[col].dropna().unique()\n",
    "                n_categories = len(unique_vals)\n",
    "                \n",
    "                # Calculate number of binary columns needed\n",
    "                import math\n",
    "                n_binary_cols = math.ceil(math.log2(n_categories)) if n_categories > 1 else 1\n",
    "                \n",
    "                # Create label encoding first\n",
    "                encoder = LabelEncoder()\n",
    "                non_null_mask = result_df[col].notna()\n",
    "                \n",
    "                if non_null_mask.sum() == 0:\n",
    "                    print(f\"Warning: Column '{col}' has no non-null values, skipping\")\n",
    "                    continue\n",
    "                \n",
    "                encoded_values = encoder.fit_transform(result_df.loc[non_null_mask, col])\n",
    "                \n",
    "                # Convert to binary representation\n",
    "                binary_cols = []\n",
    "                for i in range(n_binary_cols):\n",
    "                    col_name = f\"{col}_binary_{i}\"\n",
    "                    binary_cols.append(col_name)\n",
    "                    result_df[col_name] = 0\n",
    "                    \n",
    "                    # Set binary digits\n",
    "                    binary_values = (encoded_values >> i) & 1\n",
    "                    result_df.loc[non_null_mask, col_name] = binary_values\n",
    "                \n",
    "                # Drop original column\n",
    "                result_df = result_df.drop(columns=[col])\n",
    "                \n",
    "                encoded_info.append({\n",
    "                    'original_column': col,\n",
    "                    'method': 'binary',\n",
    "                    'n_categories': n_categories,\n",
    "                    'new_columns': binary_cols,\n",
    "                    'n_binary_columns': n_binary_cols\n",
    "                })\n",
    "        \n",
    "        else:\n",
    "            print(f\"Error: Unknown encoding method '{method}'. Use 'onehot', 'label', or 'binary'\")\n",
    "            return result_df\n",
    "        \n",
    "        print(f\"=== CATEGORICAL ENCODING RESULTS ({method.upper()}) ===\")\n",
    "        print(f\"Columns processed: {len(valid_columns)}\")\n",
    "        print(f\"Columns encoded successfully: {len(encoded_info)}\")\n",
    "        \n",
    "        if encoded_info:\n",
    "            print(\"\\nEncoding details:\")\n",
    "            for enc in encoded_info:\n",
    "                if method == 'onehot':\n",
    "                    print(f\"  {enc['original_column']}: {enc['n_categories']} categories → {enc['n_new_columns']} dummy columns\")\n",
    "                elif method == 'label':\n",
    "                    print(f\"  {enc['original_column']}: {enc['n_categories']} categories → 1 label-encoded column\")\n",
    "                elif method == 'binary':\n",
    "                    print(f\"  {enc['original_column']}: {enc['n_categories']} categories → {enc['n_binary_columns']} binary columns\")\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during categorical encoding: {e}\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ak0phiv1mhv",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_rare_categories(df, column, threshold=0.01, replace_with='Other'):\n",
    "    \"\"\"\n",
    "    Handle rare categories by grouping them into a single category.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - column: column name to process\n",
    "    - threshold: frequency threshold (0.01 = 1% of total values)\n",
    "    - replace_with: replacement value for rare categories\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with rare categories handled\n",
    "    \"\"\"\n",
    "    if column not in df.columns:\n",
    "        print(f\"Error: Column '{column}' not found in DataFrame\")\n",
    "        return df\n",
    "    \n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Calculate value counts and frequencies\n",
    "    value_counts = result_df[column].value_counts(dropna=False)\n",
    "    total_count = len(result_df)\n",
    "    frequencies = value_counts / total_count\n",
    "    \n",
    "    # Identify rare categories\n",
    "    rare_categories = frequencies[frequencies < threshold].index.tolist()\n",
    "    common_categories = frequencies[frequencies >= threshold].index.tolist()\n",
    "    \n",
    "    if len(rare_categories) == 0:\n",
    "        print(f\"No rare categories found in column '{column}' (threshold: {threshold:.1%})\")\n",
    "        return result_df\n",
    "    \n",
    "    # Replace rare categories\n",
    "    result_df[column] = result_df[column].replace(rare_categories, replace_with)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    rare_count = sum(value_counts[cat] for cat in rare_categories)\n",
    "    rare_percent = (rare_count / total_count) * 100\n",
    "    \n",
    "    print(f\"=== RARE CATEGORY HANDLING RESULTS ===\")\n",
    "    print(f\"Column: {column}\")\n",
    "    print(f\"Threshold: {threshold:.1%}\")\n",
    "    print(f\"Original unique categories: {len(value_counts)}\")\n",
    "    print(f\"Rare categories found: {len(rare_categories)}\")\n",
    "    print(f\"Rare categories affected {rare_count} rows ({rare_percent:.1f}% of data)\")\n",
    "    print(f\"Categories after grouping: {result_df[column].nunique()}\")\n",
    "    \n",
    "    if len(rare_categories) <= 10:  # Show rare categories if not too many\n",
    "        print(f\"\\\\nRare categories replaced: {rare_categories}\")\n",
    "    else:\n",
    "        print(f\"\\\\nFirst 10 rare categories: {rare_categories[:10]}... (+{len(rare_categories)-10} more)\")\n",
    "    \n",
    "    # Show final category distribution\n",
    "    final_counts = result_df[column].value_counts().head(10)\n",
    "    print(f\"\\\\nTop categories after processing:\")\n",
    "    for cat, count in final_counts.items():\n",
    "        pct = (count / total_count) * 100\n",
    "        print(f\"  {cat}: {count} ({pct:.1f}%)\")\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3li7u8qejhh",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy_variables(df, columns=None, drop_first=True):\n",
    "    \"\"\"\n",
    "    Create dummy variables with multicollinearity handling.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - columns: list of column names to process (None = all object/category columns)\n",
    "    - drop_first: drop first dummy to avoid multicollinearity\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with dummy variables created\n",
    "    \"\"\"\n",
    "    result_df = df.copy()\n",
    "    dummy_info = []\n",
    "    \n",
    "    if columns is None:\n",
    "        columns = result_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    # Validate columns\n",
    "    valid_columns = []\n",
    "    for col in columns:\n",
    "        if col not in result_df.columns:\n",
    "            print(f\"Warning: Column '{col}' not found in DataFrame\")\n",
    "            continue\n",
    "        if not (result_df[col].dtype == 'object' or result_df[col].dtype.name == 'category'):\n",
    "            print(f\"Warning: Column '{col}' is not categorical, skipping\")\n",
    "            continue\n",
    "        valid_columns.append(col)\n",
    "    \n",
    "    if not valid_columns:\n",
    "        print(\"No valid categorical columns found for dummy variable creation\")\n",
    "        return result_df\n",
    "    \n",
    "    try:\n",
    "        for col in valid_columns:\n",
    "            # Get unique values info\n",
    "            unique_vals = result_df[col].dropna().unique()\n",
    "            n_categories = len(unique_vals)\n",
    "            \n",
    "            if n_categories == 1:\n",
    "                print(f\"Warning: Column '{col}' has only 1 unique value, skipping\")\n",
    "                continue\n",
    "            \n",
    "            if n_categories > 100:\n",
    "                print(f\"Warning: Column '{col}' has {n_categories} categories (>100). Consider handling rare categories first.\")\n",
    "                continue\n",
    "            \n",
    "            # Create dummy variables\n",
    "            dummies = pd.get_dummies(result_df[col], prefix=col, dummy_na=False, drop_first=drop_first)\n",
    "            \n",
    "            # Handle multicollinearity check\n",
    "            if drop_first and n_categories > 1:\n",
    "                n_dummies = n_categories - 1\n",
    "                dropped_category = unique_vals[0]  # First category (alphabetically) is dropped by default\n",
    "            else:\n",
    "                n_dummies = n_categories\n",
    "                dropped_category = None\n",
    "            \n",
    "            # Add dummies to result DataFrame\n",
    "            for dummy_col in dummies.columns:\n",
    "                result_df[dummy_col] = dummies[dummy_col]\n",
    "            \n",
    "            # Remove original column\n",
    "            result_df = result_df.drop(columns=[col])\n",
    "            \n",
    "            dummy_info.append({\n",
    "                'original_column': col,\n",
    "                'n_original_categories': n_categories,\n",
    "                'n_dummy_variables': n_dummies,\n",
    "                'dummy_columns': list(dummies.columns),\n",
    "                'dropped_first': drop_first,\n",
    "                'dropped_category': dropped_category\n",
    "            })\n",
    "        \n",
    "        print(f\"=== DUMMY VARIABLE CREATION RESULTS ===\")\n",
    "        print(f\"Columns processed: {len(valid_columns)}\")\n",
    "        print(f\"Columns successfully converted: {len(dummy_info)}\")\n",
    "        print(f\"Drop first category: {drop_first}\")\n",
    "        \n",
    "        total_original_cols = len(valid_columns)\n",
    "        total_dummy_cols = sum([info['n_dummy_variables'] for info in dummy_info])\n",
    "        net_change = total_dummy_cols - total_original_cols\n",
    "        \n",
    "        print(f\"\\\\nColumn count impact:\")\n",
    "        print(f\"  Original categorical columns: {total_original_cols}\")\n",
    "        print(f\"  New dummy columns: {total_dummy_cols}\")\n",
    "        print(f\"  Net change: {net_change:+d} columns\")\n",
    "        \n",
    "        if dummy_info:\n",
    "            print(f\"\\\\nDummy variable details:\")\n",
    "            for info in dummy_info:\n",
    "                dropped_info = f\" (dropped: {info['dropped_category']})\" if info['dropped_category'] else \"\"\n",
    "                print(f\"  {info['original_column']}: {info['n_original_categories']} categories → {info['n_dummy_variables']} dummies{dropped_info}\")\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during dummy variable creation: {e}\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c99ee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_docs = \"\"\" Helper functions available:\n",
    "- normalize_columns(df, columns=None, method='minmax'): Normalize numeric columns to 0-1 scale. Methods: 'minmax' (0-1), 'maxabs' (-1 to 1), 'unit' (unit vector). Returns DataFrame with normalized columns.\n",
    "- scale_columns(df, columns=None, method='standard'): Scale numeric columns using z-score or robust scaling. Methods: 'standard' (z-score), 'robust' (median/IQR), 'quantile' (uniform distribution). Returns DataFrame with scaled columns.\n",
    "- encode_categorical(df, columns=None, method='onehot'): Encode categorical columns. Methods: 'onehot' (dummy variables), 'label' (integer encoding), 'binary' (binary representation). Returns DataFrame with encoded columns.\n",
    "- handle_rare_categories(df, column, threshold=0.01, replace_with='Other'): Group rare categories below threshold into single category. Returns DataFrame with rare categories handled.\n",
    "- create_dummy_variables(df, columns=None, drop_first=True): Create dummy variables with multicollinearity handling. Drop_first prevents perfect collinearity. Returns DataFrame with dummy columns.\n",
    "\n",
    "Examples:\n",
    "- \"Normalize numeric columns\" -> df = normalize_columns(df)\n",
    "- \"Scale features for machine learning\" -> df = scale_columns(df, method='standard')\n",
    "- \"One-hot encode categorical variables\" -> df = encode_categorical(df, method='onehot')\n",
    "- \"Handle rare categories in city column\" -> df = handle_rare_categories(df, 'city', threshold=0.05)\n",
    "- \"Create dummy variables\" -> df = create_dummy_variables(df, drop_first=True)\n",
    "- \"Normalize with min-max scaling\" -> df = normalize_columns(df, method='minmax')\n",
    "- \"Use robust scaling for outlier-resistant standardization\" -> df = scale_columns(df, method='robust')\n",
    "- \"Label encode all categorical columns\" -> df = encode_categorical(df, method='label')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b02315b",
   "metadata": {},
   "source": [
    "# **MAIN FEATURE FUNCTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "881f15c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardization(df, user_query):\n",
    "    \"\"\"\n",
    "    Main function that gets called by the main router.\n",
    "    MUST take (df, user_query) and return df\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create message chain\n",
    "    messages = []\n",
    "    messages.append(SystemMessage(content=helper_docs))\n",
    "    messages.append(SystemMessage(content=f\"\"\"\n",
    "    You are a data cleaning agent focused on data standardization and preprocessing for machine learning.\n",
    "    \n",
    "    Dataset info: Shape: {df.shape}, Sample: {df.head(3).to_string()}\n",
    "\n",
    "    Libraries available:\n",
    "    - pd (pandas), np (numpy)\n",
    "    - math, re, datetime\n",
    "    - sklearn.preprocessing (StandardScaler, RobustScaler, MinMaxScaler, LabelEncoder, OneHotEncoder)\n",
    "    - All helper functions listed above\n",
    "    \n",
    "    Rules:\n",
    "    - Return only executable Python code, no explanations, no markdown blocks\n",
    "    - Use helper functions when appropriate for standardization tasks\n",
    "    - ASSUME \"df\" IS ALREADY DEFINED\n",
    "    - For normalization/scaling, use helper functions that modify DataFrame\n",
    "    - For categorical encoding, use appropriate helper functions based on query\n",
    "    - ALWAYS assign the result back to df when modifying: df = normalize_columns(df)\n",
    "    - In order to generate a response/message to the user use print statements\n",
    "    print(\"message\")\n",
    "    - Write a detailed print message to summarise actions taken and reasons\n",
    "    \n",
    "    Common query patterns:\n",
    "    - \"Normalize\" or \"normalize columns\" -> df = normalize_columns(df)\n",
    "    - \"Scale\" or \"standardize\" -> df = scale_columns(df, method='standard')\n",
    "    - \"Min-max scaling\" -> df = normalize_columns(df, method='minmax')\n",
    "    - \"Robust scaling\" -> df = scale_columns(df, method='robust')\n",
    "    - \"One-hot encode\" -> df = encode_categorical(df, method='onehot')\n",
    "    - \"Label encode\" -> df = encode_categorical(df, method='label')\n",
    "    - \"Handle rare categories\" -> df = handle_rare_categories(df, 'column_name', threshold=0.05)\n",
    "    - \"Create dummy variables\" -> df = create_dummy_variables(df)\n",
    "    - \"Prepare for machine learning\" -> Apply scaling + encoding as appropriate\n",
    "    \"\"\"))\n",
    "    messages.append(HumanMessage(content=f\"User request: {user_query}\"))\n",
    "    \n",
    "    # Call LLM with message chain\n",
    "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")\n",
    "    response = llm.invoke(messages)\n",
    "    generated_code = response.content.strip()\n",
    "    \n",
    "    # Execute code\n",
    "    try:\n",
    "        original_df = df.copy()\n",
    "        # Create local namespace with our variables\n",
    "        local_vars = {\n",
    "            'df': df.copy(),\n",
    "            'original_df': original_df,\n",
    "            'pd': pd,\n",
    "            'np': np,\n",
    "            'normalize_columns': normalize_columns,\n",
    "            'scale_columns': scale_columns,\n",
    "            'encode_categorical': encode_categorical,\n",
    "            'handle_rare_categories': handle_rare_categories,\n",
    "            'create_dummy_variables': create_dummy_variables,\n",
    "            'StandardScaler': StandardScaler,\n",
    "            'RobustScaler': RobustScaler,\n",
    "            'MinMaxScaler': MinMaxScaler,\n",
    "            'LabelEncoder': LabelEncoder,\n",
    "            'OneHotEncoder': OneHotEncoder,\n",
    "            'print': print\n",
    "        }\n",
    "        \n",
    "        exec(generated_code, globals(), local_vars)\n",
    "        return local_vars['df']\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"Generated Code:{generated_code}\")\n",
    "        return original_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30928b6",
   "metadata": {},
   "source": [
    "# **Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b618978f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test DataFrame created:\n",
      "Shape: (100, 7)\n",
      "\\nData types:\n",
      "numeric_1              int64\n",
      "numeric_2              int64\n",
      "numeric_3              int64\n",
      "category_small        object\n",
      "category_medium       object\n",
      "category_with_rare    object\n",
      "binary_like           object\n",
      "dtype: object\n",
      "\\nFirst few rows:\n",
      "   numeric_1  numeric_2  numeric_3 category_small category_medium  \\\n",
      "0          1         60          1              A           Cat_0   \n",
      "1          2         70          4              B           Cat_1   \n",
      "2          3         80          9              C           Cat_2   \n",
      "3          4         90         16              A           Cat_3   \n",
      "4          5        100         25              B           Cat_4   \n",
      "\n",
      "  category_with_rare binary_like  \n",
      "0            Common1         Yes  \n",
      "1            Common1          No  \n",
      "2            Common1         Yes  \n",
      "3            Common1          No  \n",
      "4            Common1         Yes  \n",
      "\\nCategory distributions:\n",
      "\\ncategory_small: 3 unique values\n",
      "category_small\n",
      "A    34\n",
      "B    33\n",
      "C    33\n",
      "Name: count, dtype: int64\n",
      "\\ncategory_medium: 10 unique values\n",
      "category_medium\n",
      "Cat_0    10\n",
      "Cat_1    10\n",
      "Cat_2    10\n",
      "Cat_3    10\n",
      "Cat_4    10\n",
      "Name: count, dtype: int64\n",
      "\\ncategory_with_rare: 13 unique values\n",
      "category_with_rare\n",
      "Common1    40\n",
      "Common2    30\n",
      "Common3    20\n",
      "Rare_0      1\n",
      "Rare_1      1\n",
      "Name: count, dtype: int64\n",
      "\\nbinary_like: 2 unique values\n",
      "binary_like\n",
      "Yes    50\n",
      "No     50\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# # Create sample data with various standardization opportunities\n",
    "# test_data = {\n",
    "#     'numeric_1': range(1, 101),  # Can be normalized/scaled\n",
    "#     'numeric_2': [x*10 + 50 for x in range(1, 101)],  # Different scale\n",
    "#     'numeric_3': [x**2 for x in range(1, 101)],  # Non-linear distribution\n",
    "#     'category_small': ['A', 'B', 'C'] * 33 + ['A'],  # Low cardinality - good for one-hot\n",
    "#     'category_medium': [f'Cat_{i%10}' for i in range(100)],  # Medium cardinality\n",
    "#     'category_with_rare': ['Common1'] * 40 + ['Common2'] * 30 + ['Common3'] * 20 + \n",
    "#                          [f'Rare_{i}' for i in range(10)],  # Has rare categories\n",
    "#     'binary_like': ['Yes', 'No'] * 50,  # Binary categorical\n",
    "# }\n",
    "\n",
    "# test_df = pd.DataFrame(test_data)\n",
    "# print(\"Test DataFrame created:\")\n",
    "# print(f\"Shape: {test_df.shape}\")\n",
    "# print(\"\\\\nData types:\")\n",
    "# print(test_df.dtypes)\n",
    "# print(\"\\\\nFirst few rows:\")\n",
    "# print(test_df.head())\n",
    "# print(\"\\\\nCategory distributions:\")\n",
    "# for col in ['category_small', 'category_medium', 'category_with_rare', 'binary_like']:\n",
    "#     print(f\"\\\\n{col}: {test_df[col].nunique()} unique values\")\n",
    "#     print(test_df[col].value_counts().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9610f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SCALING RESULTS (STANDARD) ===\n",
      "Columns scaled: 3\n",
      "\n",
      "Scaling details:\n",
      "  numeric_1: mean 50.50→0.0000, std 29.01→1.0050\n",
      "  numeric_2: mean 555.00→0.0000, std 290.11→1.0050\n",
      "  numeric_3: mean 3383.50→-0.0000, std 3024.36→1.0050\n",
      "Features have been standardized using z-score scaling to prepare for machine learning. This ensures that each feature contributes equally to the model by having a mean of 0 and a standard deviation of 1.\n"
     ]
    }
   ],
   "source": [
    "# query = \"Scale features for machine learning\"\n",
    "# result = standardization(test_df, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f9c065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTING STANDARDIZATION FEATURE ===\n",
      "\\n1. Testing standard scaling:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 7 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   numeric_1           100 non-null    float64\n",
      " 1   numeric_2           100 non-null    float64\n",
      " 2   numeric_3           100 non-null    float64\n",
      " 3   category_small      100 non-null    object \n",
      " 4   category_medium     100 non-null    object \n",
      " 5   category_with_rare  100 non-null    object \n",
      " 6   binary_like         100 non-null    object \n",
      "dtypes: float64(3), object(4)\n",
      "memory usage: 5.6+ KB\n",
      "\\n2. Testing one-hot encoding:\n",
      "=== CATEGORICAL ENCODING RESULTS (ONEHOT) ===\n",
      "Columns processed: 4\n",
      "Columns encoded successfully: 4\n",
      "\n",
      "Encoding details:\n",
      "  category_small: 3 categories → 3 dummy columns\n",
      "  category_medium: 10 categories → 10 dummy columns\n",
      "  category_with_rare: 13 categories → 13 dummy columns\n",
      "  binary_like: 2 categories → 2 dummy columns\n",
      "One-hot encoding has been applied to all categorical variables in the DataFrame to prepare it for machine learning. This process converts categorical variables into a format that can be provided to ML algorithms to improve predictions.\n",
      "\\nShape after one-hot encoding: (100, 31)\n",
      "\\n3. Testing rare category handling:\n",
      "No rare categories found in column 'category_with_rare' (threshold: 1.0%)\n",
      "Handled rare categories in 'category_with_rare' column by grouping categories below the threshold of 0.01 into a single category.\n",
      "\\n4. Testing normalization:\n",
      "=== NORMALIZATION RESULTS (MINMAX) ===\n",
      "Columns normalized: 3\n",
      "\n",
      "Normalization details:\n",
      "  numeric_1: [1.00, 100.00] → [0.0000, 1.0000]\n",
      "  numeric_2: [60.00, 1050.00] → [0.0000, 1.0000]\n",
      "  numeric_3: [1.00, 10000.00] → [0.0000, 1.0000]\n",
      "Normalized numeric columns using min-max scaling to bring all values into the range of 0 to 1.\n"
     ]
    }
   ],
   "source": [
    "# print(\"=== TESTING STANDARDIZATION FEATURE ===\")\n",
    "# print(\"\\\\n1. Testing standard scaling:\")\n",
    "# result.info()\n",
    "# print(\"\\\\n2. Testing one-hot encoding:\")\n",
    "# result2 = standardization(test_df.copy(), \"One-hot encode categorical variables\")\n",
    "# print(f\"\\\\nShape after one-hot encoding: {result2.shape}\")\n",
    "# print(\"\\\\n3. Testing rare category handling:\")\n",
    "# result3 = standardization(test_df.copy(), \"Handle rare categories in category_with_rare column\")\n",
    "# print(\"\\\\n4. Testing normalization:\")\n",
    "# result4 = standardization(test_df.copy(), \"Normalize numeric columns with min-max scaling\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
