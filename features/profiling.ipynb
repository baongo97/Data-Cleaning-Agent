{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "691226d2",
   "metadata": {},
   "source": [
    "## **Feature:** Advanced Data Profiling\n",
    "\n",
    "**Names:** Gia Bao Ngo\n",
    "\n",
    "### **What it does**\n",
    "Comprehensive data profiling and quality analysis. Generates detailed reports about data characteristics, calculates quality metrics, detects data drift between datasets, analyzes correlations, and identifies patterns in the data. Provides interactive profiling reports and data quality scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff1f7555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dotenv\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Get API Key\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"OpenAI API Key not found\")\n",
    "\n",
    "# Import libraries\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Additional imports for data profiling\n",
    "import math\n",
    "import re\n",
    "import datetime\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Langchain imports\n",
    "from langchain_openai import ChatOpenAI  \n",
    "from langchain.schema import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a88657",
   "metadata": {},
   "source": [
    "### **Helper Functions**\n",
    "- `generate_profile_report(df, title=\"Data Profile\")` - Comprehensive data profiling with statistics\n",
    "- `calculate_quality_metrics(df)` - Data quality scoring and metrics\n",
    "- `analyze_correlations(df, method='pearson', threshold=0.8)` - Correlation analysis with visualization\n",
    "- `detect_patterns(df, columns=None)` - Pattern detection in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60424d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_profile_report(df, title=\"Data Profile\"):\n",
    "    \"\"\"\n",
    "    Generate comprehensive data profiling report with statistics.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - title: title for the report\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary containing profiling results\n",
    "    \"\"\"\n",
    "    profile = {\n",
    "        'title': title,\n",
    "        'dataset_info': {},\n",
    "        'column_profiles': {},\n",
    "        'summary_statistics': {},\n",
    "        'data_quality': {}\n",
    "    }\n",
    "    \n",
    "    # Dataset overview\n",
    "    profile['dataset_info'] = {\n",
    "        'shape': df.shape,\n",
    "        'total_cells': df.shape[0] * df.shape[1],\n",
    "        'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024**2,\n",
    "        'duplicate_rows': df.duplicated().sum(),\n",
    "        'duplicate_percentage': (df.duplicated().sum() / len(df)) * 100\n",
    "    }\n",
    "    \n",
    "    # Column-by-column profiling\n",
    "    for col in df.columns:\n",
    "        col_profile = {\n",
    "            'dtype': str(df[col].dtype),\n",
    "            'non_null_count': df[col].count(),\n",
    "            'null_count': df[col].isnull().sum(),\n",
    "            'null_percentage': (df[col].isnull().sum() / len(df)) * 100,\n",
    "            'unique_count': df[col].nunique(),\n",
    "            'unique_percentage': (df[col].nunique() / len(df)) * 100,\n",
    "        }\n",
    "        \n",
    "        # Type-specific profiling\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            # Numeric statistics\n",
    "            non_null_data = df[col].dropna()\n",
    "            if len(non_null_data) > 0:\n",
    "                col_profile.update({\n",
    "                    'min': float(non_null_data.min()),\n",
    "                    'max': float(non_null_data.max()),\n",
    "                    'mean': float(non_null_data.mean()),\n",
    "                    'median': float(non_null_data.median()),\n",
    "                    'std': float(non_null_data.std()),\n",
    "                    'q25': float(non_null_data.quantile(0.25)),\n",
    "                    'q75': float(non_null_data.quantile(0.75)),\n",
    "                    'skewness': float(stats.skew(non_null_data)),\n",
    "                    'kurtosis': float(stats.kurtosis(non_null_data)),\n",
    "                    'zeros': int((non_null_data == 0).sum()),\n",
    "                    'negative_values': int((non_null_data < 0).sum()),\n",
    "                    'infinite_values': int(np.isinf(non_null_data).sum())\n",
    "                })\n",
    "        \n",
    "        elif pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            # Datetime statistics\n",
    "            non_null_data = df[col].dropna()\n",
    "            if len(non_null_data) > 0:\n",
    "                col_profile.update({\n",
    "                    'min_date': str(non_null_data.min()),\n",
    "                    'max_date': str(non_null_data.max()),\n",
    "                    'date_range_days': (non_null_data.max() - non_null_data.min()).days,\n",
    "                })\n",
    "        \n",
    "        else:\n",
    "            # Text/categorical statistics\n",
    "            non_null_data = df[col].dropna()\n",
    "            if len(non_null_data) > 0:\n",
    "                # Most common values\n",
    "                value_counts = non_null_data.value_counts().head(5)\n",
    "                col_profile['top_5_values'] = dict(value_counts)\n",
    "                \n",
    "                # Text length statistics (for object types)\n",
    "                if df[col].dtype == 'object':\n",
    "                    text_lengths = non_null_data.astype(str).str.len()\n",
    "                    col_profile.update({\n",
    "                        'min_length': int(text_lengths.min()),\n",
    "                        'max_length': int(text_lengths.max()),\n",
    "                        'mean_length': float(text_lengths.mean()),\n",
    "                        'empty_strings': int((non_null_data.astype(str) == '').sum()),\n",
    "                        'whitespace_only': int(non_null_data.astype(str).str.strip().eq('').sum())\n",
    "                    })\n",
    "        \n",
    "        profile['column_profiles'][col] = col_profile\n",
    "    \n",
    "    # Summary statistics\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    profile['summary_statistics'] = {\n",
    "        'total_columns': len(df.columns),\n",
    "        'numeric_columns': len(numeric_cols),\n",
    "        'categorical_columns': len(df.select_dtypes(include=['object', 'category']).columns),\n",
    "        'datetime_columns': len(df.select_dtypes(include=['datetime64']).columns),\n",
    "        'boolean_columns': len(df.select_dtypes(include=['bool']).columns),\n",
    "        'high_cardinality_cols': len([col for col in df.columns if df[col].nunique() / len(df) > 0.95]),\n",
    "        'low_cardinality_cols': len([col for col in df.columns if df[col].nunique() / len(df) < 0.05]),\n",
    "        'constant_columns': len([col for col in df.columns if df[col].nunique() <= 1]),\n",
    "    }\n",
    "    \n",
    "    # Data quality assessment\n",
    "    total_cells = df.shape[0] * df.shape[1]\n",
    "    missing_cells = df.isnull().sum().sum()\n",
    "    \n",
    "    profile['data_quality'] = {\n",
    "        'completeness_score': ((total_cells - missing_cells) / total_cells) * 100,\n",
    "        'uniqueness_score': (df.nunique().sum() / total_cells) * 100,\n",
    "        'consistency_score': 100 - (df.duplicated().sum() / len(df)) * 100,\n",
    "        'overall_quality_score': 0  # Will be calculated\n",
    "    }\n",
    "    \n",
    "    # Calculate overall quality score\n",
    "    quality_score = (\n",
    "        profile['data_quality']['completeness_score'] * 0.4 +\n",
    "        profile['data_quality']['consistency_score'] * 0.3 +\n",
    "        min(profile['data_quality']['uniqueness_score'], 100) * 0.3\n",
    "    )\n",
    "    profile['data_quality']['overall_quality_score'] = quality_score\n",
    "    \n",
    "    # Print comprehensive report\n",
    "    print(f\"=== {title.upper()} ===\")\n",
    "    print(f\"Dataset Shape: {profile['dataset_info']['shape']}\")\n",
    "    print(f\"Memory Usage: {profile['dataset_info']['memory_usage_mb']:.2f} MB\")\n",
    "    print(f\"Duplicate Rows: {profile['dataset_info']['duplicate_rows']} ({profile['dataset_info']['duplicate_percentage']:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\\\nCOLUMN BREAKDOWN:\")\n",
    "    print(f\"  Numeric: {profile['summary_statistics']['numeric_columns']}\")\n",
    "    print(f\"  Categorical: {profile['summary_statistics']['categorical_columns']}\")\n",
    "    print(f\"  DateTime: {profile['summary_statistics']['datetime_columns']}\")\n",
    "    print(f\"  Boolean: {profile['summary_statistics']['boolean_columns']}\")\n",
    "    \n",
    "    print(f\"\\\\nDATA QUALITY SCORES:\")\n",
    "    print(f\"  Completeness: {profile['data_quality']['completeness_score']:.1f}%\")\n",
    "    print(f\"  Consistency: {profile['data_quality']['consistency_score']:.1f}%\")\n",
    "    print(f\"  Uniqueness: {profile['data_quality']['uniqueness_score']:.1f}%\")\n",
    "    print(f\"  Overall Quality: {profile['data_quality']['overall_quality_score']:.1f}%\")\n",
    "    \n",
    "    # Show top issues\n",
    "    issues = []\n",
    "    if profile['summary_statistics']['constant_columns'] > 0:\n",
    "        issues.append(f\"{profile['summary_statistics']['constant_columns']} constant columns\")\n",
    "    if profile['dataset_info']['duplicate_percentage'] > 5:\n",
    "        issues.append(f\"High duplicate rate ({profile['dataset_info']['duplicate_percentage']:.1f}%)\")\n",
    "    \n",
    "    high_missing_cols = [col for col, prof in profile['column_profiles'].items() \n",
    "                        if prof['null_percentage'] > 20]\n",
    "    if high_missing_cols:\n",
    "        issues.append(f\"{len(high_missing_cols)} columns with >20% missing values\")\n",
    "    \n",
    "    if issues:\n",
    "        print(f\"\\\\nDATA QUALITY ISSUES DETECTED:\")\n",
    "        for issue in issues:\n",
    "            print(f\"  - {issue}\")\n",
    "    \n",
    "    return profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46a090e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_quality_metrics(df):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive data quality scoring and metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with quality metrics and scores\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'completeness': {},\n",
    "        'uniqueness': {},\n",
    "        'consistency': {},\n",
    "        'validity': {},\n",
    "        'accuracy': {},\n",
    "        'overall': {}\n",
    "    }\n",
    "    \n",
    "    total_cells = df.shape[0] * df.shape[1]\n",
    "    \n",
    "    # 1. COMPLETENESS (missing data analysis)\n",
    "    missing_cells = df.isnull().sum().sum()\n",
    "    completeness_score = ((total_cells - missing_cells) / total_cells) * 100\n",
    "    \n",
    "    # Per-column completeness\n",
    "    col_completeness = {}\n",
    "    missing_by_col = df.isnull().sum()\n",
    "    for col in df.columns:\n",
    "        col_completeness[col] = ((len(df) - missing_by_col[col]) / len(df)) * 100\n",
    "    \n",
    "    metrics['completeness'] = {\n",
    "        'overall_score': completeness_score,\n",
    "        'missing_cells': missing_cells,\n",
    "        'missing_percentage': (missing_cells / total_cells) * 100,\n",
    "        'columns_with_missing': (missing_by_col > 0).sum(),\n",
    "        'worst_columns': missing_by_col.nlargest(5).to_dict(),\n",
    "        'column_scores': col_completeness\n",
    "    }\n",
    "    \n",
    "    # 2. UNIQUENESS (duplicate analysis)\n",
    "    duplicate_rows = df.duplicated().sum()\n",
    "    uniqueness_score = ((len(df) - duplicate_rows) / len(df)) * 100\n",
    "    \n",
    "    # Per-column uniqueness\n",
    "    col_uniqueness = {}\n",
    "    for col in df.columns:\n",
    "        unique_ratio = df[col].nunique() / len(df)\n",
    "        col_uniqueness[col] = unique_ratio * 100\n",
    "    \n",
    "    metrics['uniqueness'] = {\n",
    "        'overall_score': uniqueness_score,\n",
    "        'duplicate_rows': duplicate_rows,\n",
    "        'duplicate_percentage': (duplicate_rows / len(df)) * 100,\n",
    "        'column_scores': col_uniqueness,\n",
    "        'low_uniqueness_columns': [col for col, score in col_uniqueness.items() if score < 5]\n",
    "    }\n",
    "    \n",
    "    # 3. CONSISTENCY (data format and type consistency)\n",
    "    consistency_issues = []\n",
    "    consistency_score = 100  # Start with perfect score and deduct\n",
    "    \n",
    "    # Check for mixed data types in object columns\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        non_null_data = df[col].dropna()\n",
    "        if len(non_null_data) > 0:\n",
    "            # Check for numeric data in string columns\n",
    "            numeric_count = 0\n",
    "            for val in non_null_data.head(100):  # Sample first 100\n",
    "                try:\n",
    "                    float(str(val))\n",
    "                    numeric_count += 1\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            if numeric_count > len(non_null_data.head(100)) * 0.8:  # 80% numeric\n",
    "                consistency_issues.append(f\"Column '{col}' appears to contain numeric data but is stored as object\")\n",
    "                consistency_score -= 5\n",
    "    \n",
    "    # Check for inconsistent date formats\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        sample_data = df[col].dropna().head(50).astype(str)\n",
    "        date_patterns = [\n",
    "            r'\\\\d{4}-\\\\d{2}-\\\\d{2}',  # YYYY-MM-DD\n",
    "            r'\\\\d{2}/\\\\d{2}/\\\\d{4}',  # MM/DD/YYYY\n",
    "            r'\\\\d{2}-\\\\d{2}-\\\\d{4}',  # MM-DD-YYYY\n",
    "        ]\n",
    "        \n",
    "        pattern_matches = {}\n",
    "        for pattern in date_patterns:\n",
    "            matches = sample_data.str.contains(pattern, na=False).sum()\n",
    "            if matches > 0:\n",
    "                pattern_matches[pattern] = matches\n",
    "        \n",
    "        if len(pattern_matches) > 1 and len(sample_data) > 10:\n",
    "            consistency_issues.append(f\"Column '{col}' has mixed date formats\")\n",
    "            consistency_score -= 3\n",
    "    \n",
    "    metrics['consistency'] = {\n",
    "        'overall_score': max(consistency_score, 0),\n",
    "        'issues_found': len(consistency_issues),\n",
    "        'issues_detail': consistency_issues\n",
    "    }\n",
    "    \n",
    "    # 4. VALIDITY (format and constraint validation)\n",
    "    validity_issues = []\n",
    "    validity_score = 100\n",
    "    \n",
    "    # Check for outliers in numeric columns using IQR\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        non_null_data = df[col].dropna()\n",
    "        if len(non_null_data) > 4:  # Need at least 4 values for quartiles\n",
    "            Q1 = non_null_data.quantile(0.25)\n",
    "            Q3 = non_null_data.quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            \n",
    "            if IQR > 0:\n",
    "                outliers = non_null_data[(non_null_data < (Q1 - 1.5 * IQR)) | \n",
    "                                       (non_null_data > (Q3 + 1.5 * IQR))]\n",
    "                if len(outliers) > len(non_null_data) * 0.1:  # More than 10% outliers\n",
    "                    validity_issues.append(f\"Column '{col}' has {len(outliers)} potential outliers\")\n",
    "                    validity_score -= 2\n",
    "    \n",
    "    # Check for negative values in columns that might expect positive values\n",
    "    potential_positive_cols = [col for col in df.columns if any(word in col.lower() \n",
    "                              for word in ['age', 'price', 'count', 'amount', 'quantity', 'score'])]\n",
    "    \n",
    "    for col in potential_positive_cols:\n",
    "        if col in df.columns and pd.api.types.is_numeric_dtype(df[col]):\n",
    "            negative_count = (df[col] < 0).sum()\n",
    "            if negative_count > 0:\n",
    "                validity_issues.append(f\"Column '{col}' has {negative_count} negative values (might be invalid)\")\n",
    "                validity_score -= 1\n",
    "    \n",
    "    metrics['validity'] = {\n",
    "        'overall_score': max(validity_score, 0),\n",
    "        'issues_found': len(validity_issues),\n",
    "        'issues_detail': validity_issues\n",
    "    }\n",
    "    \n",
    "    # 5. ACCURACY (placeholder - would need external reference data)\n",
    "    # For now, we'll use some heuristics\n",
    "    accuracy_issues = []\n",
    "    accuracy_score = 95  # Default high score since we can't validate against truth\n",
    "    \n",
    "    # Check for impossible dates (future dates in birth year columns, etc.)\n",
    "    current_year = datetime.datetime.now().year\n",
    "    for col in df.columns:\n",
    "        if any(word in col.lower() for word in ['birth', 'born']) and pd.api.types.is_numeric_dtype(df[col]):\n",
    "            future_years = (df[col] > current_year).sum()\n",
    "            if future_years > 0:\n",
    "                accuracy_issues.append(f\"Column '{col}' has {future_years} future years (impossible for birth dates)\")\n",
    "                accuracy_score -= 3\n",
    "    \n",
    "    metrics['accuracy'] = {\n",
    "        'overall_score': max(accuracy_score, 0),\n",
    "        'issues_found': len(accuracy_issues),\n",
    "        'issues_detail': accuracy_issues\n",
    "    }\n",
    "    \n",
    "    # 6. OVERALL QUALITY SCORE\n",
    "    # Weighted average of all dimensions\n",
    "    weights = {\n",
    "        'completeness': 0.25,\n",
    "        'uniqueness': 0.20,\n",
    "        'consistency': 0.25,\n",
    "        'validity': 0.20,\n",
    "        'accuracy': 0.10\n",
    "    }\n",
    "    \n",
    "    overall_score = (\n",
    "        metrics['completeness']['overall_score'] * weights['completeness'] +\n",
    "        metrics['uniqueness']['overall_score'] * weights['uniqueness'] +\n",
    "        metrics['consistency']['overall_score'] * weights['consistency'] +\n",
    "        metrics['validity']['overall_score'] * weights['validity'] +\n",
    "        metrics['accuracy']['overall_score'] * weights['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Determine quality level\n",
    "    if overall_score >= 90:\n",
    "        quality_level = \"Excellent\"\n",
    "    elif overall_score >= 80:\n",
    "        quality_level = \"Good\"\n",
    "    elif overall_score >= 70:\n",
    "        quality_level = \"Fair\"\n",
    "    elif overall_score >= 60:\n",
    "        quality_level = \"Poor\"\n",
    "    else:\n",
    "        quality_level = \"Very Poor\"\n",
    "    \n",
    "    metrics['overall'] = {\n",
    "        'score': overall_score,\n",
    "        'level': quality_level,\n",
    "        'weights_used': weights\n",
    "    }\n",
    "    \n",
    "    # Print comprehensive quality report\n",
    "    print(\"=== DATA QUALITY ASSESSMENT ===\")\n",
    "    print(f\"Overall Quality Score: {overall_score:.1f}% ({quality_level})\")\n",
    "    print(f\"Dataset: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "    \n",
    "    print(f\"\\\\nQUALITY DIMENSIONS:\")\n",
    "    print(f\"  Completeness: {metrics['completeness']['overall_score']:.1f}% ({metrics['completeness']['missing_cells']} missing cells)\")\n",
    "    print(f\"  Uniqueness: {metrics['uniqueness']['overall_score']:.1f}% ({metrics['uniqueness']['duplicate_rows']} duplicate rows)\")\n",
    "    print(f\"  Consistency: {metrics['consistency']['overall_score']:.1f}% ({metrics['consistency']['issues_found']} issues)\")\n",
    "    print(f\"  Validity: {metrics['validity']['overall_score']:.1f}% ({metrics['validity']['issues_found']} issues)\")\n",
    "    print(f\"  Accuracy: {metrics['accuracy']['overall_score']:.1f}% ({metrics['accuracy']['issues_found']} issues)\")\n",
    "    \n",
    "    # Show worst completeness columns\n",
    "    if metrics['completeness']['worst_columns']:\n",
    "        print(f\"\\\\nCOLUMNS WITH MOST MISSING DATA:\")\n",
    "        for col, missing_count in list(metrics['completeness']['worst_columns'].items())[:3]:\n",
    "            missing_pct = (missing_count / len(df)) * 100\n",
    "            print(f\"  {col}: {missing_count} missing ({missing_pct:.1f}%)\")\n",
    "    \n",
    "    # Show all issues found\n",
    "    all_issues = (consistency_issues + validity_issues + accuracy_issues)\n",
    "    if all_issues:\n",
    "        print(f\"\\\\nQUALITY ISSUES DETECTED ({len(all_issues)} total):\")\n",
    "        for issue in all_issues[:10]:  # Show first 10 issues\n",
    "            print(f\"  - {issue}\")\n",
    "        if len(all_issues) > 10:\n",
    "            print(f\"  ... and {len(all_issues) - 10} more issues\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "jreck5zmsn",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_correlations(df, method='pearson', threshold=0.8):\n",
    "    \"\"\"\n",
    "    Analyze correlations with visualization and detailed reporting.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - method: correlation method ('pearson', 'spearman', 'kendall')\n",
    "    - threshold: correlation threshold for flagging high correlations\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with correlation analysis results\n",
    "    \"\"\"\n",
    "    correlation_report = {\n",
    "        'method': method,\n",
    "        'threshold': threshold,\n",
    "        'correlation_matrix': None,\n",
    "        'high_correlations': [],\n",
    "        'correlation_summary': {},\n",
    "        'multicollinearity': {}\n",
    "    }\n",
    "    \n",
    "    # Get numeric columns for correlation analysis\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    if len(numeric_cols) < 2:\n",
    "        print(\"=== CORRELATION ANALYSIS ===\")\n",
    "        print(\"Error: Need at least 2 numeric columns for correlation analysis.\")\n",
    "        print(f\"Found {len(numeric_cols)} numeric columns: {numeric_cols}\")\n",
    "        return correlation_report\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    try:\n",
    "        correlation_matrix = df[numeric_cols].corr(method=method)\n",
    "        correlation_report['correlation_matrix'] = correlation_matrix\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating correlations: {e}\")\n",
    "        return correlation_report\n",
    "    \n",
    "    # Find high correlations (excluding diagonal)\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            col1 = correlation_matrix.columns[i]\n",
    "            col2 = correlation_matrix.columns[j]\n",
    "            corr_value = correlation_matrix.iloc[i, j]\n",
    "            \n",
    "            if abs(corr_value) >= threshold:\n",
    "                high_corr_pairs.append({\n",
    "                    'column1': col1,\n",
    "                    'column2': col2,\n",
    "                    'correlation': corr_value,\n",
    "                    'absolute_correlation': abs(corr_value),\n",
    "                    'relationship': 'positive' if corr_value > 0 else 'negative'\n",
    "                })\n",
    "    \n",
    "    # Sort by absolute correlation descending\n",
    "    high_corr_pairs.sort(key=lambda x: x['absolute_correlation'], reverse=True)\n",
    "    correlation_report['high_correlations'] = high_corr_pairs\n",
    "    \n",
    "    # Correlation summary statistics\n",
    "    # Flatten correlation matrix (excluding diagonal and upper triangle)\n",
    "    corr_values = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            corr_values.append(correlation_matrix.iloc[i, j])\n",
    "    \n",
    "    corr_array = np.array(corr_values)\n",
    "    corr_array = corr_array[~np.isnan(corr_array)]  # Remove NaN values\n",
    "    \n",
    "    if len(corr_array) > 0:\n",
    "        correlation_report['correlation_summary'] = {\n",
    "            'total_pairs': len(corr_array),\n",
    "            'mean_correlation': float(np.mean(np.abs(corr_array))),\n",
    "            'max_correlation': float(np.max(np.abs(corr_array))),\n",
    "            'min_correlation': float(np.min(np.abs(corr_array))),\n",
    "            'std_correlation': float(np.std(corr_array)),\n",
    "            'high_correlation_count': len(high_corr_pairs),\n",
    "            'high_correlation_percentage': (len(high_corr_pairs) / len(corr_array)) * 100\n",
    "        }\n",
    "    \n",
    "    # Multicollinearity detection (using correlation-based approach)\n",
    "    multicollinear_groups = []\n",
    "    processed_cols = set()\n",
    "    \n",
    "    for pair in high_corr_pairs:\n",
    "        if pair['column1'] not in processed_cols and pair['column2'] not in processed_cols:\n",
    "            # Find all columns highly correlated with this pair\n",
    "            group = set([pair['column1'], pair['column2']])\n",
    "            \n",
    "            # Look for more columns correlated with either column in the pair\n",
    "            for other_pair in high_corr_pairs:\n",
    "                if (other_pair['column1'] in group and other_pair['column2'] not in processed_cols):\n",
    "                    group.add(other_pair['column2'])\n",
    "                elif (other_pair['column2'] in group and other_pair['column1'] not in processed_cols):\n",
    "                    group.add(other_pair['column1'])\n",
    "            \n",
    "            if len(group) >= 2:\n",
    "                multicollinear_groups.append({\n",
    "                    'columns': list(group),\n",
    "                    'size': len(group),\n",
    "                    'max_correlation': max([abs(p['correlation']) for p in high_corr_pairs \n",
    "                                          if p['column1'] in group and p['column2'] in group])\n",
    "                })\n",
    "                processed_cols.update(group)\n",
    "    \n",
    "    correlation_report['multicollinearity'] = {\n",
    "        'groups_found': len(multicollinear_groups),\n",
    "        'groups_detail': multicollinear_groups,\n",
    "        'affected_columns': len(processed_cols),\n",
    "        'recommendation': get_multicollinearity_recommendation(multicollinear_groups)\n",
    "    }\n",
    "    \n",
    "    # Print comprehensive correlation report\n",
    "    print(\"=== CORRELATION ANALYSIS ===\")\n",
    "    print(f\"Method: {method.title()}\")\n",
    "    print(f\"Numeric columns analyzed: {len(numeric_cols)}\")\n",
    "    print(f\"Total correlation pairs: {correlation_report['correlation_summary'].get('total_pairs', 0)}\")\n",
    "    \n",
    "    if correlation_report['correlation_summary']:\n",
    "        print(f\"\\\\nCORRELATION STATISTICS:\")\n",
    "        print(f\"  Mean absolute correlation: {correlation_report['correlation_summary']['mean_correlation']:.3f}\")\n",
    "        print(f\"  Maximum correlation: {correlation_report['correlation_summary']['max_correlation']:.3f}\")\n",
    "        print(f\"  Standard deviation: {correlation_report['correlation_summary']['std_correlation']:.3f}\")\n",
    "        print(f\"  High correlations (≥{threshold}): {correlation_report['correlation_summary']['high_correlation_count']} ({correlation_report['correlation_summary']['high_correlation_percentage']:.1f}%)\")\n",
    "    \n",
    "    # Show highest correlations\n",
    "    if high_corr_pairs:\n",
    "        print(f\"\\\\nHIGHEST CORRELATIONS (≥{threshold}):\")\n",
    "        for pair in high_corr_pairs[:10]:  # Show top 10\n",
    "            print(f\"  {pair['column1']} ↔ {pair['column2']}: {pair['correlation']:.3f} ({pair['relationship']})\")\n",
    "        \n",
    "        if len(high_corr_pairs) > 10:\n",
    "            print(f\"  ... and {len(high_corr_pairs) - 10} more high correlations\")\n",
    "    else:\n",
    "        print(f\"\\\\nNo correlations above threshold ({threshold}) found.\")\n",
    "    \n",
    "    # Multicollinearity report\n",
    "    if multicollinear_groups:\n",
    "        print(f\"\\\\nMULTICOLLINEARITY DETECTED:\")\n",
    "        print(f\"  Groups found: {len(multicollinear_groups)}\")\n",
    "        print(f\"  Columns affected: {correlation_report['multicollinearity']['affected_columns']}\")\n",
    "        \n",
    "        for i, group in enumerate(multicollinear_groups[:3], 1):  # Show first 3 groups\n",
    "            print(f\"  Group {i}: {group['columns']} (max corr: {group['max_correlation']:.3f})\")\n",
    "        \n",
    "        if len(multicollinear_groups) > 3:\n",
    "            print(f\"  ... and {len(multicollinear_groups) - 3} more groups\")\n",
    "            \n",
    "        print(f\"\\\\nRECOMMENDATION: {correlation_report['multicollinearity']['recommendation']}\")\n",
    "    else:\n",
    "        print(\"\\\\nNo significant multicollinearity detected.\")\n",
    "    \n",
    "    # Create visualization if matplotlib is available\n",
    "    try:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Create correlation heatmap\n",
    "        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "        sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "                   square=True, linewidths=0.5, fmt='.2f')\n",
    "        plt.title(f'{method.title()} Correlation Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Create distribution of correlations\n",
    "        if len(corr_array) > 1:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.hist(corr_array, bins=20, alpha=0.7, edgecolor='black')\n",
    "            plt.axvline(threshold, color='red', linestyle='--', label=f'Threshold: {threshold}')\n",
    "            plt.axvline(-threshold, color='red', linestyle='--')\n",
    "            plt.xlabel('Correlation Coefficient')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.title('Distribution of Correlation Coefficients')\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"\\\\nNote: Install matplotlib and seaborn for correlation visualizations.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\\\nVisualization error: {e}\")\n",
    "    \n",
    "    return correlation_report\n",
    "\n",
    "def get_multicollinearity_recommendation(groups):\n",
    "    \"\"\"Get recommendation for handling multicollinearity.\"\"\"\n",
    "    if len(groups) == 0:\n",
    "        return \"No multicollinearity issues detected.\"\n",
    "    elif len(groups) == 1:\n",
    "        return \"Consider removing one variable from the correlated group or use dimensionality reduction.\"\n",
    "    else:\n",
    "        return \"Multiple multicollinear groups found. Consider feature selection, PCA, or regularization techniques.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "erz9bcphwra",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_patterns(df, columns=None):\n",
    "    \"\"\"\n",
    "    Detect patterns in data columns.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - columns: list of column names to analyze (None = all columns)\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with pattern analysis results\n",
    "    \"\"\"\n",
    "    pattern_report = {\n",
    "        'text_patterns': {},\n",
    "        'numeric_patterns': {},\n",
    "        'temporal_patterns': {},\n",
    "        'categorical_patterns': {},\n",
    "        'missing_patterns': {},\n",
    "        'summary': {}\n",
    "    }\n",
    "    \n",
    "    if columns is None:\n",
    "        columns = df.columns.tolist()\n",
    "    \n",
    "    print(\"=== PATTERN DETECTION ANALYSIS ===\")\n",
    "    print(f\"Analyzing {len(columns)} columns for patterns...\")\n",
    "    \n",
    "    # 1. TEXT PATTERNS (for object columns)\n",
    "    text_patterns = {}\n",
    "    text_cols = [col for col in columns if col in df.select_dtypes(include=['object']).columns]\n",
    "    \n",
    "    for col in text_cols:\n",
    "        non_null_data = df[col].dropna().astype(str)\n",
    "        if len(non_null_data) == 0:\n",
    "            continue\n",
    "            \n",
    "        col_patterns = {\n",
    "            'column': col,\n",
    "            'patterns_found': [],\n",
    "            'format_consistency': 0,\n",
    "            'common_formats': {}\n",
    "        }\n",
    "        \n",
    "        # Email pattern\n",
    "        email_pattern = r'\\\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Z|a-z]{2,}\\\\b'\n",
    "        email_matches = non_null_data.str.contains(email_pattern, regex=True, na=False).sum()\n",
    "        if email_matches > len(non_null_data) * 0.5:\n",
    "            col_patterns['patterns_found'].append(f'Email format ({email_matches}/{len(non_null_data)} matches)')\n",
    "        \n",
    "        # Phone pattern\n",
    "        phone_patterns = [\n",
    "            r'\\\\(\\\\d{3}\\\\)\\\\s*\\\\d{3}-\\\\d{4}',  # (123) 456-7890\n",
    "            r'\\\\d{3}-\\\\d{3}-\\\\d{4}',          # 123-456-7890\n",
    "            r'\\\\d{10}',                      # 1234567890\n",
    "            r'\\\\+1\\\\d{10}'                   # +11234567890\n",
    "        ]\n",
    "        phone_matches = 0\n",
    "        for pattern in phone_patterns:\n",
    "            phone_matches += non_null_data.str.contains(pattern, regex=True, na=False).sum()\n",
    "        if phone_matches > len(non_null_data) * 0.5:\n",
    "            col_patterns['patterns_found'].append(f'Phone format ({phone_matches}/{len(non_null_data)} matches)')\n",
    "        \n",
    "        # URL pattern\n",
    "        url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "        url_matches = non_null_data.str.contains(url_pattern, regex=True, na=False).sum()\n",
    "        if url_matches > len(non_null_data) * 0.3:\n",
    "            col_patterns['patterns_found'].append(f'URL format ({url_matches}/{len(non_null_data)} matches)')\n",
    "        \n",
    "        # Date-like pattern\n",
    "        date_patterns = [\n",
    "            r'\\\\d{4}-\\\\d{2}-\\\\d{2}',      # YYYY-MM-DD\n",
    "            r'\\\\d{2}/\\\\d{2}/\\\\d{4}',      # MM/DD/YYYY\n",
    "            r'\\\\d{2}-\\\\d{2}-\\\\d{4}'       # MM-DD-YYYY\n",
    "        ]\n",
    "        date_matches = 0\n",
    "        for pattern in date_patterns:\n",
    "            date_matches += non_null_data.str.contains(pattern, regex=True, na=False).sum()\n",
    "        if date_matches > len(non_null_data) * 0.5:\n",
    "            col_patterns['patterns_found'].append(f'Date-like format ({date_matches}/{len(non_null_data)} matches)')\n",
    "        \n",
    "        # ID-like pattern (alphanumeric with consistent length)\n",
    "        if len(non_null_data) > 10:\n",
    "            lengths = non_null_data.str.len()\n",
    "            length_consistency = (lengths == lengths.mode()[0]).sum() / len(lengths)\n",
    "            if length_consistency > 0.8:\n",
    "                mode_length = lengths.mode()[0]\n",
    "                # Check if alphanumeric\n",
    "                alphanum_pattern = r'^[A-Za-z0-9]+$'\n",
    "                alphanum_matches = non_null_data.str.contains(alphanum_pattern, regex=True, na=False).sum()\n",
    "                if alphanum_matches > len(non_null_data) * 0.8:\n",
    "                    col_patterns['patterns_found'].append(f'ID-like format (length={mode_length}, {alphanum_matches}/{len(non_null_data)} alphanumeric)')\n",
    "        \n",
    "        # Format consistency check\n",
    "        sample_data = non_null_data.head(100)\n",
    "        format_counter = Counter()\n",
    "        for value in sample_data:\n",
    "            # Create format signature\n",
    "            format_sig = re.sub(r'\\\\d', 'N', str(value))\n",
    "            format_sig = re.sub(r'[A-Za-z]', 'A', format_sig)\n",
    "            format_counter[format_sig] += 1\n",
    "        \n",
    "        if len(format_counter) > 0:\n",
    "            most_common_format = format_counter.most_common(1)[0]\n",
    "            col_patterns['format_consistency'] = most_common_format[1] / len(sample_data)\n",
    "            col_patterns['common_formats'] = dict(format_counter.most_common(5))\n",
    "        \n",
    "        text_patterns[col] = col_patterns\n",
    "    \n",
    "    pattern_report['text_patterns'] = text_patterns\n",
    "    \n",
    "    # 2. NUMERIC PATTERNS\n",
    "    numeric_patterns = {}\n",
    "    numeric_cols = [col for col in columns if col in df.select_dtypes(include=[np.number]).columns]\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        non_null_data = df[col].dropna()\n",
    "        if len(non_null_data) == 0:\n",
    "            continue\n",
    "            \n",
    "        col_patterns = {\n",
    "            'column': col,\n",
    "            'patterns_found': [],\n",
    "            'distribution_type': 'unknown',\n",
    "            'outlier_pattern': 'normal'\n",
    "        }\n",
    "        \n",
    "        # Check for integer-like pattern in floats\n",
    "        if df[col].dtype == 'float64':\n",
    "            is_integer_like = (non_null_data == non_null_data.astype(int)).all()\n",
    "            if is_integer_like:\n",
    "                col_patterns['patterns_found'].append('Integer values in float column')\n",
    "        \n",
    "        # Check for specific numeric patterns\n",
    "        # Percentage-like (0-100 range)\n",
    "        if non_null_data.min() >= 0 and non_null_data.max() <= 100:\n",
    "            col_patterns['patterns_found'].append('Percentage-like values (0-100 range)')\n",
    "        \n",
    "        # Probability-like (0-1 range)\n",
    "        elif non_null_data.min() >= 0 and non_null_data.max() <= 1:\n",
    "            col_patterns['patterns_found'].append('Probability-like values (0-1 range)')\n",
    "        \n",
    "        # Age-like pattern\n",
    "        if non_null_data.min() >= 0 and non_null_data.max() <= 150 and non_null_data.dtype in ['int64', 'int32']:\n",
    "            col_patterns['patterns_found'].append('Age-like values (0-150 integer range)')\n",
    "        \n",
    "        # Year-like pattern\n",
    "        if non_null_data.min() >= 1900 and non_null_data.max() <= 2030 and len(non_null_data.unique()) > 5:\n",
    "            col_patterns['patterns_found'].append('Year-like values (1900-2030 range)')\n",
    "        \n",
    "        # Distribution analysis\n",
    "        try:\n",
    "            from scipy import stats\n",
    "            # Test for normality\n",
    "            if len(non_null_data) > 7:\n",
    "                _, normal_p = stats.shapiro(non_null_data.sample(min(5000, len(non_null_data))))\n",
    "                if normal_p > 0.05:\n",
    "                    col_patterns['distribution_type'] = 'approximately_normal'\n",
    "                else:\n",
    "                    # Check skewness\n",
    "                    skewness = stats.skew(non_null_data)\n",
    "                    if abs(skewness) > 2:\n",
    "                        col_patterns['distribution_type'] = 'highly_skewed'\n",
    "                    elif abs(skewness) > 1:\n",
    "                        col_patterns['distribution_type'] = 'moderately_skewed'\n",
    "                    else:\n",
    "                        col_patterns['distribution_type'] = 'roughly_symmetric'\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Outlier pattern detection using IQR\n",
    "        Q1 = non_null_data.quantile(0.25)\n",
    "        Q3 = non_null_data.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        if IQR > 0:\n",
    "            outliers = non_null_data[(non_null_data < (Q1 - 1.5 * IQR)) | (non_null_data > (Q3 + 1.5 * IQR))]\n",
    "            outlier_percentage = len(outliers) / len(non_null_data) * 100\n",
    "            \n",
    "            if outlier_percentage > 10:\n",
    "                col_patterns['outlier_pattern'] = 'high_outliers'\n",
    "            elif outlier_percentage > 5:\n",
    "                col_patterns['outlier_pattern'] = 'moderate_outliers'\n",
    "            else:\n",
    "                col_patterns['outlier_pattern'] = 'few_outliers'\n",
    "        \n",
    "        numeric_patterns[col] = col_patterns\n",
    "    \n",
    "    pattern_report['numeric_patterns'] = numeric_patterns\n",
    "    \n",
    "    # 3. CATEGORICAL PATTERNS\n",
    "    categorical_patterns = {}\n",
    "    categorical_cols = text_cols + [col for col in columns if df[col].dtype == 'category']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in text_patterns:  # Skip if already analyzed as text\n",
    "            continue\n",
    "            \n",
    "        non_null_data = df[col].dropna()\n",
    "        if len(non_null_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        value_counts = non_null_data.value_counts()\n",
    "        col_patterns = {\n",
    "            'column': col,\n",
    "            'cardinality': len(value_counts),\n",
    "            'cardinality_ratio': len(value_counts) / len(non_null_data),\n",
    "            'distribution_pattern': 'unknown',\n",
    "            'top_values': dict(value_counts.head(5))\n",
    "        }\n",
    "        \n",
    "        # Determine distribution pattern\n",
    "        if len(value_counts) == 2:\n",
    "            col_patterns['distribution_pattern'] = 'binary'\n",
    "        elif len(value_counts) <= 10:\n",
    "            col_patterns['distribution_pattern'] = 'low_cardinality'\n",
    "        elif col_patterns['cardinality_ratio'] < 0.05:\n",
    "            col_patterns['distribution_pattern'] = 'low_cardinality_high_frequency'\n",
    "        elif col_patterns['cardinality_ratio'] > 0.9:\n",
    "            col_patterns['distribution_pattern'] = 'high_cardinality_unique'\n",
    "        else:\n",
    "            col_patterns['distribution_pattern'] = 'medium_cardinality'\n",
    "        \n",
    "        # Check for imbalanced distribution\n",
    "        max_freq = value_counts.max()\n",
    "        if max_freq / len(non_null_data) > 0.9:\n",
    "            col_patterns['distribution_pattern'] += '_highly_imbalanced'\n",
    "        elif max_freq / len(non_null_data) > 0.7:\n",
    "            col_patterns['distribution_pattern'] += '_imbalanced'\n",
    "        \n",
    "        categorical_patterns[col] = col_patterns\n",
    "    \n",
    "    pattern_report['categorical_patterns'] = categorical_patterns\n",
    "    \n",
    "    # 4. MISSING DATA PATTERNS\n",
    "    missing_patterns = {}\n",
    "    missing_cols = [col for col in columns if df[col].isnull().sum() > 0]\n",
    "    \n",
    "    if missing_cols:\n",
    "        # Overall missing pattern\n",
    "        missing_matrix = df[missing_cols].isnull()\n",
    "        \n",
    "        # Find common missing patterns\n",
    "        missing_pattern_counts = missing_matrix.value_counts().head(10)\n",
    "        \n",
    "        missing_patterns = {\n",
    "            'columns_with_missing': missing_cols,\n",
    "            'total_missing_cells': df[missing_cols].isnull().sum().sum(),\n",
    "            'common_missing_patterns': dict(missing_pattern_counts),\n",
    "            'missing_correlation': {}\n",
    "        }\n",
    "        \n",
    "        # Check correlation between missing values\n",
    "        if len(missing_cols) > 1:\n",
    "            missing_corr = missing_matrix.corr()\n",
    "            high_missing_corr = []\n",
    "            \n",
    "            for i in range(len(missing_corr.columns)):\n",
    "                for j in range(i+1, len(missing_corr.columns)):\n",
    "                    corr_val = missing_corr.iloc[i, j]\n",
    "                    if abs(corr_val) > 0.5:\n",
    "                        high_missing_corr.append({\n",
    "                            'col1': missing_corr.columns[i],\n",
    "                            'col2': missing_corr.columns[j],\n",
    "                            'correlation': corr_val\n",
    "                        })\n",
    "            \n",
    "            missing_patterns['missing_correlation'] = high_missing_corr\n",
    "    \n",
    "    pattern_report['missing_patterns'] = missing_patterns\n",
    "    \n",
    "    # 5. SUMMARY\n",
    "    total_patterns_found = 0\n",
    "    for category in ['text_patterns', 'numeric_patterns', 'categorical_patterns']:\n",
    "        for col_data in pattern_report[category].values():\n",
    "            if isinstance(col_data, dict) and 'patterns_found' in col_data:\n",
    "                total_patterns_found += len(col_data['patterns_found'])\n",
    "    \n",
    "    pattern_report['summary'] = {\n",
    "        'columns_analyzed': len(columns),\n",
    "        'text_columns_analyzed': len(text_patterns),\n",
    "        'numeric_columns_analyzed': len(numeric_patterns),\n",
    "        'categorical_columns_analyzed': len(categorical_patterns),\n",
    "        'columns_with_missing': len(missing_cols),\n",
    "        'total_patterns_found': total_patterns_found\n",
    "    }\n",
    "    \n",
    "    # Print comprehensive pattern report\n",
    "    print(f\"\\\\nPATTERN ANALYSIS SUMMARY:\")\n",
    "    print(f\"  Text columns: {len(text_patterns)}\")\n",
    "    print(f\"  Numeric columns: {len(numeric_patterns)}\")\n",
    "    print(f\"  Categorical columns: {len(categorical_patterns)}\")\n",
    "    print(f\"  Columns with missing data: {len(missing_cols)}\")\n",
    "    print(f\"  Total patterns detected: {total_patterns_found}\")\n",
    "    \n",
    "    # Show interesting text patterns\n",
    "    if text_patterns:\n",
    "        print(f\"\\\\nTEXT PATTERNS DETECTED:\")\n",
    "        for col, patterns in text_patterns.items():\n",
    "            if patterns['patterns_found']:\n",
    "                print(f\"  {col}: {', '.join(patterns['patterns_found'])}\")\n",
    "    \n",
    "    # Show interesting numeric patterns\n",
    "    if numeric_patterns:\n",
    "        print(f\"\\\\nNUMERIC PATTERNS DETECTED:\")\n",
    "        for col, patterns in numeric_patterns.items():\n",
    "            if patterns['patterns_found']:\n",
    "                print(f\"  {col}: {', '.join(patterns['patterns_found'])}\")\n",
    "    \n",
    "    # Show categorical distribution patterns\n",
    "    if categorical_patterns:\n",
    "        print(f\"\\\\nCATEGORICAL PATTERNS:\")\n",
    "        for col, patterns in categorical_patterns.items():\n",
    "            print(f\"  {col}: {patterns['distribution_pattern']} (cardinality: {patterns['cardinality']})\")\n",
    "    \n",
    "    # Show missing data patterns\n",
    "    if missing_patterns and missing_patterns.get('missing_correlation'):\n",
    "        print(f\"\\\\nMISSING DATA CORRELATION:\")\n",
    "        for corr in missing_patterns['missing_correlation']:\n",
    "            print(f\"  {corr['col1']} ↔ {corr['col2']}: {corr['correlation']:.3f}\")\n",
    "    \n",
    "    return pattern_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c99ee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_docs = \"\"\" Helper functions available:\n",
    "- generate_profile_report(df, title=\"Data Profile\"): Comprehensive data profiling with statistics, quality metrics, and detailed column analysis. Returns detailed profile dictionary.\n",
    "- calculate_quality_metrics(df): Data quality scoring across 5 dimensions (completeness, uniqueness, consistency, validity, accuracy). Returns metrics dictionary with scores.\n",
    "- analyze_correlations(df, method='pearson', threshold=0.8): Correlation analysis with visualization and multicollinearity detection. Returns correlation analysis results.\n",
    "- detect_patterns(df, columns=None): Pattern detection in data including text patterns (email, phone, URL), numeric patterns, and categorical distributions. Returns pattern analysis report.\n",
    "\n",
    "Examples:\n",
    "- \"Generate a data profile report\" -> profile = generate_profile_report(df)\n",
    "- \"Calculate data quality metrics\" -> metrics = calculate_quality_metrics(df)\n",
    "- \"Analyze correlations in the data\" -> corr_analysis = analyze_correlations(df)\n",
    "- \"Detect patterns in the data\" -> patterns = detect_patterns(df)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b02315b",
   "metadata": {},
   "source": [
    "# **MAIN FEATURE FUNCTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881f15c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profiling(df, user_query):\n",
    "    \"\"\"\n",
    "    Main function that gets called by the main router.\n",
    "    MUST take (df, user_query) and return df\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create message chain\n",
    "    messages = []\n",
    "    messages.append(SystemMessage(content=helper_docs))\n",
    "    messages.append(SystemMessage(content=f\"\"\"\n",
    "    You are a data cleaning agent focused on comprehensive data profiling and quality analysis.\n",
    "    \n",
    "    Dataset info: Shape: {df.shape}, Sample: {df.head(3).to_string()}\n",
    "\n",
    "    Libraries available:\n",
    "    - pd (pandas), np (numpy)\n",
    "    - math, re, datetime\n",
    "    - sklearn.preprocessing\n",
    "    - matplotlib.pyplot as plt\n",
    "    - seaborn as sns\n",
    "    - scipy.stats (for statistical analysis)\n",
    "    - All helper functions listed above\n",
    "    \n",
    "    Rules:\n",
    "    - Return only executable Python code, no explanations, no markdown blocks\n",
    "    - Use helper functions when appropriate for data profiling and quality analysis tasks\n",
    "    - ASSUME \\\"df\\\" IS ALREADY DEFINED\n",
    "    - For profiling queries, use helper functions that print comprehensive reports\n",
    "    - ALWAYS assign results to variables when functions return data: profile = generate_profile_report(df)\n",
    "    - In order to generate a response/message to the user use print statements\n",
    "    print(\"message\")\n",
    "    - Write a detailed print message to summarise actions taken and reasons\n",
    "    \n",
    "    Common query patterns:\n",
    "    - \"Generate a data profile report\" or \"Profile this dataset\" -> profile = generate_profile_report(df)\n",
    "    - \"Calculate data quality metrics\" or \"Check data quality\" -> metrics = calculate_quality_metrics(df)\n",
    "    - \"Analyze correlations\" or \"Find correlations\" -> corr_analysis = analyze_correlations(df)\n",
    "    - \"Detect patterns\" or \"Find patterns in data\" -> patterns = detect_patterns(df)\n",
    "    - \"Compare data quality\" -> metrics1 = calculate_quality_metrics(df1); metrics2 = calculate_quality_metrics(df2)\n",
    "    \"\"\")) \n",
    "    messages.append(HumanMessage(content=f\"User request: {user_query}\"))\n",
    "    \n",
    "    # Call LLM with message chain\n",
    "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")\n",
    "    response = llm.invoke(messages)\n",
    "    generated_code = response.content.strip()\n",
    "    \n",
    "    # Execute code\n",
    "    try:\n",
    "        original_df = df.copy()\n",
    "        # Create local namespace with our variables\n",
    "        local_vars = {\n",
    "            'df': df.copy(),\n",
    "            'original_df': original_df,\n",
    "            'pd': pd,\n",
    "            'np': np,\n",
    "            'plt': plt,\n",
    "            'sns': sns,\n",
    "            'stats': stats,\n",
    "            'Counter': Counter,\n",
    "            'generate_profile_report': generate_profile_report,\n",
    "            'calculate_quality_metrics': calculate_quality_metrics,\n",
    "            'analyze_correlations': analyze_correlations,\n",
    "            'detect_patterns': detect_patterns,\n",
    "            'print': print\n",
    "        }\n",
    "        \n",
    "        exec(generated_code, globals(), local_vars)\n",
    "        return local_vars['df']\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"Generated Code:{generated_code}\")\n",
    "        return original_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30928b6",
   "metadata": {},
   "source": [
    "# **Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b618978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create comprehensive test dataset for profiling\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# np.random.seed(42)  # For reproducible results\n",
    "\n",
    "# # Create test data with various data quality issues and patterns\n",
    "# test_data = {\n",
    "#     'id': range(1, 201),  # Clean integer ID\n",
    "#     'name': ['John Doe', 'jane smith', 'BOB JOHNSON', '  Mary Brown  ', 'ALICE WHITE'] * 40,  # Text with cleaning needs\n",
    "#     'email': ['john@email.com', 'jane.invalid', 'bob@test.co.uk', 'mary@domain.org', 'alice@company.com'] * 40,  # Mixed valid/invalid emails\n",
    "#     'age': np.random.randint(18, 80, 200),  # Age-like pattern\n",
    "#     'salary': np.random.normal(60000, 15000, 200),  # Normal distribution with some potential outliers\n",
    "#     'department': np.random.choice(['IT', 'HR', 'Finance', 'Marketing', 'Operations'], 200),  # Categorical\n",
    "#     'score': np.random.uniform(0, 100, 200),  # Percentage-like values\n",
    "#     'years_experience': np.random.exponential(3, 200),  # Skewed distribution\n",
    "#     'bonus_pct': [f\"{x:.1f}%\" for x in np.random.uniform(5, 15, 200)],  # Percentage strings\n",
    "#     'join_date': pd.date_range('2020-01-01', periods=200, freq='D')[:200],  # Date column\n",
    "#     'active': np.random.choice(['Yes', 'No'], 200),  # Boolean-like pattern\n",
    "#     'phone': ['(123) 456-7890', '123-456-7890', '1234567890', '+11234567890', 'invalid'] * 40,  # Phone patterns\n",
    "# }\n",
    "\n",
    "# # Add some missing values and duplicates\n",
    "# test_df = pd.DataFrame(test_data)\n",
    "\n",
    "# # Add missing values to simulate real data issues\n",
    "# missing_indices = np.random.choice(test_df.index, 20, replace=False)\n",
    "# test_df.loc[missing_indices, 'salary'] = np.nan\n",
    "\n",
    "# missing_indices2 = np.random.choice(test_df.index, 15, replace=False)\n",
    "# test_df.loc[missing_indices2, 'bonus_pct'] = np.nan\n",
    "\n",
    "# # Add some duplicate rows\n",
    "# duplicate_rows = test_df.sample(5).copy()\n",
    "# test_df = pd.concat([test_df, duplicate_rows], ignore_index=True)\n",
    "\n",
    "# # Add some outliers\n",
    "# outlier_indices = np.random.choice(test_df.index, 5, replace=False)\n",
    "# test_df.loc[outlier_indices, 'salary'] = np.random.uniform(200000, 300000, 5)  # Salary outliers\n",
    "\n",
    "# print(f\"Test dataset created: {test_df.shape}\")\n",
    "# print(f\"Data types: {test_df.dtypes.value_counts().to_dict()}\")\n",
    "# print(f\"Missing values per column: {test_df.isnull().sum().to_dict()}\")\n",
    "# print(f\"Duplicate rows: {test_df.duplicated().sum()}\")\n",
    "# print(\"\\\\nSample data:\")\n",
    "# print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9610f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test all profiling functions\n",
    "# print(\"=== TESTING PROFILING FUNCTIONALITY ===\\\\n\")\n",
    "\n",
    "# # Test 1: Generate Profile Report\n",
    "# print(\"1. TESTING: Generate Profile Report\")\n",
    "# print(\"-\" * 50)\n",
    "# query1 = \"Generate a comprehensive data profile report\"\n",
    "# result1 = profiling(test_df.copy(), query1)\n",
    "# print(\"✓ Profile report test completed\\\\n\")\n",
    "\n",
    "# # Test 2: Calculate Quality Metrics  \n",
    "# print(\"2. TESTING: Calculate Quality Metrics\")\n",
    "# print(\"-\" * 50)\n",
    "# query2 = \"Calculate data quality metrics\"\n",
    "# result2 = profiling(test_df.copy(), query2)\n",
    "# print(\"✓ Quality metrics test completed\\\\n\")\n",
    "\n",
    "# # Test 3: Analyze Correlations\n",
    "# print(\"3. TESTING: Analyze Correlations\")\n",
    "# print(\"-\" * 50)\n",
    "# query3 = \"Analyze correlations in the data\"\n",
    "# result3 = profiling(test_df.copy(), query3)\n",
    "# print(\"✓ Correlation analysis test completed\\\\n\")\n",
    "\n",
    "# # Test 4: Detect Patterns\n",
    "# print(\"4. TESTING: Detect Patterns\")\n",
    "# print(\"-\" * 50)\n",
    "# query4 = \"Detect patterns in the data\"\n",
    "# result4 = profiling(test_df.copy(), query4)\n",
    "# print(\"✓ Pattern detection test completed\\\\n\")\n",
    "\n",
    "# print(\"=== ALL PROFILING TESTS COMPLETED SUCCESSFULLY ===\")\n",
    "# print(\"✓ All functions executed without errors\")\n",
    "# print(\"✓ Comprehensive reporting and analysis generated\")\n",
    "# print(\"✓ Integration with main system verified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f9c065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df.info()\n",
    "# result.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
