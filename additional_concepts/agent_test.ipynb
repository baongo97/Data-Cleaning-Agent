{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b62d947c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /home/ngbao/Projects/Data-Cleaning-Agent_Workshop-Version/venv/lib/python3.12/site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-openai in /home/ngbao/Projects/Data-Cleaning-Agent_Workshop-Version/venv/lib/python3.12/site-packages (0.3.32)\n",
      "Requirement already satisfied: langchain-core in /home/ngbao/Projects/Data-Cleaning-Agent_Workshop-Version/venv/lib/python3.12/site-packages (0.3.75)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /home/ngbao/Projects/Data-Cleaning-Agent_Workshop-Version/venv/lib/python3.12/site-packages (from langchain) (0.3.11)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /home/ngbao/Projects/Data-Cleaning-Agent_Workshop-Version/venv/lib/python3.12/site-packages (from langchain) (0.4.27)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/ngbao/Projects/Data-Cleaning-Agent_Workshop-Version/venv/lib/python3.12/site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/ngbao/Projects/Data-Cleaning-Agent_Workshop-Version/venv/lib/python3.12/site-packages (from langchain) (2.0.43)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/ngbao/Projects/Data-Cleaning-Agent_Workshop-Version/venv/lib/python3.12/site-packages (from langchain) (2.32.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/ngbao/Projects/Data-Cleaning-Agent_Workshop-Version/venv/lib/python3.12/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.99.9 in /home/ngbao/Projects/Data-Cleaning-Agent_Workshop-Version/venv/lib/python3.12/site-packages (from langchain-openai) (1.107.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /home/ngbao/Projects/Data-Cleaning-Agent_Workshop-Version/venv/lib/python3.12/site-packages (from langchain-openai) (0.11.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /home/ngbao/Projects/Data-Cleaning-Agent_Workshop-Version/venv/lib/python3.12/site-packages (from langchain-core) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/ngbao/Projects/Data-Cleaning-Agent_Workshop-Version/venv/lib/python3.12/site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/ngbao/Projects/Data-Cleaning-Agent_Workshop-Version/venv/lib/python3.12/site-packages (from langchain-core) (4.15.0)\n",
      "Requirement already satisfied: packaging>=23.2 in /home/ngbao/Projects/Data-Cleaning-Agent_Workshop-Version/venv/lib/python3.12/site-packages (from langchain-core) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/ngbao/Projects/Data-Cleaning-Agent_Workshop-Version/venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/ngbao/Projects/Data-Cleaning-Agent_Workshop-Version/venv/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /home/ngbao/Projects/Data-Cleaning-Agent_Workshop-Version/venv/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /home/ngbao/Projects/Data-Cleaning-Agent_Workshop-Version/venv/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /home/ngbao/Projects/Data-Cleaning-Agent_Workshop-Version/venv/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/ngbao/Projects/Data-Cleaning-Agent_Workshop-Version/venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/ngbao/Projects/Data-Cleaning-Agent_Workshop-Version/venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/ngbao/Projects/Data-Cleaning-Agent_Workshop-Version/venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /home/ngbao/Projects/Data-Cleaning-Agent_Workshop-Version/venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/ngbao/Projects/Data-Cleaning-Agent_Workshop-Version/venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ngbao/Projects/Data-Cleaning-Agent_Workshop-Version/venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/ngbao/Projects/Data-Cleaning-Agent_Workshop-Version/venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/ngbao/Projects/Data-Cleaning-Agent_Workshop-Version/venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ngbao/Projects/Data-Cleaning-Agent_Workshop-Version/venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ngbao/Projects/Data-Cleaning-Agent_Workshop-Version/venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ngbao/Projects/Data-Cleaning-Agent_Workshop-Version/venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ngbao/Projects/Data-Cleaning-Agent_Workshop-Version/venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
      "Requirement already satisfied: greenlet>=1 in /home/ngbao/Projects/Data-Cleaning-Agent_Workshop-Version/venv/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/ngbao/Projects/Data-Cleaning-Agent_Workshop-Version/venv/lib/python3.12/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2025.9.1)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ngbao/Projects/Data-Cleaning-Agent_Workshop-Version/venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/ngbao/Projects/Data-Cleaning-Agent_Workshop-Version/venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain langchain-openai langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c84439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from datetime import datetime\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"OpenAI API Key not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d630ff13",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a1b5457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global DataFrame variable that tools can access\n",
    "current_df = None\n",
    "\n",
    "@tool\n",
    "def get_dataframe_info() -> str:\n",
    "    \"\"\"Get basic information about the DataFrame including shape, columns, and data types.\"\"\"\n",
    "    global current_df\n",
    "    if current_df is None:\n",
    "        return \"No DataFrame loaded. Please load a DataFrame first.\"\n",
    "    \n",
    "    info = {\n",
    "        \"shape\": current_df.shape,\n",
    "        \"columns\": list(current_df.columns),\n",
    "        \"dtypes\": {col: str(dtype) for col, dtype in current_df.dtypes.items()},\n",
    "        \"memory_usage\": f\"{current_df.memory_usage(deep=True).sum()} bytes\"\n",
    "    }\n",
    "    return json.dumps(info, indent=2)\n",
    "\n",
    "@tool\n",
    "def get_column_stats(column_name: str) -> str:\n",
    "    \"\"\"Get statistical summary for a specific column.\"\"\"\n",
    "    global current_df\n",
    "    if current_df is None:\n",
    "        return \"No DataFrame loaded. Please load a DataFrame first.\"\n",
    "    \n",
    "    if column_name not in current_df.columns:\n",
    "        return f\"Column '{column_name}' not found. Available columns: {list(current_df.columns)}\"\n",
    "    \n",
    "    col = current_df[column_name]\n",
    "    \n",
    "    if col.dtype in ['int64', 'float64']:\n",
    "        stats = {\n",
    "            \"count\": int(col.count()),\n",
    "            \"mean\": float(col.mean()),\n",
    "            \"std\": float(col.std()),\n",
    "            \"min\": float(col.min()),\n",
    "            \"25%\": float(col.quantile(0.25)),\n",
    "            \"50%\": float(col.median()),\n",
    "            \"75%\": float(col.quantile(0.75)),\n",
    "            \"max\": float(col.max())\n",
    "        }\n",
    "    else:\n",
    "        stats = {\n",
    "            \"count\": int(col.count()),\n",
    "            \"unique\": int(col.nunique()),\n",
    "            \"top\": str(col.mode().iloc[0]) if not col.mode().empty else \"N/A\",\n",
    "            \"freq\": int(col.value_counts().iloc[0]) if len(col.value_counts()) > 0 else 0\n",
    "        }\n",
    "    \n",
    "    return json.dumps(stats, indent=2)\n",
    "\n",
    "@tool\n",
    "def get_missing_values() -> str:\n",
    "    \"\"\"Check for missing values in the DataFrame.\"\"\"\n",
    "    global current_df\n",
    "    if current_df is None:\n",
    "        return \"No DataFrame loaded. Please load a DataFrame first.\"\n",
    "    \n",
    "    missing = current_df.isnull().sum()\n",
    "    missing_dict = {col: int(count) for col, count in missing.items()}\n",
    "    total_missing = missing.sum()\n",
    "    \n",
    "    return json.dumps({\n",
    "        \"total_missing_values\": int(total_missing),\n",
    "        \"missing_by_column\": missing_dict,\n",
    "        \"percentage_missing\": {col: round((count/len(current_df))*100, 2) for col, count in missing_dict.items()}\n",
    "    }, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2f69ab",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b95bb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year       State Smoke everyday Smoke some days Former smoker Never smoked\n",
      "0  2010          AL         15.60%           6.30%        23.90%       54.20%\n",
      "1  2010          AK         13.50%           6.80%        26.10%       53.60%\n",
      "2  2010          AZ         10.70%           4.40%        27.90%       57.10%\n",
      "3  2100    Arkansas         17.30%           5.60%        24.10%          53%\n",
      "4  2010  California          7.50%           4.60%        23.10%       64.80%\n"
     ]
    }
   ],
   "source": [
    "# Create a sample DataFrame for testing\n",
    "sample_data = {\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'age': [25, 30, 35, 28, 22],\n",
    "    'salary': [50000, 75000, 90000, 60000, 45000],\n",
    "    'department': ['Engineering', 'Sales', 'Engineering', 'Marketing', 'Sales'],\n",
    "    'years_experience': [3, 8, 12, 5, 1]\n",
    "}\n",
    "df = pd.read_csv(\"../datasets/smoke.csv\")\n",
    "current_df = df  # Set the global DataFrame for tools to access\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b5e42351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: gpt-4o-mini\n",
      "Supports tool calling: True\n"
     ]
    }
   ],
   "source": [
    "# Initialize GPT-4o-mini with tool calling enabled\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.1,\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    ")\n",
    "\n",
    "# Check if the model supports tool calling\n",
    "print(f\"Model: {llm.model_name}\")\n",
    "print(f\"Supports tool calling: {hasattr(llm, 'bind_tools')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46a1bb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all tools imported from pandas_tools.ipynb (doesn't work)\n",
    "# tools = pandas_tools.tools\n",
    "\n",
    "# Local tools\n",
    "tools = [get_dataframe_info, get_column_stats, get_missing_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a736a250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available pandas tools:\n",
      "- get_dataframe_info: Get basic information about the DataFrame including shape, columns, and data types.\n",
      "- get_column_stats: Get statistical summary for a specific column.\n",
      "- get_missing_values: Check for missing values in the DataFrame.\n"
     ]
    }
   ],
   "source": [
    "# Bind tools to the LLM\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "print(\"Available pandas tools:\")\n",
    "for tool_func in tools:\n",
    "    print(f\"- {tool_func.name}: {tool_func.description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7e5a6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a helpful data analysis assistant. You have access to pandas DataFrame tools that can help analyze data.\n",
    "    \n",
    "    The current DataFrame contains data from a CSV file. Use the appropriate tools to explore and understand the data structure.\n",
    "\n",
    "    When users ask about data analysis, use the appropriate tools to get the information they need.\n",
    "    Always provide clear, helpful explanations of the results.\"\"\"),\n",
    "    (\"placeholder\", \"{chat_history}\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"placeholder\", \"{agent_scratchpad}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c9cba41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent and AgentExecutor created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create the agent using create_tool_calling_agent\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "# Create the agent executor\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, \n",
    "    tools=tools, \n",
    "    verbose=True,  # Shows which tools are being called\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "print(\"Agent and AgentExecutor created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "00ec4d6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mStopIteration\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mGive me statistics of smoke everyday column\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m messages.append(HumanMessage(content=query))\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m response = \u001b[43magent_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(response[\u001b[33m'\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Data-Cleaning-Agent_Workshop-Version/venv/lib/python3.12/site-packages/langchain/chains/base.py:144\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    141\u001b[39m include_run_info = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33minclude_run_info\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    142\u001b[39m return_only_outputs = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mreturn_only_outputs\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprep_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m callback_manager = CallbackManager.configure(\n\u001b[32m    146\u001b[39m     callbacks,\n\u001b[32m    147\u001b[39m     \u001b[38;5;28mself\u001b[39m.callbacks,\n\u001b[32m   (...)\u001b[39m\u001b[32m    152\u001b[39m     \u001b[38;5;28mself\u001b[39m.metadata,\n\u001b[32m    153\u001b[39m )\n\u001b[32m    154\u001b[39m new_arg_supported = inspect.signature(\u001b[38;5;28mself\u001b[39m._call).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Data-Cleaning-Agent_Workshop-Version/venv/lib/python3.12/site-packages/langchain/chains/base.py:535\u001b[39m, in \u001b[36mChain.prep_inputs\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    531\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    532\u001b[39m         \u001b[38;5;66;03m# If there are multiple input keys, but some get set by memory so that\u001b[39;00m\n\u001b[32m    533\u001b[39m         \u001b[38;5;66;03m# only one is not set, we can still figure out which key it is.\u001b[39;00m\n\u001b[32m    534\u001b[39m         _input_keys = _input_keys.difference(\u001b[38;5;28mself\u001b[39m.memory.memory_variables)\n\u001b[32m--> \u001b[39m\u001b[32m535\u001b[39m     inputs = {\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_input_keys\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m: inputs}\n\u001b[32m    536\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    537\u001b[39m     external_context = \u001b[38;5;28mself\u001b[39m.memory.load_memory_variables(inputs)\n",
      "\u001b[31mStopIteration\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# response = agent_executor.invoke({\"input\": \"What's the shape and structure of the DataFrame?\"})\n",
    "# print(response['output'])\n",
    "\n",
    "# response = agent_executor.invoke({\"input\": \"Are there any missing values in the dataset?\"})\n",
    "# print(response['output'])\n",
    "\n",
    "messages = []\n",
    "\n",
    "query = \"Give me statistics of smoke everyday column\"\n",
    "messages.append(HumanMessage(content=query))\n",
    "\n",
    "response = agent_executor.invoke(messages)\n",
    "print(response['output'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
